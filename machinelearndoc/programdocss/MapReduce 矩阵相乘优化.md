MapReduce 矩阵相乘优化

[传送](http://my.oschina.net/u/1047640/blog/266219)


##### 一、 最简单的乘法

把m*n 和n*l的矩阵A和B相乘，这估计是最容易想到的方法了：

把A（m*n）的元素，每个发送l次，把B（n*l）的元素每个发送m次。将发送到一起的数据相乘求和，得到最后的结果。

![](http://static.oschina.net/uploads/space/2014/0518/155725_d8KQ_1047640.png)


优点：在知道坐标的情况下，这个过程就一轮mapreduce。
缺点：每个值要被发送多次。m*n 和n*l的矩阵，发送的元素有m*l*2次，比如100万的方正相乘，那么中间文件有100*100百万*百万的记录，这么多元素在网络上传输，结果你懂的。另外，这个并没有有效的避开0元素，也就是说对稀疏矩阵来说很浪费。而且必须要知道矩阵的下标和行列数。因为在map里需要用到。实际情况是，现实中的矩阵，并不一定都是用递增的整数来表示下标的。有可能是人名，帐号等。

上面的方法乍一看还ok，用小数据测试一下也是正确的，不过这个方法问题在于，reduce里收到的数据也许非常非常多，多到内存放不下。然后节点就heapSpace overFlow了。话说回来如果一定要在这个方法上改进，可以把map reduce分成两步。第一步还是和上面一样，但是key中再加一个字段k，表示，这是结果矩阵中坐标为（i,j)的元素的第k个用来求和的元素。k其实是A的列号和B的行号。这样上面的过程就变成

A： key(1,1,1) value(a,1)  key(1,1,2) value(a,2)  key(1,1,3) value(a,3)
       key(1,2,1) value(a,1)  key(1,2,2) value(a,2) ..........
B:   key(1,1,1) value(b,1)    key(1,1,2) value(b,6) key(1,1,3) value(b,0)
      key(2,1,1) value(b,1)    key(2,1,2) value(b,6) ...........
然后在reduce中对元素求乘积 生成：
      key(1,1)  value= (a,1)*(b,1)   key(1,1)  value= (a,2)*(b,6)    key(1,1)  value= (a,3)*(b,0). 其实连元素来自A还是B都可以省了。
再来第二轮mapreduce，对上一步mapreduce的结果再group by，求和。这样改进之后，没有reduce内存溢出的问题。看起来像是多大的矩阵都可以计算，其实由于没能解决一个A或B的元素要发送很多次的问题，在计算超级大的矩阵的时候，这个方法还是完全不能用。


##### 二、 分解成 行和列 来作

观察上面的最细的矩阵乘法，如果矩阵是比较稀疏的，其实有很多东西就白白发送了，发出去在reduce里其实没有用到。也是这个原因导致中间文件急剧膨胀，如果矩阵超级大，即使在mapreduce里，也要跑相当长时间。再来回忆一下矩阵乘法，能发现一些规律。
假设现在有两个矩阵，分别是同现矩阵和用户评分矩阵，现在要把两个矩阵相乘来获得推荐分数。

![](http://static.oschina.net/uploads/space/2014/0518/163937_31Jr_1047640.png)

![](http://static.oschina.net/uploads/space/2014/0518/164027_BBR4_1047640.png)

通过观察，把A矩阵的一列和B矩阵的一行发送到同一个reduce中，然后在reduce中，对来自A的元算和来自B的元素做笛卡尔乘积，对笛卡尔乘积的结果再做一次mapReduce，对相应的坐标求和就可以得到结果矩阵的一个个元素了。
优点：相对与最细的矩阵乘法而言，大大压缩了中间文件的大小，对稀疏矩阵而言，避免了找不到对应元素而造成的浪费。在这个方法里，为0的元素不发送就行了。另外一个明显的优点就是，我用不着一定用数字来表示矩阵的坐标，是字符串也行。只要A矩阵的列坐标和B矩阵的行坐标对的上就行。
缺点：变成了两轮mapreduce才搞得定。另一个明显缺点是，在一个reduce的节点内存里面，未必放的下A的一行+B的一列。因为仅凭key不能区分数据是来自A矩阵还是B矩阵，最起码要往内存里读一次才知道怎么做笛卡尔乘积。当然写成一个文件放到reduce本地，然后再来算也行，不过这样有点低效了，不如直接在内存里来的快。这样其实还是限制了这个方法的范围，我觉得1百万乘以1百万的稀疏矩阵用这个方法来做还是撑的住的，当然得看矩阵稀疏程度。


##### 三、 广义分块乘法

上面的行*列的做法，其实也是矩阵分块乘法的一种特别情况，我把A矩阵按列分块，B矩阵按行分块。然后进行乘法计算，于是得到了上面第二种方法。但是缺点在于，reduce的时候，数据太大，会导致不能直接在内存里计算。那解决办法自然而然就是把块再划小一点，别一整行一整列来分块。但是具体怎么分块，我觉得这个问题还得看矩阵到底长什么样子。商品同现矩阵往往大不到哪里去，而用户矩阵有可能就很长，没看到矩阵本身，很难想出通用的方法来分块。在实现上面分块也好复杂，
优点：速度快，内存利用好。而且每个reduce，在本地算矩阵乘法时，可以采用一些已经发展很好的单机算法。效率倍儿高。
缺点：不好分块，难度略大

![](http://static.oschina.net/uploads/space/2014/0519/092804_eydi_1047640.jpg)


![](http://img.blog.csdn.net/20130609192256515)



##### 利用Hadoop实现超大矩阵相乘之我见（一）

[传送](http://www.cnblogs.com/eczhou/p/3340731.html)


###### 前记
 最近，公司一位挺优秀的总务离职，欢送宴上，她对我说“你是一位挺优秀的程序员”，刚说完，立马道歉说“对不起，我说你是程序员是不是侮辱你了？”我挺诧异，程序员现在是很低端，很被人瞧不起的工作吗？或许现在连卖盗版光盘的，修电脑的都称自己为搞IT的，普通人可能已经分不清搞IT的到底是做什么的了。其实我想说，程序员也分很多种的，有些只能写if-then-else,有些只能依葫芦画瓢，但真正的程序员我想肯定是某个领域的专家，或许他是一位数学家，或许他是一位物理学家，再或许他是计算机某个细分领域的专家，他是理论与现实的结合，是凌驾于纯理论的存在！而笔者我正立志成为这样的能让人感到骄傲的程序员。

切入正题吧，谈到云计算，不得不提大数据，处理大数据，肯定逃不离分布式计算。互联网行业，无论是商品推荐还是好友推荐，还是PageRank,所要处理的Items规模、用户规模都是极其庞大的，小则数以百万、千万记，大则数以亿记。在此数据基础上，诞生了很多优秀的推荐算法，推荐算法中大部分会运用到矩阵运算。如此大规模的数据，一台计算机已经没有能力处理，说简单点，一台服务器的内存可能连加载半个矩阵数据都不够，更别谈处理了。“当一头牛拉不动车时，很少有人去找一头更大更强壮的牛，而是找来更多的牛一起拉。”，这就是分布式计算，而Hadoop就是在分布式集群上处理大记录集的强大利器。

笔者最近对推荐算法挺感兴趣的，也研究了一些！部分算法数学公式研究的透彻了，便有自己想实现的冲动，可公式里的矩阵运算可不是那么简单!所以就想从研究超大规模矩阵相乘开始，一方面为以后做大规模矩阵运算、实现推荐算法做技术储备；另一方面也想真正体验一把用Hadoop实现分布式运算的乐趣；最重要的是能够写一些包含独特思想，有研究成分，有技术含量的代码。

###### 摘要

本文首先讨论了目前现有的大矩阵运算方法，并指出其不足；接着提出自己的矩阵运算方法来解决目前现有方法所存在的问题，同时通过实验来观测本文方法所存在的问题，并针对这些问题，对本文方法进行再优化。

现有方法

 * 行列相乘运算
   *  简介

传统的矩阵运算是A矩阵中的每一行分别与B矩阵中的每一列相乘。假设矩阵A的规模为（m*r）,矩阵B的规模为（r*n）,则矩阵C的规模为（m*n）。矩阵C中元素Ci,j是A中第i行与B中第j列元素依次对应相乘并汇总的结果。公式表示如下：

![](http://images.cnitblog.com/blog/310680/201309/26135143-6daac7d49c4a4aa1b65ac37de6d29472.png)


每一个Ci,j的计算都是独立的，所以可以交由不同的计算节点完成。

* 缺点
 1. 矩阵规模有一定限制，如果A矩阵或B矩阵有一个超大，则某个运算节点就很有可能由于内存限制，加载不了A矩阵的第i行或B矩阵的第j列。

 2. 对于稀疏矩阵计算没优势。若A，B中有稀疏矩阵存在，需判断A中i行与B中第j列对应的位置上是否有0元素，换句话说，还是需要加载第i行，第j列的全部内容，若某个位置没有输入，在运算过程中需要将相应位置用0填充，这样会造成上一点所存在的问题：内存放不下。


* 矩阵分块运算
*  简介

 当矩阵大到一定程度时，一台服务器由于内存限制已经无法处理，不过由于矩阵具体天然的可分块的特性，许多基于分块的矩阵运算算法诞生了，《数学之美》这本书上介绍的大矩阵相乘方法就是基于分块的，现简单介绍如下：

1、当A矩阵纵向很大，横向不大时，我们将A矩阵分块，将A矩阵中的分块分别与B矩阵相乘，通过Hadoop，这些计算可以并行进行，如图1所示：

![](http://images.cnitblog.com/blog/310680/201309/26133812-99b31a08aa934015a11a19cc178713db.png)

图中A1*B=C1,A2*B=C2,…,每部分计算分别可在不同的计算节点完成，最后将结果组合在一起。

2. 当A矩阵为一个真正的超大矩阵（横向纵向都很大），与之相乘的B矩阵也必是一个超大矩阵（至少纵向很大），此时A，B矩阵都需要按行按列进行分块，并将不同的分块计算交由不同的计算节点完成，如图2所示。

![](http://images.cnitblog.com/blog/310680/201309/26133859-83d01098a7ac4192a7ff02fbaacb2369.png)

图中，矩阵A中的每一块都需要和矩阵B中对应位置的块依次相乘，这些块与块之间的相乘运算可以由不同的计算节点完成，最后将不同块与块的运算结果，经过严密精确的控制，对相关结果进行合并（主要是相加），得到最终的运算结果C。

* 缺点
 1. 对于不同的矩阵规模，如何分块是难点，同时块的大小受限于内存大小。

 2. 块与块之间的运算以及组织较繁琐。

 3. 不太利于稀疏矩阵的运算（0值占用较多的存储空间，以及会做很多无效运算）


* 基于最小粒度相乘的算法

为了文档的命名结构，笔者自己根据算法原理，起了这个名字。

  * 简介

“行列相乘运算”和“分块运算”都受限于计算节点的内存限制。那么有没有一种运算，跟计算节点的内存大小无关呢？答案是：肯定有！总所周知，矩阵相乘的最小粒度计算是两个矩阵中的两个数相乘，比如，且计算结果是的一个组成部分。

假设有两个超大矩阵A和B，A的规模是（m*r）,B的规模是（r*n），将矩阵相乘中的最小粒度乘法运算进行统计，我们不难发现：A中每个元素Ai,k需要与B中第k行的元素Bk,j(j=1,2,...,n)依次相乘，计算结果分别为Ci,j的一个组成部分；而B中每个元素Bk,j需要与A中第j列的元素Ai,k(i=1,2,...,m)依次相乘，计算结果分别为Ci,j的一个组成部分。具体如图3所示

![](http://images.cnitblog.com/blog/310680/201309/26134115-f5041d455fbe4ef98e3653a77cb31774.png)

由于Ai,k*Bk,j是独立的，因此可以由不同的计算节点进行运算，最后根据key （i,j）将运算结果进行汇总相加，得到结果Ci,j。同时，每个计算节点每次计算时都是只加载两个数进行相乘，并不需要加载矩阵的某个块或者某行某列，因此没有内存的限制问题，理论上只要hadoop的HDFS文件系统够大，就可以计算任意大规模的矩阵相乘。

在Map-Reduce过程中，由于Map的每条输入记录只被处理一次便不再使用，因此根据图3理论，对于矩阵A中的每个元素，在实质进行乘法运算之前，我们需要生成n个副本，对于矩阵B中每个元素，我们需要生成m个副本，并将相应位置上的副本进行对应好。比如对于Ai,k需生成n个副本，并与B中相应元素对应好，并以A中元素的行号，B中元素的列号作为key：

![](http://images.cnitblog.com/blog/310680/201309/26135241-8e52d79205784bf395340f877967b376.png)

以以上文件作为Map输入，在Map中进行乘法运算，在Reduce阶段按key进行加法运算，就得到矩阵相乘计算结果了。



  * 缺点及难点

1、矩阵元素副本准备。

如果想以以上格式作为初始Map输入，那么我们就需要事先将数据整理成以上格式。对于两个超大矩阵相乘来说，这是一个艰巨的任务。矩阵元素一般来源于数据库（暂且如此假设，比如说做商品推荐，用户数据，商品数据都是存在数据库中的），那么整理成以上格式的文档作为Map输入文件，我们需要查询数据库的次数为

 m*r*n + r*n*m
 
 由于m,r,n都是极其庞大的，这个查询次数是我们万万不能忍受的。理想的数据库查询次数是：
 
 m*r + r*n
 
还有一种方法是矩阵元素只取一次，每个元素的副本生成交给Map-Reduce去做，但是这样存在另一个问题：如果在Map-Reduce过程中将A矩阵和B矩阵中的元素进行副本拷贝，单个Map的运算时间有点让人接受不了，打个比方，一个Map块为64M，大约存了500万条A矩阵的元素，同时B矩阵的n为10亿，那么计算这个Map的节点需生成500万*10亿条副本，这个时间是难以忍受的。

2. 两个矩阵中需相乘的元素如何对应。

由于利用数据库查询进行矩阵元素对对应时间复杂度太高，一般不太可行。所以可以考虑利用Map-Reduce对相应元素进行对应。不过Map只对输入记录进行一次处理，处理完毕便结束，不存在内存的概念，所以对两个矩阵中元素进行对应是一个难点。

3. 文件大小规模。

对于超大规模矩阵，由于A中m和B中n太大，除去稀疏元素（值为0）不纳入计算，需拷贝的元素依旧很多，拷贝完的文件大小是极其庞大的。笔者做了个实验，将A（1000*1000） B（1000*1000）两个稠密矩阵中的元素按规定（A中每个元素拷贝1000份，B中每个元素拷贝1000份）进行副本拷贝，拷贝完的记录数为2*109条，文件大小达到24G。那么对于亿万规模的矩阵，文件大小将成指数级增长。


###### 本文方法

行列相乘运算”对于稀疏矩阵可以，但是对于大型的稠密矩阵显得有点力不从心；而对于“分块矩阵运算”很多学者做了很多研究，但是笔者不太喜欢该算法，第一是逻辑控制麻烦，第二是对块的大小优化来优化去，没有解决本质上的问题。笔者我喜欢简单的东西，所以更倾向于“基于最小粒度相乘的算法”。不过，就像我们之前所说的，“基于最小粒度相乘的运算”存在三个问题，接下来，笔者将针对其中的两个问题阐述笔者自己的想法。


  新颖的矩阵相乘元素映射方法
   简介

矩阵A*B=C中，Ci,j是A中第i行与B中第j列相乘的结果,如公式（1）所示。通俗点可以写成如下格式：
![](http://images.cnitblog.com/blog/310680/201309/26135501-af70d9bdaa4a43d1805dd2b5c7616fb6.png)

传统的方法在Map输入段通常将输入记录组织成如下格式：

![](http://images.cnitblog.com/blog/310680/201309/26135645-f460f126859944c69564c1bcf195e753.png)

然后在Map端进行各条记录的相乘运算，最后在Reduce阶段进行汇总，得出最终矩阵相乘的结果。不过正像“基于最小粒度相乘的算法”中所说的，由于key i-j 不具备明显的区分度，且Map过程中，内存不保留矩阵元素，将数据组织成以上格式是极其困难的。如果在Map输入前，将数据组织成以上格式，查询数据库的时间复杂度也是难以接受的。

通过思考我们不难发现，最终结果Ci,j是由r个值相加而成的，第k个组成成分为：Ai,k-Bk,j，为了使key更有区分度，我们将key修改为：

![](http://images.cnitblog.com/blog/310680/201309/26135723-e4dd8026d8ba448e8e8036271650d4ac.png)

这样的key所代表的两个值相乘，得到了Ci,j中第k个组成元素。所以对于A矩阵和B矩阵在Map阶段完成数据副本拷贝完后，所有的Map数据记录中，i-j-k的key有且至多只有两个（由于稀疏元素不纳入计算与拷贝，所以若为一个，则说明与之相乘的另一个元素为0，若一个也没有，则说明Ai,k与Bk,j都为0，没有纳入计算与拷贝）。

由于A中每个元素理论上都需要被计算n遍，所以可以将A中元素按如下规则进行n遍拷贝，对于Ai,k，拷贝方式如下：
![](http://images.cnitblog.com/blog/310680/201309/26135755-31d00d267d404907aa236eddc2781b63.png)

对于B中每个元素，理论上每个元素都需要被计算m遍，所以可以将B中元素按如下规则进行m遍拷贝，对于Bk,j，拷贝方式如下：

![](http://images.cnitblog.com/blog/310680/201309/26135842-1511977decdb4b959fb2fd0e9952b11b.png)

* 实验结果

笔者用以上方法做了实验，A(m,r)*B(r,n)=C，其中m=r=n=1000,所以两个矩阵中共有2*106个元素，A与B都为稠密矩阵，以“A-i-k value”和“B-k-j value”的形式存储原始的A矩阵和B矩阵的元素，文件大小为24M。由于文件太小，所以只交给一个Map进行副本的拷贝工作，每个元素都被拷贝一千遍，拷贝完总记录数为2*109条。消耗时间如下：
![](http://images.cnitblog.com/blog/310680/201309/26134335-18688ac1dab94a2084dc1b4f397765fd.png)

由图4可以看出，一个Map执行的时间非常长，这是因为Map中的每条记录都需要拷贝1000遍。如果在现实应用中，两个矩阵超大，那么许多Map的块大小都将被填满，一个块大概放500万条记录，同时由于每条记录都被拷贝m或n遍（m,n很可能就是数以亿计），那么一个Map的执行时间就是无底洞了。

为了减少每个Map的执行时间，笔者苦苦冥想，终于想出一种方法，将在接下来的小节进行介绍。


  * 创新的细胞分裂拷贝算法

上一小节中有讲到Map的执行时间过长，有同事建议我说将Map的块变小点，这样里面的记录数也少点，不同的块由不同的节点执行。但是笔者认为这种想法不合理，一个块是变小了，里面的记录也小了，但是若每条记录需拷贝的数量是庞大的，那根本于事无补。而且对于不同大小的矩阵相乘，矩阵元素需拷贝的数量也都是不一样的，因此块的大小很难控制。再者，对于Hadoop运算，正常情况下都是加大Map块的大小，这样有利于计算的集中。

而在本方法中，Map拷贝过程之所以时间太长，笔者认为是由于每条记录拷贝的数量太多造成的，如果一条记录的拷贝能分段在不同的节点完成就好了，出于这样的想法，笔者设计了一种利用Map迭代进行拷贝的方法，由于迭代过程中Map数量的扩张有点像细胞分裂，笔者称之为“细胞分裂拷贝算法”。

 * 简介
 由于每个矩阵元素需要拷贝多少遍是确定的，因此我们可以设计一种分段拷贝方法来让不同的节点进行拷贝工作。这里有两个变量需要介绍，一是num_split,代表一条记录在一次迭代过程中最多被分的段数，另一个是num_copy,代表每个最终分段最多需拷贝的记录数。在迭代过程中，如果某条记录的某个分段范围大于num_copy,则继续进行分段，否则就进行拷贝工作。现举例说明迭代过程。
 
对于A中元素Ai,k，其需要被拷贝1000遍，为了将其拷贝成公式（3）所示，我们利用细胞分裂拷贝算法将拷贝工作分配到不同的计算节点进行，该例中num_split和num_copy的数值都为10，那么迭代过程如下：

![](http://images.cnitblog.com/blog/310680/201309/26134415-34b901c4aaa04f8aac6867da97c9d5d0.png)

而对于B中元素，我们同理利用迭代拷贝的方法将其拷贝成公式（4）所示格式，即主要的范围辨别集中在i上。

由图5可以可以看到，每一次迭代，数据记录都是成num_split的倍数增长，这样，随着记录集文件大小的增长，文件被分成越来越多的Map，自然也就被分配到越来越多的计算节点进行执行。查看图5中的第三次迭代工作，由于记录范围符合记录生成条件，即记录范围<=num_copy，第三次迭代过程中，每个Map上的每条记录只被copy了num_copy遍，相较于之前每条记录被copy1000遍，时间大大减少，这种方法对于规模大的矩阵尤其适用。

此外值得一提的是，由于现实中矩阵A和矩阵B的规模往往不一样，在实现“细胞分裂拷贝算法”时，需要设置两个标志变量来判断不同矩阵的记录分段迭代过程是否结束，若两个矩阵的分段迭代过程都结束了，则进入最后一次迭代过程：记录拷贝的生成。

* 试验结果

笔者对，A(m,r)*B(r,n)=C，其中m=r=n=1000做实验，共经历三次迭代完成矩阵元素的拷贝工作，如图6所示，第一次迭代，输入只有24M，所以只有一个Map，输出了3个Map，第二次迭代，由于输入的3个Map，输出了30个Map，符合num_split的扩张倍数，第三次迭代的工作是执行拷贝工作。

![](http://images.cnitblog.com/blog/310680/201309/26134545-a14bfc50e3af4e2c93ec19fcd16dc553.png)

同时，我们可以看到，在最后生成拷贝的过程中，每个Map的执行时间比较稳定，如图7所示，这样，当我们的集群够大时，这30个Map在一轮过程中便可以执行完毕。

最后，当矩阵元素拷贝工作与对应工作完成后，接下来就比较简单了，再经历两轮Map-Reduce过程，就可以得到运算结果了。

 * 总结

本文方法针对“基于最小粒度相乘的算法”中所固有的缺点及难点，利用巧妙的设计，有效的利用Map-Reduce工具进行相乘元素的对应，同时为了减少单个元素在一个节点上拷贝太多记录所造成的时间损耗，设计了“细胞分裂拷贝算法”，有效的将同一条记录的拷贝工作分发到不同的节点进行，大大缩短了一个节点的执行时间，同时充分利用和发挥了集群运算的优势。

但是本文由于算法的固有特性，并没有解决“基于最小粒度相乘的算法”中最后一个缺点:文件占用空间太大。理论上说，这个缺点对于HDFS系统是不算一个缺点的，本身HDFS系统就有足够大的空间容纳足够的数据。但是，通过实验发现，这个文件实在是庞大的，对于（1000,1000）与（1000,1000）两个稠密矩阵的相乘，所有元素的拷贝工作完成后，记录数目达到2*109条，占用空间大小为二三十G，如图8所示。那么对于更大规模的矩阵运算，文件空间要占用多大？答案是：难以估量。           


![](http://images.cnitblog.com/blog/310680/201309/26140029-805acc1a3a9e45bb810914eba8a25278.png)


###### 总结

大部分算法都有其优势与局限性，针对本文方法本质所固有的文件存储空间占用大这个问题，笔者一直是耿耿于怀，至少这样的算法是不完美的，虽然它解决了一些问题。连续几天笔者是冥思又苦想，连做梦时脑海里都是两个矩阵元素在打架了！黄天不负有心人，灵光一现，一种新的方法在笔者脑海里浮现！尽请期待下期《利用Hadoop实现超大矩阵相乘之我见（二）》，在下期中，笔者会分析本文方法本质上造成文件占用空间大的原因，同时介绍笔者新想到的自认为还比较完美的方法。新方法非常适用于大规模稠密矩阵与稀疏矩阵的相乘计算，尤其是对于稀疏矩阵，基本上没有无效计算，也不会造成多余空间的浪费。

前文

在《利用Hadoop实现超大矩阵相乘之我见（一）》中我们所介绍的方法有着“计算过程中文件占用存储空间大”这个缺陷，本文中我们着重解决这个问题。

##### 矩阵相乘计算思想

传统的矩阵相乘方法为行、列相乘的方式，即利用左矩阵的一行乘以右矩阵的一列。不过该方法针对稀疏矩阵相乘，会造成过多的无效计算，降低计算效率。为了解决这个问题，本发明采用列、行相乘计算方式，即利用左矩阵的一列中的元素与右矩阵对应行中的所有元素依次相乘，该方法有效避免了稀疏矩阵相乘过程中产生的无效计算。具体计算过程示意图如图1所示。

![](http://images.cnitblog.com/i/310680/201403/141538281963947.gif)

数据预处理

为了便于Map-Reduce模型对矩阵元素进行处理，所有的矩阵元素都存储在文本文件中，一行记录代表一个矩阵元素，针对稀疏矩阵，0元素不纳入输入文本。如图2所示。

 ![](http://images.cnitblog.com/i/310680/201403/141539218992207.gif)
 

 ![](http://images.cnitblog.com/i/310680/201403/141540104215315.gif)

我们对图2进行举例说明，假如一行记录为$L_1_2 V^L_{1,2}$。则其代表左矩阵第一行第二列的元素值为 $V_{1,2}^L$。

在一个Map过程中，我们对每一行输入数据进行预处理，若一行记录代表左矩阵元素，则提取列号作为Key值，剩余信息组成Value值；若一行记录代表右矩阵元素，则提取行号作为Key值，剩余信息组成Value值，如图3所示。之所以这样做，是为了下一步在Reduce过程中能够按照Key值统计左矩阵第Key列极其对应右矩阵第Key行中的元素。
统计与分段

当矩阵规模大到一定程度时，内存可能会碰到加载不了左矩阵的一列或右矩阵的一行元素的问题。为了提高矩阵相乘运算的可扩展性，本发明提出了对左矩阵元素按列进行分段，对右矩阵元素按行进行分段的方法，这样，单个计算节点就可以加载左矩阵的一段与右矩阵的一段至内存进行相乘运算，突破了内存的限制。分段相乘示意图如图4所示。

![](http://images.cnitblog.com/i/310680/201403/141543423587599.gif)

接下来我们结合图4来说明Reduce阶段如何来完成统计与分段工作。Reduce阶段首先将所有Key相同的Value集合在一起，形成一个Value-List。若Key为k，那么Value-List则代表了左矩阵第k列与右矩阵第k行的所有元素，这些元素时混合在一起的。在Reduce阶段，我们第一轮遍历Value-List,获得左矩阵第k列的元素个数为Mk，右矩阵第k行的元素个数为Nk。接下来我们通过第二轮遍历对左矩阵第k列、右矩阵第k行的元素进行分段操作，假设每个分段包含w个元素，则左矩阵第k列被分为段，右矩阵被分为段。

![](http://images.cnitblog.com/i/310680/201403/141545071163508.gif)

本发明将L矩阵中第k列中第i个分段表示成如下格式：

![](http://images.cnitblog.com/i/310680/201403/141545374032785.gif)

![](http://images.cnitblog.com/i/310680/201403/141546082747490.gif)表该分段在接下来的过程中总共需要![](http://images.cnitblog.com/i/310680/201403/141546157946657.gif) 个拷贝，element_list表示该分段中的元素集合。

同理，R矩阵中第k行中第j个分段表示成如下格式： 
 ![](http://images.cnitblog.com/i/310680/201403/141546472593775.gif)
 
为了便于后续Map-Reduce过程的处理，我们将每一个分段都存储在磁盘文件中，文件中的一样代表一个分段。同时，我们将两个矩阵中的具体分段信息存储在分布式缓存中，有利于解决后续步骤中不同节点间的通信与数据查询问题。具体存储格式如图5所示。

图5中![](http://images.cnitblog.com/i/310680/201403/141547304964301.gif) 代表矩阵L中第1列的元素个数为M1，每个分段的元素个数为w，所以对该列的分段数目为 $L_i^L$; 同理：
$ L_1^R = [m_1 / w]$ 表示矩阵R中第1行的元素个数为N1，每个分段的元素个数为w，所以对该行的分段数目为 $l_1^R$。

拷贝任务分发——Map迭代算法

l  Map迭代算法

如图4所示，我们需要将两个矩阵中的分段一一对应相乘。我们做如下举例：由于矩阵L中的第k列中的第i段需要与矩阵R中第k行中的所有段依次相乘，所以需要将L中第k列第i段的内容拷贝 [N1/ w]份；同理，R中第k行中的每一个分段需要拷贝 [M1 / w]份。当然，拷贝工作是通过Map-Reduce来完成的，现在的问题是，若两个矩阵中每一个分段需要拷贝的数量都很大，则一个Map对每行记录都需要执行好多遍拷贝工作，大大延长了Map执行的时间，同时，可能使得很多计算节点没有参与运算。

![](http://images.cnitblog.com/i/310680/201403/141551139247078.gif)

 为了解决上述问题，本发明提出了“Map迭代拷贝任务分发算法”来对每条记录（每个分段）的拷贝任务进行分发，这样有效的控制每个节点对每个分段的拷贝数量，同时更有效的使得更多的节点参与拷贝运算工作。

 为了便于每条数据知道自己需要拷贝的分数，我们对公式（1）、（2）进行简单的修改：
  
  ![](http://images.cnitblog.com/i/310680/201403/141552165262817.gif)
  
  式（3）中代表该记录（分段）需要拷贝$[N_k / w] - 1 + 1$份，拷贝标识号为1至$[N_k/w]$；同理解释式（4）。

这里，我们结合图7进行举例说明，假设…1#10000…是式（3）或式（4）的缩写形式，代表一条记录需要拷贝10000份，同时假设所有分段需拷贝的份数都是10000,那么初始时，将有N个节点参与拷贝工作。为了使得更多的计算节点参与拷贝工作，我们设计了此Map迭代拷贝任务分发算法。假设分发扩展率为10，则经过一次迭代后，文件大小扩大了约10倍，则大约有10*N个计算节点将参与拷贝工作，依次类推，三次迭代后，约有1000*N个计算节点参与拷贝工作，当有1000*N个节点参与拷贝工作时，每条记录被拷贝的最大份数为10，如图7所示。

  迭代次数控制

在现实大矩阵相乘中，由于大部分情况下矩阵都为稀疏矩阵，那么每行每列包含的元素个数就不一样，所以每个分段需拷贝的份数都不确定。这样我们就需要计算Map迭代过程的迭代次数，依次来控制Map迭代的过程。在此，我们利用图5所示存储在分布式缓存中的各个分段信息来得到最大分段数目，同时结合分发扩展率n，利用公式（5）来计算Map迭代的次数，依次来控制Map迭代过程。

![](http://images.cnitblog.com/i/310680/201403/141553396355026.gif)

最后计算模块

完成记录的拷贝工作后，我们还需要两轮Map-Reduce过程完成矩阵的运算。

l  第一轮Map-Reduce——分段拷贝与对应

在此轮中，我们首先在Map阶段完成分发到的拷贝任务，若图（7）中的….2191#2200…格式符合式（3），则其原本的形态为：
  ![](http://images.cnitblog.com/i/310680/201403/141554204681253.gif)
  

在Map中执行拷贝工作后记录样式为：

k−i−2191   element_list

k−i−2192   element_list

......

k−i−2199   element_list

k−i−2200   element_list

经过此轮Map阶段，每个key(拷贝后每条记录的前半部分)对应两个Value，也就是L矩阵中的一个段与R矩阵中的一个段，同一个key的两个Value将在该轮的Reduce阶段进行汇合，汇合后如下所示：

![](http://images.cnitblog.com/i/310680/201403/141554441107164.gif)

(L,k,i)代表L矩阵第k列第i个分段，(R,k,l)代表R矩阵第k行第j个分段，然后在下一轮Map-Reduce进行如图（4）所示的两个段的相乘工作。

###### 第二轮Map-Reduce——相乘并汇总

Map阶段对每条记录进行相乘运算，即将L中每个元素依次与R中每个元素相乘，若element_list_(L,k,i)中某个元素Li,k与element_list_(R,k,j)中某个元素Rk,j相乘，则结果记录成“i-j value”格式。 然后每个Map结束后执行combine操作，combine操作与该轮Reduce操作一样，执行相同key的value相加，便得到了最终的矩阵运算结果。



















































