几种特征选择方法以及机器学习的流程


[传送](http://chaoslog.com/te-zheng-xuan-ze.html)

[toc]


#### 概览图

![](http://7nj1qk.com1.z0.glb.clouddn.com/@/feature_engineering/intro/feature_engineering.jpg)

[传送](http://dataunion.org/20276.html)
##### 引言

在之前学习机器学习技术中，很少关注特征工程(Feature Engineering)，然而，单纯学习机器学习的算法流程，可能仍然不会使用这些算法，尤其是应用到实际问题的时候，常常不知道怎么提取特征来建模。

特征是机器学习系统的原材料，对最终模型的影响是毋庸置疑的。

##### 特征工程的重要意义

数据特征会直接影响你使用的预测模型和实现的预测结果。准备和选择的特征越好，则实现的结果越好。

影响预测结果好坏的因素：模型的选择、可用的数据、特征的提取。

优质的特征往往描述了数据的固有结构。

大多数模型都可以通过数据中良好的结构很好的学习，即使不是最优的模型，优质的特征也可以得到不错的效果。优质特征的灵活性可以让你使用简单的模型运算的更快，更容易理解，更容易维护。

优质的特征可以在使用不是最优的模型参数的情况下得到不错的预测结果，这样你就不必费力去选择最适合的模型和最优的参数了。

##### 特征工程定义

特征工程是将原始数据转化为特征，更好表示预测模型处理的实际问题，提升对于未知数据的准确性。它是用目标问题所在的特定领域知识或者自动化的方法来生成、提取、删减或者组合变化得到特征。


##### 特征工程的子问题

###### 1. 机器学习中的特征（Feature）

在机器学习和模式识别中，特征是在观测现象中的一种独立、可测量的属性。选择信息量大的、有差别性的、独立的特征是模式识别、分类和回归问题的关键一步。

最初的原始特征数据集可能太大，或者信息冗余，因此在机器学习的应用中，一个初始步骤就是选择特征的子集，或构建一套新的特征集，减少功能来促进算法的学习，提高泛化能力和可解释性。

在表格数据中，观测数据或实例（对应表格的一行）由不同的变量或者属性（表格的一列）构成，这里属性其实就是特征。但是与属性一词不同的是，特征是对于分析和解决问题有用、有意义的属性。

在机器视觉中，一幅图像是一个观测，但是特征可能是图中的一条线；在自然语言处理中，一个文本是一个观测，但是其中的段落或者词频可能才是一种特征；在语音识别中，一段语音是一个观测，但是一个词或者音素才是一种特征。

###### 2. 特征的重要性（Feature Importance）

你可以客观的评价特征的实用性。判别特征的重要性是对特征进行选择的预先指标，特征根据重要性被分配分数，然后根据分数不同进行排序，其中高分的特征被选择出来放入训练数据集。

如果与因变量（预测的事物）高度相关，则这个特征可能很重要，其中相关系数和独立变量方法是常用的方法。

在构建模型的过程中，一些复杂的预测模型会在算法内部进行特征重要性的评价和选择，如多元自适应回归样条法(Multivariate Adaptive Regression Splines， MARS)、随机森林(Random Forest)、梯度提升机(Gradient Boosted Machines)。这些模型在模型准备阶段会进行变量重要性的确定。

###### 3. 特征提取（Feature Extraction）

一些观测数据如果直接建模，其原始状态的数据太多。像图像、音频和文本数据，如果将其看做是表格数据，那么其中包含了数以千计的属性。
特征提取是自动地对原始观测降维，使其特征集合小到可以进行建模的过程。

对于表格式数据，可以使用主元素分析(Principal Component Analysis)、聚类等映射方法；对于图像数据，可以进行线(line)或边缘(edge)的提取；根据相应的领域，图像、视频和音频数据可以有很多数字信号处理的方法对其进行处理。

###### 4. 特征选择（Feature Selection）

不同的特征对模型的准确度的影响不同，有些特征与要解决的问题不相关，有些特征是冗余信息，这些特征都应该被移除掉。

特征选择是自动地选择出对于问题最重要的那些特征子集的过程。

特征选择算法可以使用评分的方法来进行排序；还有些方法通过反复试验来搜索出特征子集，自动地创建并评估模型以得到客观的、预测效果最好的特征子集；还有一些方法，将特征选择作为模型的附加功能，像逐步回归法(Stepwise regression) 就是一个在模型构建过程中自动进行特征选择的算法。

###### 5. 特征构建（Feature Construction）

特征重要性和选择是告诉使用者特征的客观特性，但这些工作之后，需要你人工进行特征的构建。

特征构建需要花费大量的时间对实际样本数据进行处理，思考数据的结构，和如何将特征数据输入给预测算法。

对于表格数据，特征构建意味着将特征进行混合或组合以得到新的特征，或通过对特征进行分解或切分来构造新的特征；对于文本数据，特征够自己按意味着设计出针对特定问题的文本指标；对于图像数据，这意味着自动过滤，得到相关的结构。

###### 6. 特征学习（Feature Learning）

特征学习是在原始数据中自动识别和使用特征。

现代深度学习方法在特征学习领域有很多成功案例，比如自编码器和受限玻尔兹曼机。它们以无监督或半监督的方式实现自动的学习抽象的特征表示（压缩形式），其结果用于支撑像语音识别、图像分类、物体识别和其他领域的先进成果。

抽象的特征表达可以自动得到，但是你无法理解和利用这些学习得到的结果，只有黑盒的方式才可以使用这些特征。你不可能轻易懂得如何创造和那些效果很好的特征相似或相异的特征。这个技能是很难的，但同时它也是很有魅力的，很重要的。
特征工程的流程

##### 机器学习中数据的转换过程：

 ###### 1. 选择数据：收集整合数据，将数据规划化为一个数据集
 ###### 2. 预处理数据：对数据进行清洗、格式化、采样
 ###### 3. 转换数据：特征工程所在
 ###### 4. 对数据建模：构建模型、评估模型、调整模型

##### 特征工程的迭代过程：

###### 1. 对特征进行头脑风暴：深入分析问题，观察数据特点，参考其他问题的有关特征工程的方法并应用到自己问题中
###### 2. 特征的设计：你可以自动提取特征，手动构造特征，或将两者相结合
###### 3. 特征选择：使用不同的特征重要性评分方法或特征选择方法
###### 4. 评估模型：利用所选择的特征对测试数据进行预测，评估模型准确性





##### 简介
特征选择(排序)对于数据科学家、机器学习从业者来说非常重要。好的特征选择能够提升模型的性能，更能帮助我们理解数据的特点、底层结构，这对进一步改善模型、算法都有着重要作用。

特征选择主要有两个功能：

1. 减少特征数量、降维，使模型泛化能力更强，减少过拟合
2. 增强对特征和特征值之间的理解

在许多机器学习相关的书里，很难找到关于特征选择的内容，因为特征选择要解决的问题往往被视为机器学习的一种副作用，一般不会单独拿出来讨论。

本文将结合Scikit-learn提供的例子介绍几种常用的特征选择方法，它们各自的优缺点和问题。

###### 一、去掉值变化小的特征  Removing features with low variance

这应该是最简单的特征选择方法了：假设某特征的特征值只有0和1，并且在所有输入样本中，95%的实例的该特征取值都是1，那就可以认为这个特征作用不大。如果100%都是1，那这个特征就没意义了。当特征值都是离散型变量的时候这种方法才能用，如果是连续型变量，就需要将连续变量离散化之后才能用，而且实际当中，一般不太会有95%以上都取某个值的特征存在，所以这种方法虽然简单但是不太好用。可以把它作为特征选择的预处理，先去掉那些取值变化小的特征，然后再从接下来提到的的特征选择方法中选择合适的进行进一步的特征选择。

##### 二、 单变量特征选择 Univariate feature selection

单变量特征选择能够对每一个特征进行测试，衡量该特征和响应变量之间的关系，根据得分扔掉不好的特征。对于回归和分类问题可以采用卡方检验等方式对特征进行测试。

这种方法比较简单，易于运行，易于理解，通常对于理解数据有较好的效果（但对特征优化、提高泛化能力来说不一定有效）；这种方法有许多改进的版本、变种。

###### 2.1 Pearson相关系数 Pearson Correlation

皮尔森相关系数是一种最简单的，能帮助理解特征和响应变量之间关系的方法，该方法衡量的是变量之间的线性相关性，结果的取值区间为[-1，1]，-1表示完全的负相关(这个变量下降，那个就会上升)，+1表示完全的正相关，0表示没有线性相关。

Pearson Correlation速度快、易于计算，经常在拿到数据(经过清洗和特征提取之后的)之后第一时间就执行。Scipy的pearsonr方法能够同时计算相关系数和p-value，

```python
import numpy as np
from scipy.stats import pearsonr

np.random.seed(0)
size = 300

x = np.random.normal(0,1,size)
print "Lower noise", pearsonr(x,x + np.random.normal(0,1,size))
print "Higher nosie", pearsonr(x,x + np.random.normal(0,10,size))

```
Lower noise (0.71824836862138386, 7.3240173129992273e-49)
Higher noise (0.057964292079338148, 0.31700993885324746)

这个例子中，我们比较了变量在加入噪音之前和之后的差异。当噪音比较小的时候，相关性很强，p-value很低。

Scikit-learn提供的f_regrssion方法能够批量计算特征的p-value，非常方便，参考sklearn的pipeline

Pearson相关系数的一个明显缺陷是，作为特征排序机制，他只对线性关系敏感。如果关系是非线性的，即便两个变量具有一一对应的关系，Pearson相关性也可能会接近0。

```python
x = np.random.uniform(-1,1,100000)
print pearsonr(x,x**2)[0]
```
更多类似的例子参考sample plots。另外，如果仅仅根据相关系数这个值来判断的话，有时候会具有很强的误导性，如Anscombe’s quartet，最好把数据可视化出来，以免得出错误的结论。

###### 2.2  互信息和最大信息系数 Mutual information and maximal information coefficient (MIC)

![](http://dataunion.org/wp-content/uploads/2015/04/4dbuAXe.png)

以上就是经典的互信息公式了。想把互信息直接用于特征选择其实不是太方便：1、它不属于度量方式，也没有办法归一化，在不同数据及上的结果无法做比较；2、对于连续变量的计算不是很方便（X和Y都是集合，x，y都是离散的取值），通常变量需要先离散化，而互信息的结果对离散化的方式很敏感。

最大信息系数克服了这两个问题。它首先寻找一种最优的离散化方式，然后把互信息取值转换成一种度量方式，取值区间在[0，1]。minepy提供了MIC功能。

反过头来看y=x^2这个例子，MIC算出来的互信息值为1(最大的取值)。

```python
from minepy import MINE

m = MINE()
x = np.random.uniform(-1,1,10000)
m.compute_score(x,x**2)
print m.mic()
```

MIC的统计能力遭到了一些质疑，当零假设不成立时，MIC的统计就会受到影响。在有的数据集上不存在这个问题，但有的数据集上就存在这个问题。

###### 2.3 距离相关系数(Distance correlation)

距离相关系数是为了克服Pearson相关系数的弱点而生的。在x和x^2这个例子中，即便Pearson相关系数是0，我们也不能断定这两个变量是独立的（有可能是非线性相关）；但如果距离相关系数是0，那么我们就可以说这两个变量是独立的。

R的energy包里提供了距离相关系数的实现，另外这是Python gist的实现。
```R
> x= runif(1000,-1,1)

> dcor(x,x**2)
[1] 0.4943864
```

尽管有MIC和距离相关系数在了，但当变量之间的关系接近线性相关的时候，Pearson相关系数仍然是不可替代的。第一、Pearson相关系数计算速度快，这在处理大规模数据的时候很重要。第二、Pearson相关系数的取值区间是[-1，1]，而MIC和距离相关系数都是[0，1]。这个特点使得Pearson相关系数能够表征更丰富的关系，符号表示关系的正负，绝对值能够表示强度。当然，Pearson相关性有效的前提是两个变量的变化关系是单调的。

###### 2.4 基于学习模型的特征排序 (Model based ranking)

这种方法的思路是直接使用你要用的机器学习算法，针对每个单独的特征和响应变量建立预测模型。其实Pearson相关系数等价于线性回归里的标准化回归系数。假如某个特征和响应变量之间的关系是非线性的，可以用基于树的方法（决策树、随机森林）、或者扩展的线性模型等。基于树的方法比较易于使用，因为他们对非线性关系的建模比较好，并且不需要太多的调试。但要注意过拟合问题，因此树的深度最好不要太大，再就是运用交叉验证。

在波士顿房价数据集上使用sklearn的随机森林回归给出一个单变量选择的例子：

```python
from sklearn.cross_validation import scorss_val_score, ShuffleSplit
from sklearn.datasets import load_boston
from sklearn.ensemble import RandomForestRegressor


# load boston housing dataset as an example

boston = load_boston()

X = boston["data"]
Y = boston["target"]

names = boston["feature_names"]

# 生成 随机森林model
rf = RandomForestRegressor(n_estiimators=20,max_depth=4)

scores = []

for i in range(X.shape[1]):
	score = cross_val_score(rf,X[:,i:i+1],Y,scoring="r2",
                         cv=ShuffleSplit(len(X),3,.3))
    scores.append((round(np.mean(score),3),names[i]))
    
print sorted(scores,reverse=True)

```
[(0.636, ‘LSTAT’), (0.59, ‘RM’), (0.472, ‘NOX’), (0.369, ‘INDUS’), (0.311, ‘PTRATIO’), (0.24, ‘TAX’), (0.24, ‘CRIM’), (0.185, ‘RAD’), (0.16, ‘ZN’), (0.087, ‘B’), (0.062, ‘DIS’), (0.036, ‘CHAS’), (0.027, ‘AGE’)]


##### 三、 线性模型和正则化

单变量特征选择方法独立的衡量每个特征与响应变量之间的关系，另一种主流的特征选择方法是基于机器学习模型的方法。有些机器学习方法本身就具有对特征进行打分的机制，或者很容易将其运用到特征选择任务中，例如回归模型，SVM，决策树，随机森林等等。说句题外话，这种方法好像在一些地方叫做wrapper类型，大概意思是说，特征排序模型和机器学习模型是耦盒在一起的，对应的非wrapper类型的特征选择方法叫做filter类型。

下面将介绍如何用回归模型的系数来选择特征。越是重要的特征在模型中对应的系数就会越大，而跟输出变量越是无关的特征对应的系数就会越接近于0。在噪音不多的数据上，或者是数据量远远大于特征数的数据上，如果特征之间相对来说是比较独立的，那么即便是运用最简单的线性回归模型也一样能取得非常好的效果。

```python
from sklearn.linear_model import LinearRegression
import numpy as np

np.random.seed(0)
size=5000

# a dataset with 3 feature
X = np.random.normal(0,1,(size,3))
# Y = X0 + 2*X1 + noise
Y = X[:,0] + 2 * X[:,1] + np.random.normal(0,2,size)
lr = LinearRegression()
lr.fit(X,Y)

# 优化输出
def pretty_print_linear(coefs,names=None,sort=False):
	if names == None:
    	names=["X%s" % x for x in range(len(coefs))]
    lst = zip(coefs,names)
    if sort:
    	lst = sorted(lst,key=lambda x:-np.abs(x[0]))
    return " + ".join("%s * %s" % (round(coef,3),name) for coef,name in lst)
    
print "Linear model",pretty_print_linear(lr.coef_)

```

Linear model: 0.984 * X0 + 1.995 * X1 + -0.041 * X2

在这个例子当中，尽管数据中存在一些噪音，但这种特征选择模型仍然能够很好的体现出数据的底层结构。当然这也是因为例子中的这个问题非常适合用线性模型来解：特征和响应变量之间全都是线性关系，并且特征之间均是独立的。

在很多实际的数据当中，往往存在多个互相关联的特征，这时候模型就会变得不稳定，数据中细微的变化就可能导致模型的巨大变化（模型的变化本质上是系数，或者叫参数，可以理解成W），这会让模型的预测变得困难，这种现象也称为多重共线性。例如，假设我们有个数据集，它的真实模型应该是Y=X1+X2，当我们观察的时候，发现Y’=X1+X2+e，e是噪音。如果X1和X2之间存在线性关系，例如X1约等于X2，这个时候由于噪音e的存在，我们学到的模型可能就不是Y=X1+X2了，有可能是Y=2X1，或者Y=-X1+3X2。

下边这个例子当中，在同一个数据上加入了一些噪音，用随机森林算法进行特征选择。

```python
from sklearn.linear_model import LinearRegression

size = 100

np.random.seed(seed=5)

X_seed = np.random.normal(0,1,size)
X1 = X_seed + np.random.normal(0,.1,size)
X2 = X_seed + np.random.normal(0,.1,size)
X3 = X_seed + np.random.normal(0,.1,size)

Y = X1 + X2 + X3 + np.random.normal(0,1,size)
X = np.array([X1,X2,X3]).T

lr = LinearRegression()
lr.fit(X,Y)

print "Linear model:", pretty_print_linear(lr.coef_)
```

系数之和接近3，基本上和上上个例子的结果一致，应该说学到的模型对于预测来说还是不错的。但是，如果从系数的字面意思上去解释特征的重要性的话，X3对于输出变量来说具有很强的正面影响，而X1具有负面影响，而实际上所有特征与输出变量之间的影响是均等的。

同样的方法和套路可以用到类似的线性模型上，比如逻辑回归。


###### 3.1 正则化模型

正则化就是把额外的约束或者惩罚项加到已有模型（损失函数）上，以防止过拟合并提高泛化能力。损失函数由原来的E(X,Y)变为E(X,Y)+alpha||w||，w是模型系数组成的向量（有些地方也叫参数parameter，coefficients），||·||一般是L1或者L2范数，alpha是一个可调的参数，控制着正则化的强度。当用在线性模型上时，L1正则化和L2正则化也称为Lasso和Ridge。

###### 3.2 L1 正则化/Lasso

L1正则化将系数w的l1范数作为惩罚项加到损失函数上，由于正则项非零，这就迫使那些弱的特征所对应的系数变成0。因此L1正则化往往会使学到的模型很稀疏（系数w经常为0），这个特性使得L1正则化成为一种很好的特征选择方法。

Scikit-learn为线性回归提供了Lasso，为分类提供了L1逻辑回归。

下面的例子在波士顿房价数据上运行了Lasso，其中参数alpha是通过grid search进行优化的。

```python
from sklearn.linear_model import Lasso
from sklearn.preprocessing import StandardScaler
from sklearn.datasets import load_boston

boston = load_boston()
scaler = StandardScaler()
X = scaler.fit_transform(bostom["data"])
Y = boston["target"]
names = boston["feature_names"]

lasso =  Lasso(alpha=.3)
lasso.fit(X,Y)

print "Lasso model: "，pretty_print_linear(lasso.coef_,names,sort=false)
```

Lasso model: -3.707 * LSTAT + 2.992 * RM + -1.757 * PTRATIO + -1.081 * DIS + -0.7 * NOX + 0.631 * B + 0.54 * CHAS + -0.236 * CRIM + 0.081 * ZN + -0.0 * INDUS + -0.0 * AGE + 0.0 * RAD + -0.0 * TAX

可以看到，很多特征的系数都是0。如果继续增加alpha的值，得到的模型就会越来越稀疏，即越来越多的特征系数会变成0。

然而，L1正则化像非正则化线性模型一样也是不稳定的，如果特征集合中具有相关联的特征，当数据发生细微变化时也有可能导致很大的模型差异。


###### 3.3 正则化/Ridge regression

L2正则化将系数向量的L2范数添加到了损失函数中。由于L2惩罚项中系数是二次方的，这使得L2和L1有着诸多差异，最明显的一点就是，L2正则化会让系数的取值变得平均。对于关联特征，这意味着他们能够获得更相近的对应系数。还是以Y=X1+X2为例，假设X1和X2具有很强的关联，如果用L1正则化，不论学到的模型是Y=X1+X2还是Y=2X1，惩罚都是一样的，都是2alpha。但是对于L2来说，第一个模型的惩罚项是2alpha，但第二个模型的是4*alpha。可以看出，系数之和为常数时，各系数相等时惩罚是最小的，所以才有了L2会让各个系数趋于相同的特点。

可以看出，L2正则化对于特征选择来说一种稳定的模型，不像L1正则化那样，系数会因为细微的数据变化而波动。所以L2正则化和L1正则化提供的价值是不同的，L2正则化对于特征理解来说更加有用：表示能力强的特征对应的系数是非零。

回过头来看看3个互相关联的特征的例子，分别以10个不同的种子随机初始化运行10次，来观察L1和L2正则化的稳定性。

```python
from sklearn.linear_model import Ridge
from sklearn.metrics import r2_socre

size = 100

# We run the method 10 times with different random seeds
for i in range(10):
	print "Random seed %s " % i
    np.random.seed(seed=i)
    X_seed = np.random.normal(0,1,size)
    X1 = X_seed + np.random.normal(0,.1,size)
    X2 = X_seed + np.random.normal(0,.1,size)
    X3 = X_seed + np.random.normal(0,.1,size)
    Y = X1 + X2 + X3 + np.random.normal(0,1,size)
    X = np.array([X1,X2,X3]).t
    
    
    lr = LinearRegression()
    lr.fit(X,Y)
    print "Linear model:" , pretty_print_linear(lr.coef_)
    
    ridge = Ridge(alpha=10)
    ridge.fit(X,Y)
    print "Ridge model:" ,pretty_print_linear(ridge.coef_)
    print
```

结果：
```shell
Random seed 0 Linear model: 0.728 * X0 + 2.309 * X1 + -0.082 * X2 Ridge model: 0.938 * X0 + 1.059 * X1 + 0.877 * X2

Random seed 1 Linear model: 1.152 * X0 + 2.366 * X1 + -0.599 * X2 Ridge model: 0.984 * X0 + 1.068 * X1 + 0.759 * X2

Random seed 2 Linear model: 0.697 * X0 + 0.322 * X1 + 2.086 * X2 Ridge model: 0.972 * X0 + 0.943 * X1 + 1.085 * X2

Random seed 3 Linear model: 0.287 * X0 + 1.254 * X1 + 1.491 * X2 Ridge model: 0.919 * X0 + 1.005 * X1 + 1.033 * X2

Random seed 4 Linear model: 0.187 * X0 + 0.772 * X1 + 2.189 * X2 Ridge model: 0.964 * X0 + 0.982 * X1 + 1.098 * X2

Random seed 5 Linear model: -1.291 * X0 + 1.591 * X1 + 2.747 * X2 Ridge model: 0.758 * X0 + 1.011 * X1 + 1.139 * X2

Random seed 6 Linear model: 1.199 * X0 + -0.031 * X1 + 1.915 * X2 Ridge model: 1.016 * X0 + 0.89 * X1 + 1.091 * X2

Random seed 7 Linear model: 1.474 * X0 + 1.762 * X1 + -0.151 * X2 Ridge model: 1.018 * X0 + 1.039 * X1 + 0.901 * X2

Random seed 8 Linear model: 0.084 * X0 + 1.88 * X1 + 1.107 * X2 Ridge model: 0.907 * X0 + 1.071 * X1 + 1.008 * X2

Random seed 9 Linear model: 0.714 * X0 + 0.776 * X1 + 1.364 * X2 Ridge model: 0.896 * X0 + 0.903 * X1 + 0.98 * X2
```

可以看出，不同的数据上线性回归得到的模型（系数）相差甚远，但对于L2正则化模型来说，结果中的系数非常的稳定，差别较小，都比较接近于1，能够反映出数据的内在结构。

##### 四、随机森林

随机森林具有准确率高、鲁棒性好、易于使用等优点，这使得它成为了目前最流行的机器学习算法之一。随机森林提供了两种特征选择的方法：mean decrease impurity和mean decrease accuracy。

###### 4.1 平均不纯度减少 mean decrease impurity

随机森林由多个决策树构成。决策树中的每一个节点都是关于某个特征的条件，为的是将数据集按照不同的响应变量一分为二。利用不纯度可以确定节点（最优条件），对于分类问题，通常采用基尼不纯度或者信息增益，对于回归问题，通常采用的是方差或者最小二乘拟合。当训练决策树的时候，可以计算出每个特征减少了多少树的不纯度。对于一个决策树森林来说，可以算出每个特征平均减少了多少不纯度，并把它平均减少的不纯度作为特征选择的值。

下边的例子是sklearn中基于随机森林的特征重要度度量方法：

```python
from sklearn.datasets import load_boston
from sklearn.ensemble import RandomFroestRegressor
import numpy as np

# Load boston housing dataset as an example

boston = load_boston()
X=boston["data"]
Y=boston["target"]

names = bostom["fearure_names"]
rf=RandomForestRegressor()
fr.fit(X,Y)
print "Features sorted by their score:"
print sorted(zip(map(lambda x: round(x,4),rf.feature_importtances_),
names),reverse=True)

```
Features sorted by their score: [(0.5298, ‘LSTAT’), (0.4116, ‘RM’), (0.0252, ‘DIS’), (0.0172, ‘CRIM’), (0.0065, ‘NOX’), (0.0035, ‘PTRATIO’), (0.0021, ‘TAX’), (0.0017, ‘AGE’), (0.0012, ‘B’), (0.0008, ‘INDUS’), (0.0004, ‘RAD’), (0.0001, ‘CHAS’), (0.0, ‘ZN’)]


这里特征得分实际上采用的是Gini Importance。使用基于不纯度的方法的时候，要记住：1、这种方法存在偏向，对具有更多类别的变量会更有利；2、对于存在关联的多个特征，其中任意一个都可以作为指示器（优秀的特征），并且一旦某个特征被选择之后，其他特征的重要度就会急剧下降，因为不纯度已经被选中的那个特征降下来了，其他的特征就很难再降低那么多不纯度了，这样一来，只有先被选中的那个特征重要度很高，其他的关联特征重要度往往较低。在理解数据时，这就会造成误解，导致错误的认为先被选中的特征是很重要的，而其余的特征是不重要的，但实际上这些特征对响应变量的作用确实非常接近的（这跟Lasso是很像的）。

特征随机选择方法稍微缓解了这个问题，但总的来说并没有完全解决。下面的例子中，X0、X1、X2是三个互相关联的变量，在没有噪音的情况下，输出变量是三者之和。

```python
size = 10000

np.random.seed(seed=10)

X_seed = np.random.normal(0,1,size)

X0 = X_seed + np.random.normal(0,.1,size)
X1 = X_seed + np.random.normal(0,.1,size)
X2 = X_seed + np.random.normal(0,.1,size)
X = np.array([X0,X1,X0]).T
Y = X0 + X1 + X2

rf_2 = RandomForestRegressior(n_estimators=20,max_features=2)

rf_2.fit(X,Y)

print "Scores for X0,X1,X2:" ,map(lambda x: round(x,3),rf_2.feature_importances_)
```

Scores for X0, X1, X2: [0.278, 0.66, 0.062]

当计算特征重要性时，可以看到X1的重要度比X2的重要度要高出10倍，但实际上他们真正的重要度是一样的。尽管数据量已经很大且没有噪音，且用了20棵树来做随机选择，但这个问题还是会存在。

需要注意的一点是，关联特征的打分存在不稳定的现象，这不仅仅是随机森林特有的，大多数基于模型的特征选择方法都存在这个问题。

###### 4.2 平均精确度减少  Mean decrease accuracy

另一种常用的特征选择方法就是直接度量每个特征对模型精确率的影响。主要思路是打乱每个特征的特征值顺序，并且度量顺序变动对模型的精确率的影响。很明显，对于不重要的变量来说，打乱顺序对模型的精确率影响不会太大，但是对于重要的变量来说，打乱顺序就会降低模型的精确率。

这个方法sklearn中没有直接提供，但是很容易实现，下面继续在波士顿房价数据集上进行实现。

```python
from sklearn.cross_validation import ShuffleSplit
from sklearn.metrics import r2_score
from collections import defaultdict

X = boston["data"]
Y = boston["target"]

rf = RandomForestRegressor()
scores = defaultdict(list)

# crossvalidate the scores on a number of different random splits of the data

for train_idx ,test_idx in ShuffleSplit(len(x),100,.3):
	X_train,X_test = X[train_idx],X[test_idx]
    Y_train,Y_test = Y[train_idx],Y[test_idx]
    
    r = rf.fit(X_train,Y_train)
    acc = r2_score(Y_test,rf.predict(X_test))
    for i in range(X.shape[1]):
    	X_t = X_test.copy()
        np.random.shuffle(X_t[:,i])
        shuff_acc = r2_score(Y_test,rf.predict(X_t))
        scores[names[i]].append((acc - shuff_acc)/acc)

print "Features sorted by their score:"
print sorted([(round(np.mean(score),4),feat) for 
       feat,score in socres.items()],reverse=Ture)

```

在这个例子当中，LSTAT和RM这两个特征对模型的性能有着很大的影响，打乱这两个特征的特征值使得模型的性能下降了73%和57%。注意，尽管这些我们是在所有特征上进行了训练得到了模型，然后才得到了每个特征的重要性测试，这并不意味着我们扔掉某个或者某些重要特征后模型的性能就一定会下降很多，因为即便某个特征删掉之后，其关联特征一样可以发挥作用，让模型性能基本上不变。

##### 五、 两种顶层特征选择算法
之所以叫做顶层，是因为他们都是建立在 **基于模型的特征选择方法** 基础之上的，例如回归和SVM，在不同的子集上建立模型，然后汇总最终确定特征得分。

###### 5.1 稳定性选择 Stability selection

稳定性选择是一种基于二次抽样和选择算法相结合较新的方法，选择算法可以是回归、SVM或其他类似的方法。它的主要思想是在不同的数据子集和特征子集上运行特征选择算法，不断的重复，最终汇总特征选择结果，比如可以统计某个特征被认为是重要特征的频率（被选为重要特征的次数除以它所在的子集被测试的次数）。理想情况下，重要特征的得分会接近100%。稍微弱一点的特征得分会是非0的数，而最无用的特征得分将会接近于0。

sklearn在随机lasso和随机逻辑回归中有对稳定性选择的实现。

```python
from sklearn.linear_model import RandaomizedLasso
from sklearn.datasets import load_boston

boston = load_boston()

# using the Boston housing data
# Data gets sacled automatically by sklearn`s implementation

X = boston["data"]
Y = boston["target"]
names = boston["feature_names"]

rlasso = RandomizedLasso(alpha=0.025)
rlasso.fit(X,Y)

print "feature sorted by their score:"
print scored(zip(map(lambda x: round(x,4),rlasso.scores_),names),\
         reverse=True)
         
```
Features sorted by their score: [(1.0, ‘RM’), (1.0, ‘PTRATIO’), (1.0, ‘LSTAT’), (0.62, ‘CHAS’), (0.595, ‘B’), (0.39, ‘TAX’), (0.385, ‘CRIM’), (0.25, ‘DIS’), (0.22, ‘NOX’), (0.125, ‘INDUS’), (0.045, ‘ZN’), (0.02, ‘RAD’), (0.015, ‘AGE’)]

在上边这个例子当中，最高的3个特征得分是1.0，这表示他们总会被选作有用的特征（当然，得分会收到正则化参数alpha的影响，但是sklearn的随机lasso能够自动选择最优的alpha）。接下来的几个特征得分就开始下降，但是下降的不是特别急剧，这跟纯lasso的方法和随机森林的结果不一样。能够看出稳定性选择对于克服过拟合和对数据理解来说都是有帮助的：总的来说，好的特征不会因为有相似的特征、关联特征而得分为0，这跟Lasso是不同的。对于特征选择任务，在许多数据集和环境下，稳定性选择往往是性能最好的方法之一。


###### 5.2 递归特征消除  Recursive feature elimination (RFE)

递归特征消除的主要思想是反复的构建模型（如SVM或者回归模型）然后选出最好的（或者最差的）的特征（可以根据系数来选），把选出来的特征放到一遍，然后在剩余的特征上重复这个过程，直到所有特征都遍历了。这个过程中特征被消除的次序就是特征的排序。因此，这是一种寻找最优特征子集的贪心算法。

RFE的稳定性很大程度上取决于在迭代的时候底层用哪种模型。例如，假如RFE采用的普通的回归，没有经过正则化的回归是不稳定的，那么RFE就是不稳定的；假如采用的是Ridge，而用Ridge正则化的回归是稳定的，那么RFE就是稳定的。

Sklearn提供了RFE包，可以用于特征消除，还提供了RFECV，可以通过交叉验证来对的特征进行排序。

```python
from sklearn.feature_selection import RFE
from sklearn.linear_model import LinearRegression

boston = load_boston()
X = boston["data"]
Y = boston["target"]
names = boston["feature_names"]

# use linear regression as the model
lr = LinearRegression()
# rank all features i.e continue the elimination until the last one
rfe = RFE(lr,n_features_to_select=1)
rfe.fit(X,Y)

print "Features sorted by their rank:"
print sorted(zip(map(lambda x: round(x,4),rfe.ranking_),names))
```
Features sorted by their rank: [(1.0, ‘NOX’), (2.0, ‘RM’), (3.0, ‘CHAS’), (4.0, ‘PTRATIO’), (5.0, ‘DIS’), (6.0, ‘LSTAT’), (7.0, ‘RAD’), (8.0, ‘CRIM’), (9.0, ‘INDUS’), (10.0, ‘ZN’), (11.0, ‘TAX’), (12.0, ‘B’), (13.0, ‘AGE’)]

##### 六、 一个完整的例子

下面将本文所有提到的方法进行实验对比，数据集采用Friedman #1 回归数据（这篇论文中的数据）。数据是用这个公式产生的：

![](http://dataunion.org/wp-content/uploads/2015/04/ERGNG8c.png)

X1到X5是由单变量分布生成的，e是标准正态变量N(0,1)。另外，原始的数据集中含有5个噪音变量 X5,…,X10，跟响应变量是独立的。我们增加了4个额外的变量X11,…X14，分别是X1,…,X4的关联变量，通过f(x)=x+N(0,0.01)生成，这将产生大于0.999的关联系数。这样生成的数据能够体现出不同的特征排序方法应对关联特征时的表现。

接下来将会在上述数据上运行所有的特征选择方法，并且将每种方法给出的得分进行归一化，让取值都落在0-1之间。对于RFE来说，由于它给出的是顺序而不是得分，我们将最好的5个的得分定为1，其他的特征的得分均匀的分布在0-1之间。

```python
from sklearn.datasets import load_boston
from sklearn.linear_model import (LinearRegression,Ridge,\
          Lasso,RandomizedLasso)
from sklearn.feature_selection import REF,f_regression
from sklearn.preprocessing import MinMaxScaler
from sklearn.ensemble import RandomForestRegressor
import numpy as np
from minepy import MINE

np.random.seed(0)

size = 750
X = np.random.uniform(0,1,(size,14))

# "Friedam #1" regressiion problem
Y = (10 * np.sin(np.pi*X[:,0]*X[:,1]) + 20 * (X[:,2] - .5)**2 +\
   10 * X[:,3] + 5 *X[:,4] +  np.random.normal(0,1))
   
# add 3 additional correlated variables (correlated with X1-X3)

X[:,10:] = X[:,:4] + np.random.normal(0,.025,(size,4))

names = ["x%s" % i for i in range(1,15)]

ranks = {}

def rank_to_dict(ranks,names,order=1):
	minmax = MinMaxScaler()
    ranks = minmax.fit_transform(order*np.array([ranks]).T).T[0]
    ranks = map(lambda x:round(x,2),ranks)
    return dict(zip(names,ranks))
    
lr = LinearRegression(normal=True)
lr.fit(X,Y)
ranks["Linear reg"] = rank_to_dict(np.abs(lr.coef_),names)

ridge = Ridge(alpha=7)
ridge.fit(X,Y)
ranks["Ridge"] = rank_to_dict(np.abs(ridge.coef_),names)

lasso = Lasso(alpha=.05)
lasso.fit(X,Y)
ranks["Lasso"] = rank_to_dict(np.abs(lasso.coef_),names)

rlasso = RandomizedLasso(alpha=0.04)
rlasso.fit(X,Y)
ranks["Stability"] = rank_to_dict(np.abs(rlasso.socres_),names)


# stop the search when 5 feature are left (they will get equal scores)
rfe = RFE(lr,n_features_to_select=5)
rfe.fit(X,Y)
ranks["RFE"] = rank_to_dict(map(float,rfe.ranking_),names,oreder=-1)

rf = RandomForestRegressor()
rf.fit(X,Y)
ranks["RF"] = rank_to_dict(rf.feature_imortances_,names)


f,pval = f_regression(X,Y,center=True)
ranks["corr."] = rank_to_dict(f,names)

mine = MINE()
mic_scored = []

for i in range(X.shape[1]):
	mine.compute_score(X[:,I],Y)
    m = mine.mic()
    mic_scores.append(m)
    
ranks["MIC"] = rank_to_dict(mic_scores,names)

r = {}

for name in names:
	r[name] = round(np.mean([ranks[method][name] \
        for method in ranks.keys()]),2)
        
methods = sorted(ranks.keys())
ranks["Mean"] = r
methods.append("Mean")

print "\t%s" % "\t".join(methods)

for name in names:
	print "%s\t%s" % (name,"\t".join(map(str, \
        [ranks[method][name] for method in methods])))
        
```
![](http://dataunion.org/wp-content/uploads/2015/04/5ybQKSP.png)

从以上结果中可以找到一些有趣的发现：

特征之间存在线性关联关系，每个特征都是独立评价的，因此X1,…X4的得分和X11,…X14的得分非常接近，而噪音特征X5,…,X10正如预期的那样和响应变量之间几乎没有关系。由于变量X3是二次的，因此X3和响应变量之间看不出有关系（除了MIC之外，其他方法都找不到关系）。这种方法能够衡量出特征和响应变量之间的线性关系，但若想选出优质特征来提升模型的泛化能力，这种方法就不是特别给力了，因为所有的优质特征都不可避免的会被挑出来两次。

**Lasso**能够挑出一些优质特征，同时让其他特征的系数趋于0。当如需要减少特征数的时候它很有用，但是对于数据理解来说不是很好用。（例如在结果表中，X11,X12,X13的得分都是0，好像他们跟输出变量之间没有很强的联系，但实际上不是这样的）

**MIC**对特征一视同仁，这一点上和关联系数有点像，另外，它能够找出X3和响应变量之间的非线性关系。

**随机森林**基于不纯度的排序结果非常鲜明，在得分最高的几个特征之后的特征，得分急剧的下降。从表中可以看到，得分第三的特征比第一的小4倍。而其他的特征选择算法就没有下降的这么剧烈。

**Ridge**将回归系数均匀的分摊到各个关联变量上，从表中可以看出，X11,…,X14和X1,…,X4的得分非常接近。

**稳定性选择**常常是一种既能够有助于理解数据又能够挑出优质特征的这种选择，在结果表中就能很好的看出。像Lasso一样，它能找到那些性能比较好的特征（X1，X2，X4，X5），同时，与这些特征关联度很强的变量也得到了较高的得分。

##### 七、 总结

1. 对于理解数据、数据的结构、特点来说，单变量特征选择是个非常好的选择。尽管可以用它对特征进行排序来优化模型，但由于它不能发现冗余（例如假如一个特征子集，其中的特征之间具有很强的关联，那么从中选择最优的特征时就很难考虑到冗余的问题）。

2. 正则化的线性模型对于特征理解和特征选择来说是非常强大的工具。L1正则化能够生成稀疏的模型，对于选择特征子集来说非常有用；相比起L1正则化，L2正则化的表现更加稳定，由于有用的特征往往对应系数非零，因此L2正则化对于数据的理解来说很合适。由于响应变量和特征之间往往是非线性关系，可以采用basis expansion的方式将特征转换到一个更加合适的空间当中，在此基础上再考虑运用简单的线性模型。

3. 随机森林是一种非常流行的特征选择方法，它易于使用，一般不需要feature engineering、调参等繁琐的步骤，并且很多工具包都提供了平均不纯度下降方法。它的两个主要问题，1是重要的特征有可能得分很低（关联特征问题），2是这种方法对特征变量类别多的特征越有利（偏向问题）。尽管如此，这种方法仍然非常值得在你的应用中试一试。

4. 特征选择在很多机器学习和数据挖掘场景中都是非常有用的。在使用的时候要弄清楚自己的目标是什么，然后找到哪种方法适用于自己的任务。当选择最优特征以提升模型性能的时候，可以采用交叉验证的方法来验证某种方法是否比其他方法要好。当用特征选择的方法来理解数据的时候要留心，特征选择模型的稳定性非常重要，稳定性差的模型很容易就会导致错误的结论。对数据进行二次采样然后在子集上运行特征选择算法能够有所帮助，如果在各个子集上的结果是一致的，那就可以说在这个数据集上得出来的结论是可信的，可以用这种特征选择模型的结果来理解数据。


##### Tips

什么是卡方检验？用方差来衡量某个观测频率和理论频率之间差异性的方法

什么是皮尔森卡方检验？这是一种最常用的卡方检验方法，它有两个用途：1是计算某个变量对某种分布的拟合程度，2是根据两个观测变量的Contingency table来计算这两个变量是否是独立的。主要有三个步骤：第一步用方差和的方式来计算观测频率和理论频率之间卡方值；第二步算出卡方检验的自由度（行数-1乘以列数-1）；第三步比较卡方值和对应自由度的卡方分布，判断显著性。

什么是p-value？简单地说，p-value就是为了验证假设和实际之间一致性的统计学意义的值，即假设检验。有些地方叫右尾概率，根据卡方值和自由度可以算出一个固定的p-value，

什么是响应变量(response value)？简单地说，模型的输入叫做explanatroy variables，模型的输出叫做response variables，其实就是要验证该特征对结果造成了什么样的影响

什么是统计能力(statistical power)?

什么是度量(metric)?

什么是零假设(null hypothesis)?在相关性检验中，一般会取“两者之间无关联”作为零假设，而在独立性检验中，一般会取“两者之间是独立”作为零假设。与零假设相对的是备择假设（对立假设），即希望证明是正确的另一种可能。

什么是多重共线性？

什么是grid search？

##### References

    http://blog.datadive.net/selecting-good-features-part-i-univariate-selection/
    http://blog.datadive.net/selecting-good-features-part-ii-linear-models-and-regularization/
    http://scikit-learn.org/stable/modules/feature_selection.html#univariate-feature-selection
    http://www.quora.com/What-are-some-feature-selection-methods
    http://www.quora.com/What-are-some-feature-selection-algorithms
    http://www.quora.com/What-are-some-feature-selection-methods-for-SVMs
    http://www.quora.com/What-is-the-difference-between-principal-component-analysis-PCA-and-feature-selection-in-machine-learning-Is-PCA-a-means-of-feature-selection


##### 八、应用机器学习的流程

###### 8.1  绘制预测准确率在训练集 和 测试集 以及不同训练数据上的表现，显示模型的优劣
```python
import time
import numpy as np

np.random.seed(0)

import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.learning_curve import learning_curve

def plot_learning_curve(estimator,title,x,ylim=None,cv=None,cv=None,train_sizes=np.linespace(.1,1.0,5)):
	"""
    Generate a simple plot of the test and train learning curve
    
    parameters
    ---------------------
    estimator:object type that implements the “fit” and “predict” methods An object of that type which is cloned for each validation.
    title : string
	Title for the chart.

	x : array-like, shape(n_samples, n_features)
	Training vector, where n_samples is the number of samples and n_features is the number of features

	y : array-like, shape (n_samples) or (n_samples, n_features)
	Target relative to X for classification or regression;
	None for unsupervised learning.

	ylim : tuple, shape(ymin, ymax), optional
	Defines minimum and maximum yvalues plotted.

	cv : integer, cross-validation generator, optional
	If an integer is passed, it is the number of folds (defaults to 3).
	Specific cross-validation objects can be passed, see sklearn.cross_validation module for the list of possible objects
    """
    plt.figure()
    train_sizes,train_scores,test_scores = learning_curve(estimator,X,y,cv=5,n_jobs=1,train_sizes=train_sizes)
    train_scores_mean = np.mean(train_scores, axis = 1)
	train_scores_std = np.std(train_scores, axis = 1)
	test_scores_mean = np.mean(test_scores, axis = 1)
	test_scores_std = np.std(test_scores, axis = 1)
    
    plt.fill_between(train_sizes, train_scores_mean – train_scores_std, train_scores_mean + train_scores_std, alpha = 0.1, color = “r”)
	plt.fill_between(train_sizes, test_scores_mean – test_scores_std, test_scores_mean + test_scores_std, alpha = 0.1, color = “g”)
	plt.plot(train_szies, train_scores_mean, ‘o-’, color = “r”, label = “Training score”)
	plt.plot(train_szies, test_scores_mean, ‘o-’, color = “g”, label = “Cross-validation score”)
	plt.xlabel(“Training examples”)
	plt.ylabel(“Score”)
	plt.legend(loc=”best”)
	plt.grid(“on”)
	if ylim:
		plt.ylim(ylim)
		plt.title(title)
```

###### 8.2 产生玩具数据集

我们使用sklearn的make_classification函数来生成一些简单的玩具数据：

```python
from sklearn.datasets import make_classification
X, y = make_classification(1000, n_features=20, n_informative=2, 
                           n_redundant=2, n_classes=2, random_state=0)

from pandas import DataFrame
df = DataFrame(np.hstack((X, y[:, None])), 
               columns = range(20) + ["class"])
               
```
注意到我们为二分类生成了一个数据集，这个数据集包括1000个数据点，每个特征20维。我们已经使用pandas的DataFrame类把数据和类别封装到一个共同的数据结构中。我们来看一看前5个数据点：

```shell
0 1 2 3 4 5 6 7 8 9 ... 11 12 13 14 15 16 17 18 19 class
0 -1.063780 0.676409 1.069356 -0.217580 0.460215 -0.399167 -0.079188 1.209385 -0.785315 -0.172186 ... -0.993119 0.306935 0.064058 -1.054233 -0.527496 -0.074183 -0.355628 1.057214 -0.902592 0
1 0.070848 -1.695281 2.449449 -0.530494 -0.932962 2.865204 2.435729 -1.618500 1.300717 0.348402 ... 0.225324 0.605563 -0.192101 -0.068027 0.971681 -1.792048 0.017083 -0.375669 -0.623236 1
2 0.940284 -0.492146 0.677956 -0.227754 1.401753 1.231653 -0.777464 0.015616 1.331713 1.084773 ... -0.050120 0.948386 -0.173428 -0.477672 0.760896 1.001158 -0.069464 1.359046 -1.189590 1
3 -0.299517 0.759890 0.182803 -1.550233 0.338218 0.363241 -2.100525 -0.438068 -0.166393 -0.340835 ... 1.178724 2.831480 0.142414 -0.202819 2.405715 0.313305 0.404356 -0.287546 -2.847803 1
4 -2.630627 0.231034 0.042463 0.478851 1.546742 1.637956 -1.532072 -0.734445 0.465855 0.473836 ... -1.061194 -0.888880 1.238409 -0.572829 -1.275339 1.003007 -0.477128 0.098536 0.527804 0
```

通过直接查看原始特征值，我们很难获得该问题的任何线索，即使在这个低维的例子中。因此，有很多的提供数据的更容易视图的方法；其中的小部分将在接下来的部分中讨论。

###### 8.3 可视化

当你接到一个新的问题，第一步几乎都是可视化，也就是说，观察你的数据。

Seaborn是一个不错的统计数据可视化包。我们使用它的一些函数来探索数据。

第一步是使用pairplot生成散点图和直方图。两种颜色对应了两个类别，我们使用了特征的一个子集、仅仅使用前50个数据点来简化问题。

```python
_ = sns.pairplot(df[:50],vars=[8,11,12,14,19],hue="class" ,size=1.5)
```
![](http://dataunion.org/wp-content/uploads/2015/04/6941baebgw1eqtpnsxt9vj20g40es77f.jpg)

基于该直方图(主对角线上)，我们可以看到一些特征比其他特征对分类更有用。特别地，特征11和14看起来有丰富的信息量。这两个特征的散点图显示类别在二维空间中几乎是线性可分的。要更加注意的是，特征12和19是高度负相关的。我们可以通过使用corrplot更系统地探索相关性：


```python
plt.figure(figsize=(12, 10))
_ = sns.corrplot(df, annot=False)
```
![](http://dataunion.org/wp-content/uploads/2015/04/6941baebgw1eqtpnsaaijj20hi0fat9l.jpg)

我们可以发现我们之前的观察结果在这里得到了确认：特征11和14与类强相关（他们有丰富的信息量）。更进一步，特征12和特征19强负相关，特征19和特征14强相关。因此，有一些特征是冗余的。这对于有些分类器可能会出现问题，比如，朴素贝叶斯，它假设所有的特征都是独立的。剩下的特征大部分都是噪声，他们既不相互关联，也不和类别相关。

注意到如果特征维数较大、数据点较少的时候，数据可视化会变得更有挑战性。我们在后面会给出一个高维数据可视化的例子。

###### 8.4 方法的选择
一旦我们已经使用可视化方法对数据进行了探索，我们就可以开始应用机器学习了。机器学习方法数量众多，通常很难决定先尝试哪种方法。这个简单的备忘单（归功于Andreas Müller和sklearn团队）可以帮助你为你的问题选择一个合适的机器学习方法（供选择的备忘录见
```python
from IPython.display import Image
Image(filename='ml_map.png', width=800, height=600)
```
![](http://dataunion.org/wp-content/uploads/2015/04/6941baebgw1eqtpnrsfwnj21kw0zgn9e1.jpg)

我们有了1000个样本，要预测一个类别，并且有了标签，那么备忘单推荐我们首先使用LinearSVC（LinearSVC代表线性核的支持向量分类，并且对于这类特殊问题使用一个有效的算法）。所有我们做了个试验。LinearSVC需要选择正则化；我们使用标准L2范数惩罚和C=10.我们分别画出训练分数和验证分数的学习曲线（这个例子中分数代表准确率）：
```python
from sklearn.svm import LinearSVC
plot_learning_curve(LinearSVC(C=10.0), "LinearSVC(C=10.0)",
X, y, ylim=(0.8, 1.01),
train_sizes=np.linspace(.05, 0.2, 5))
```
![](http://dataunion.org/wp-content/uploads/2015/04/6941baebgw1eqtpnqqm57j20b107vgm1.jpg)

**我们可以注意到训练数据和交叉验证数据的错误率有很大的差距。这意味什么？我们可能过度拟合训练数据了！**

###### 8.5 解决过拟合
有很多方法来减少过拟合：

    增加训练样本数

（获得更多的数据是机器学习从业者的共同愿望）

```python
plot_learning_curve(LinearSVC(C=10.0), "LinearSVC(C=10.0)",
                    X, y, ylim=(0.8, 1.1),
                    train_sizes=np.linspace(.1, 1.0, 5))
```
![](http://dataunion.org/wp-content/uploads/2015/04/6941baebgw1eqtpnq85jgj20b207vt96.jpg)

可以看到当训练数据增加时，验证分数越来越大，差距越来越小；因此现在不再过拟合了。有很多获得更多数据的方法，比如（a）可以尽力投资收集更多数据，（b）基于现有数据创造一些人为的数据（比如图像旋转，平移，扭曲），或者（c）加入人工噪声。

如果以上的这些方法都不可行，就不可能获得更多的数据，我们或者可以


   **减少特征的维数**

（从我们可视化中可以知道，特征11和14是信息量最大的）
```python
plot_learning_curve(LinearSVC(C=10.0), "LinearSVC(C=10.0) Features: 11&14", X[:, [11, 14]], y, ylim=(0.8, 1.0), train_sizes=np.linspace(.05, 0.2, 5))
```

![](http://dataunion.org/wp-content/uploads/2015/04/6941baebgw1eqtpnpicavj20b107vjrr.jpg)

因为我们是手动的挑选特征，而且在比我们给分类器更多的数据上，这有一点作弊的意味。我们可以使用自动挑选特征：

```python
from sklearn.pipeline import Pipeline  # 用于链接特征选择和 训练过程（函数）
from sklearn.feature_selection import SelectKBest, f_classif
# SelectKBest(f_classif, k=2) will select the k=2 best features according to their Anova F-value

plot_learning_curve(Pipeline([("fs", SelectKBest(f_classif, k=2)), # 选择2个特征
                               ("svc", LinearSVC(C=10.0))]) #  链接特征选择,
                    "SelectKBest(f_classif, k=2) + LinearSVC(C=10.0)",
                    X, y, ylim=(0.8, 1.0),
                    train_sizes=np.linspace(.05, 0.2, 5))
```
![](http://dataunion.org/wp-content/uploads/2015/04/6941baebgw1eqtpnp1849j20b107vdg8.jpg)

这样做效果非常好。在这个toy数据集上，特征选择是简单的。应该注意到特征选择只是减少模型复杂度的一个特殊种类。其他的方法是：
* （a）减少线性回归多项式模型的次数，
* （b）减少人工神经网络节点的个数/层数，
* （c）增加RBF核的带宽等等。

仍然有一个问题：为什么分类器不能自动的识别有用的特征？首先让我们转向另一种选择，来减少过拟合：
* （d）增加分类器的正则化
（减少线性SVC的C的系数）
```python
plot_learning_curve(LinearSVC(C=0.1), "LinearSVC(C=0.1)", 
                    X, y, ylim=(0.8, 1.0),
                    train_sizes=np.linspace(.05, 0.2, 5))
```

![](http://dataunion.org/wp-content/uploads/2015/04/6941baebgw1eqtpnoqurej20b107vjrt.jpg)

这已经有一点点作用了。我们也可以使用基于交叉验证的网格搜索自动地挑选分类器的正则化：

```python
from sklearn.grid_search import GridSearchCV
est = GridSearchCV(LinearSVC(), 
                   param_grid={"C": [0.001, 0.01, 0.1, 1.0, 10.0]})
plot_learning_curve(est, "LinearSVC(C=AUTO)", 
                    X, y, ylim=(0.8, 1.0),
                    train_sizes=np.linspace(.05, 0.2, 5))
print "Chosen parameter on 100 datapoints: %s" % est.fit(X[:100], y[:100]).best_params_
```
在100个数据点上选择参数：{‘C’: 0.01}
![](http://dataunion.org/wp-content/uploads/2015/04/6941baebgw1eqtpnno3z9j20b107vaaj.jpg)

一般说来，特征选择似乎更好。分类器可以自动识别有用的特征吗？回想一下，LinearSVC还支持L1范数惩罚，这产生了一个稀疏的解决方案。稀疏解决方案对应一个隐式的特征选择。让我们来试试这个：
```python
plot_learning_curve(LinearSVC(C=0.1, penalty='l1', dual=False), 
                    "LinearSVC(C=0.1, penalty='l1')", 
                    X, y, ylim=(0.8, 1.0),
                    train_sizes=np.linspace(.05, 0.2, 5))
```
![](http://dataunion.org/wp-content/uploads/2015/04/6941baebgw1eqtpnn38f4j20b107vwev.jpg)

这看起来也很好。让我们来探讨学到的系数：

```python
est = LinearSVC(C=0.1, penalty='l1', dual=False)
est.fit(X[:150], y[:150])  # fit on 150 datapoints
print "Coefficients learned: %s" % est.coef_
print "Non-zero coefficients: %s" % np.nonzero(est.coef_)[1]
```
结果：
```shell
Coefficients learned: [[ 0. 0. 0. 0. 0. 0.01857999
0. 0. 0. 0.004135 0. 1.05241369
0.01971419 0. 0. 0. 0. -0.05665314
0.14106505 0. ]]
Non-zero coefficients: [ 5 9 11 12 17 18]
```
大部分系数是0（对应的特征被忽略），并且目前最大的权重在特征11上。

###### 8.6 不同的数据集 (欠拟合)
我们生成另外一个二分类的数据集，并且再次应用LinearSVC。
```python
from sklearn.datasets import make_circles
X, y = make_circles(n_samples=1000, random_state=2)
```

```python
plot_learning_curve(LinearSVC(C=0.25), "LinearSVC(C=0.25)", 
                    X, y, ylim=(0.5, 1.0),
                    train_sizes=np.linspace(.1, 1.0, 5))
```

![](http://dataunion.org/wp-content/uploads/2015/04/6941baebgw1eqtpnmf83lj20aw07v74n.jpg)

啊，这非常糟糕，甚至训练误差都不如随机误差。这个可能的原因是什么？难道上面的所有方法（更多数据，特征选择，增加正则化）都不奏效了吗？

结果是：No。我们处在一个完全不同的情况：以前，训练分数一直接近完美，我们不得不解决过拟合。这次，训练误差也非常低。是欠拟合。让我们来看一看数据：

```python
df = DataFrame(np.hstack((X, y[:, None])), 
               columns = range(2) + ["class"])
_ = sns.pairplot(df, vars=[0, 1], hue="class", size=3.5)
```
![](http://dataunion.org/wp-content/uploads/2015/04/6941baebgw1eqtpnlwc0mj20fc0dsmys.jpg)

这些数据显然不是线性可分的；更多的数据或者更少的特征没有用了。我们的模型错了；因此欠拟合。

###### 8.7 解决欠拟合

减少欠拟合的方法：

 1. 使用更多或更好的特征（到原点的距离应该有用！）
 
	```python
	# add squared distance from origin as third feature
	X_extra = np.hstack((X, X[:, [0]]**2 + X[:, [1]]**2))

	plot_learning_curve(LinearSVC(C=0.25), "LinearSVC(C=0.25) + distance feature", 
                    X_extra, y, ylim=(0.5, 1.0),
                    train_sizes=np.linspace(.1, 1.0, 5))	
	```
![](http://dataunion.org/wp-content/uploads/2015/04/6941baebgw1eqtpnl5bylj20aw07vjro.jpg)
非常好！但是我们必须要花一些心思来想出这些特征。或许分类器可以自动的做到这些？这需要

 2. 使用更复杂的模型
 （减少正则化或使用非线性核）
 ```python
 from sklearn.svm import SVC
# note: we use the original X without the extra feature
plot_learning_curve(SVC(C=2.5, kernel="rbf", gamma=1.0),
                    "SVC(C=2.5, kernel='rbf', gamma=1.0)",
                    X, y, ylim=(0.5, 1.0), 
                    train_sizes=np.linspace(.1, 1.0, 5))
 ```
 ![](http://dataunion.org/wp-content/uploads/2015/04/6941baebgw1eqtpnkl5hdj20aw07vmxg.jpg)
 是的，这也可以达到满意的效果！
 
###### 8.8 更大的数据集和更高维的特征空间

回到原始的数据集上，但是这次有更多的特征和样本，并且有5类。LinearSVC在这样大小的数据集上会有一点慢；备忘单上建议使用SGDClassifier。这个分类器学习到一个线性模型（就像LinearSVC或logistic回归），但是它在训练中使用随机梯度下降（就像反向传播的人工神经网络一样）。

SGDClassifier允许小批量扫描数据，这对于数据量太大不能放到内存中时有帮助。**交叉验证和这项技术不兼容**；**使用逐步验证代替**：这里，估计器总是在训练数据集的下一块上进行测试（在用它进行训练之前）。训练之后，会再次进行测试来检查它适应数据的能力。

```python
X, y = make_classification(200000, n_features=200, n_informative=25, 
                           n_redundant=0, n_classes=10, class_sep=2,
                           random_state=0)
```

```python
from sklearn.linear_model import SGDClassifier
est = SGDClassifier(penalty="l2", alpha=0.001)
progressive_validation_score = []
train_score = []
for datapoint in range(0, 199000, 1000):
    X_batch = X[datapoint:datapoint+1000]
    y_batch = y[datapoint:datapoint+1000]
    if datapoint > 0:
        progressive_validation_score.append(est.score(X_batch, y_batch))
    est.partial_fit(X_batch, y_batch, classes=range(10))
    if datapoint > 0:
        train_score.append(est.score(X_batch, y_batch))

plt.plot(train_score, label="train score")
plt.plot(progressive_validation_score, label="progressive validation score")
plt.xlabel("Mini-batch")
plt.ylabel("Score")
plt.legend(loc='best')
```

![](http://dataunion.org/wp-content/uploads/2015/04/6941baebgw1eqtpnk8lh8j20b107m3z2.jpg)

这个图告诉我们，在50个mini-batches的数据之后，我们已经不能再提高验证数据了，因此可以停止训练了。由于训练分数不是很高，我们可能是欠拟合而不是过拟合。要是使用rbf核测试一下就更好了，但是SGDClassifier很不幸的不兼容核技巧。替代方法是可以使用一个多层的感知机，它也可以使用随机梯度下降进行训练，但是一个非线性模型，或者像备忘单建议的，使用核近似法。

现在在一个机器学习中使用的经典的解决光学字符识别的数据集上：

```python
from sklearn.datasets import load_digits
digits = load_digits(n_class=6)
X = digits.data
y = digits.target
n_samples, n_features = X.shape
print "Dataset consist of %d samples with %d features each" % (n_samples, n_features)

# Plot images of the digits
n_img_per_row = 20
img = np.zeros((10 * n_img_per_row, 10 * n_img_per_row))
for i in range(n_img_per_row):
    ix = 10 * i + 1
    for j in range(n_img_per_row):
        iy = 10 * j + 1
        img[ix:ix + 8, iy:iy + 8] = X[i * n_img_per_row + j].reshape((8, 8))

plt.imshow(img, cmap=plt.cm.binary)
plt.xticks([])
plt.yticks([])
_ = plt.title('A selection from the 8*8=64-dimensional digits dataset')
```
由1083个样本组成的数据集，每个样本由64个特征组成
![](http://dataunion.org/wp-content/uploads/2015/04/6941baebgw1eqtpnjcb8pj208f06zt9k.jpg)

因此我们有1083个手写数字（0，1，2，3，4，5）样本，每一个样本由8*8的4bit像素（0，16）灰度图片组成。因此特征的维数适中（64）；但是，这64维空间的可视化是非常重要的。我们来说明不同的减少维数（至二维）方法，基于http://scikit-learn.org/stable/auto_examples/manifold/plot_lle_digits.html#example-manifold-plot-lle-digits-py

```python
from matplotlib import offsetbox
def plot_embedding(X, title=None):
    x_min, x_max = np.min(X, 0), np.max(X, 0)
    X = (X - x_min) / (x_max - x_min)

    plt.figure(figsize=(10, 10))
    ax = plt.subplot(111)
    for i in range(X.shape[0]):
        plt.text(X[i, 0], X[i, 1], str(digits.target[i]),
                 color=plt.cm.Set1(y[i] / 10.),
                 fontdict={'weight': 'bold', 'size': 12})

    if hasattr(offsetbox, 'AnnotationBbox'):
        # only print thumbnails with matplotlib > 1.0
        shown_images = np.array([[1., 1.]])  # just something big
        for i in range(digits.data.shape[0]):
            dist = np.sum((X[i] - shown_images) ** 2, 1)
            if np.min(dist) < 4e-3:
                # don't show points that are too close
                continue
            shown_images = np.r_[shown_images, [X[i]]]
            imagebox = offsetbox.AnnotationBbox(
                offsetbox.OffsetImage(digits.images[i], cmap=plt.cm.gray_r),
                X[i])
            ax.add_artist(imagebox)
    plt.xticks([]), plt.yticks([])
    if title is not None:
        plt.title(title)
```
已经随机投影的二维数据的结果不是太差：
```python
from sklearn import (manifold, decomposition, random_projection)
rp = random_projection.SparseRandomProjection(n_components=2, random_state=42)
stime = time.time()
X_projected = rp.fit_transform(X)
plot_embedding(X_projected, "Random Projection of the digits (time: %.3fs)" % (time.time() - stime))
```

![](http://dataunion.org/wp-content/uploads/2015/04/6941baebgw1eqtpnisa7lj20g20gdtcn.jpg)

然而，有一个很著名的方法一般来说应该适合，也就是PCA（使用TruncatedSVD来实现，不需要构建协方差矩阵）：

```python
X_pca = decomposition.TruncatedSVD(n_components=2).fit_transform(X)
stime = time.time()
plot_embedding(X_pca,
               "Principal Components projection of the digits (time: %.3fs)" % (time.time() - stime))
```

![](http://dataunion.org/wp-content/uploads/2015/04/6941baebgw1eqtpnic9dlj20g20gd78d.jpg)

PCA给出一个更好的结果，而且在这个数据集上甚至更快。通过允许64维输入空间到二维目标空间的非线性变换，我们可以得到更好的结果。这有很多种方法；我们这里只介绍一种方法：t-SNE。

```python
tsne = manifold.TSNE(n_components=2, init='pca', random_state=0)
stime = time.time()
X_tsne = tsne.fit_transform(X)
plot_embedding(X_tsne,
               "t-SNE embedding of the digits (time: %.3fs)" % (time.time() - stime))
```
![](http://dataunion.org/wp-content/uploads/2015/04/6941baebgw1eqtpnhrmwej20g20gdn00.jpg)

这是一个非常优秀的嵌入，也表明只使用一个分类器完美地分开这些类是可能的（详见例子http://scikit-learn.org/stable/auto_examples/plot_digits_classification.html）。t-SNE唯一的不足是它需要更多的时间来计算，因此不适用于大数据集（在目前的条件下）

###### 8.9 损失函数的选择
损失函数的选择也非常重要。下面是不同损失函数的说明：
```python
# adapted from http://scikit-learn.org/stable/auto_examples/linear_model/plot_sgd_loss_functions.html
xmin, xmax = -4, 4
xx = np.linspace(xmin, xmax, 100)
plt.plot([xmin, 0, 0, xmax], [1, 1, 0, 0], 'k-',
         label="Zero-one loss")
plt.plot(xx, np.where(xx < 1, 1 - xx, 0), 'g-',
         label="Hinge loss")
plt.plot(xx, np.log2(1 + np.exp(-xx)), 'r-',
         label="Log loss")
plt.plot(xx, np.exp(-xx), 'c-',
         label="Exponential loss")
plt.plot(xx, -np.minimum(xx, 0), 'm-',
         label="Perceptron loss")
# the balanced relative margin machine
#R = 2
#plt.plot(xx, np.where(xx < 1, 1 - xx, (np.where(xx > R, xx-R,0))), 'b-',
#         label="L1 Balanced Relative Margin Loss")
plt.ylim((0, 8))
plt.legend(loc="upper right")
plt.xlabel(r"Decision function $f(x)$")
plt.ylabel("$L(y, f(x))$")
```

不同的损失函数有不同的优势：

 1. 0-1损失是在分类问题中你实际上需要的。不幸地是，这是非凸优化问题，由于最优化问题会变得或多或少的不好解决，因此并不实用。
 2.   合页损失（使用支持向量分类）导出一个在数据中稀疏的解（由于$f(x) > 1$，它变为0），而且对离群点比较稳健（由于$f(x)to-infty$，它仅仅成线性增长）。它不提供充分的校准的概率。
 3.   对数损失函数（比如，在逻辑回归中使用）导出很好的概率校准。因此，如果你不仅得到二值预测，还可以得出结果的概率，这个损失函数是一个很好的选择。缺点是，它的解在数据空间中是不稀疏的，它比合页损失函数更容易受到离群点的影响。
 4.   指数损失函数（在Adaboost中使用）非常容易受离群点的影响（由于当$f(x)to-infty$时它快速增加）。它主要适用于Adaboost中，因为它在一个简单有效的boosting算法中有效果。
 5.   感知器损失函数基本上是合页损失函数的移动版本。合页损失函数也惩罚非常接近边界但是在正确一边的点（间隔最大化准则）。另一方面，感知器损失函数只要数据点在边界正确的一边就可以，如果数据是线性可分就使得边界待定，导致比间隔最大化更差的泛化性。

###### 8.10 ROC 函数的绘制

为了绘制ROC曲线， 分类器必须提供每个样例被判定为阳性 或者 阴性的可信成都。 尽管大多数分类器都可以做到这一点， 但是在通常情况下，这些值会在输出离散分类标签之前被清除。

朴素贝叶斯能够提供一个可能性， 而在Logistic回归中输入到Sigmoid函数中的一个数值，在Adaboost和 SVM中， 都会计算出一个数值然后输入到sign（）函数中，所有的这些值都可以用于衡量给定分类器的预测强度， 味蕾绘制ROC曲线，首先要将分类样例按照其预测强度排序。先从排名最低的样例开始，所有排名更低的样例都被判为反例，而所有排名更高的样例都被判为正例，该情况的对应点为<1.0,1.0>。然后，将其移到排名次的样例中去，如果该样例属于正例，那么对真阳率进行修改；如果该样例属于反例，那么假阳律进行修改。

下面是ROC曲线的绘制以及AUC计数的函数

```python

def plotROC(predStrengths,classLabels):
	'''
    predStrengths : 分类器的预测强度， 是输入sign 函数之前的值
    classLabels   : 训练数据的实际标签 
    '''
	import matplotlib.pyplot as plt
    cur = (1.0,1.0)
    ySum = 0.0
    numPosClas = sum(array(classLabels) == 1.0)
    yStep = 1/float(numPosClas)    # 正样本的个数
    xStep = 1/float(len(classLabels) - numPosClas) # 负阳本的个数
    sortedIndicies = predStrengths.argsort()       # 根据预测强度进行坐标排序
    fig = plt.figure()
    fig.clf()
    ax = plt.subplot(111)
    for index in sortedIndicies.tolist()[0]:
    	if classLabels[index] == 1.0:
        	delX = 0
            delY = yStep
        else:
        	delX = xStep
            delY = 0
            ySum += cur[1]
        ax.plot([cur[0],cur[0] - delX],[cur[1] ,cur[1]- delY],c='b')
        cur = (cur[0] - delX,cur[1] - delY)
    ax.plot([0,1],[0,1],'b--')
    plt.xlabel("False Postitive Rate")
    plt.ylabel("True Positive Rate")
    plt.title("ROC curve for classifier")
    ax.axis([0,1,0,1])
    
    plt.show()
    print "the Area under the Curve is: ",ySum * sStep
```

函数的第二个输入参数是之前使用过的classLabels ，表示 训练集的真正标签， 然后构建一个浮点数二元组， 并将它初始化为（1.0，1.0） ， 该元组保存的是绘制光标的位置， 变量ySum用于计算浮点数AUC的值， 接下来，通过数组过滤的方式计算正例的数目， 并将该值赋给numPosClas，该值显示确定了在y坐标轴上的进步数目， 接着我们在X轴和Y轴的0.0 到 1.0 区间上绘制点， 因此y轴上的步长是 1.0/numPosClas , 类似的就可以得到x轴的步长了。

下面我们得到了排序索引， 但是这些索引是按照最小到最大的顺序排列的， 所以我们需要从点<1.0,1.0>开始绘制， 一直到<0,0>，跟着的三行代码是用于构建画笔， 并在所有排序值上进行循环， 这些只在一个Numpy数组或者矩阵中进行排序， Python则需要一个表来进行迭代循环，所以我们需要用tolist方法， 当遍历表时， 每得到一个表签为1.0的类，则沿着y轴的方向下降一个步长， 即不断降低真阳率， 类似的，对于每个其他类别的标签， 则是在x轴的方向上倒退了一个步长，即不断降低真阳率， 上述代码只关注1这个类别标签， 因此也就无所谓是采用1/0标签还是 +1/-1标签

为了计算AUC，我们需要对多个小矩形的面积进行叠加， 这些小矩形的宽度是xStep，因此可以先对所有的矩形的高度进行累加， 最后再乘以xStep 得到其总面积。所有高度的和ySum 随着x轴的每次移动而渐次增加， 一旦决定了是在x轴还是y轴方向上进行移动， 我们就可以在当前点和新点之间划出一条线段， 然后当前点cur更新了，最后我们就会得到一个像样的绘图函数并将AUC 打印到终端输出 


###### 8.11 交叉验证

* 下面是一个 机器学习实战中的岭回归测试中的交叉验证的例子：

```python

def crossValidation(xArr,yArr,numVal=10):
	m = len(yArr)
    indexList = range(m)
    errorMat - zeros((numVal,30))
    for i in range(numVal):    # 交叉验证次数
    	trainX=[]
        trainY=[]
        testX=[]
        testY=[]
        random.shuffle(indexList)     # 每次验证 都 混淆一次整个数据集的索引
        for j in range(m):
        	if j < m*0.9:             # 小于 指定比例 为训练集
            	trainX.append(xArr[indexList[j]])
                trainY.append(yArr[indexList[j]])
            else:                     # 构建测试集
            	testX.append(xArr[indexList[i]])
                testY.append(yArr[indexList[j]])
                
        wMat = ridgeTest(trainX,trainY)
        for k in range(30):
        	matTextX = mat(testX)
            matTrainX = mat(trainX)
            meanTrian = mean(matTrainX,0)
            varTrain = var(matTrainX,0)
            matTextX = (matTestX - meanTrain) / varTrain
            yEst = matTestX * mat(wMat[k,:]).T + mean(trainY)
            errorMat[i,k] = rssError(yEst.T.A,array(testY))
   meanErrors = mean(errorMat,0)
   minMean = float(min(meanErrors))
   bestWeights = wMat[nonzero(meanErrors==minMean)]
   xMat = mat(xArr)
   yMat = mat(yArr).T
   meanX = mean(xMat,0)
   varX = var(xMat,0)
   
   unReg = bestWeights / varX
   print "the best model from Ridge Regression is: \n",unReg
   print "with constant term: ",\
   		-1*sum(multipy(meanX,unReg)) + mean(yMat)
   
   
```

*  scikit learn 中的 交叉验证框架

 1. 划分数据集
 
 ```python
 import numpy as np
 from sklearn import cross_validation
 from sklearn import datasets
 from sklearn import svm
 
 iris = datasets.load_iris()
 
 iris.data.shape
 iris.target.shape
 
  X_train,y_train,X_test,y_test =  \          cross_validation.train_test_split(iris.data,iris.target,test_size=0.4,random_state=0)
       
 clf = svm.SVC(kernel="linear",C=1).fit(X_train,y_train)

 clf.score(X_test,y_test)
       
 ```
 
 * K-fold
 
 ```python
 clf = svm.SVC(kernel="linear",C=1)
 scores = cross_validation.cross_val_score(clf,iris.data,iris.target,cv=5)
 
 print("Accuary: %0.2f (+/- %0.2f)" % (scores.mean ,socres.std()*2))
 ```
 
 * 指定评分方法
 
 ```python
 from sklearn import metrics
 scores = cross_validation.cross_val_score(clf,iris.data,iris.target,cv=5,scoring="f1_weighted")
 
 scores
 ```
 
 * 指定CV方案
 
 ```python
 n_samples = iris.data.shape[0]
 cv = cross_validation.ShuffleSplit(n_samples,n_iters=3,
 test_size=0.3,random_state=0)
 
 cross_validation.cross_val_score(clf,iris.data,iris.target,cv=cv)
 
 ```
 
 * 划分 训练集和测试集
 
 ```python
 from sklearn import preprocessing
 
 X_train,X_test,y_train,y_test = cross_validation.train_test_split(iris.data,iris.target,text_size=0.4,random_state = 0)
 
 scaler = preprocessing.StanderdScaler().fit(X_train)
 X_train_transformed = scaler.transform(X_train)
 clf = svm.SVC(c=1).fit(X_train_transformed,y_train)
 X_test_transformed = scaler.tranform(X_test)
 
 clf.score(X_test_transformed,y_test)
 ```
 
 使用k-fold方法进行
 
 ```python
 X_folds = np.array_split(X_digits,3)
 y_folds = np.array_split(y_digits,3)
 
 scores = list()
 
 for k in range(3):
 	X_train = list(X_folds)
    X_test  = X_train.pop(k)
    X_train = np.concatenate(X_train)
    y_train = list(y_folds)
    y_test = y_train.pop(k)
    y_train = np.concatenate(y_train)
    
    scores.append(svc.fit(X_train,y_train).score(X_test,y_test))
    
 print(scores)
 ```






###### 8.12 总结
以上我们讨论了一些怎么让机器学习在一个新的问题上工作起来的建议。我们考虑了分类问题，回归和聚类问题也与之类似。然而，专注于人工数据集（为了便于理解）还有点过于简单化。在很多实际问题中，数据的收集、组织、预处理是极重要的。请参见本文中data wrangling的例子。Pandas是这方面很好的工具。

很多应用领域也有具体要求，也有符合这些要求的工具，比如：

    使用skimage图片处理
    使用pySPACE的生物信号分析和一般时间序列处理
    使用pandas处理财务数据

我们不详细探索这些领域；然而，寻找一个好的预处理流程往往比选择一个合适的分类器需要付出更大的努力。我们可以通过一个例子初识一个中等复杂的信号处理流程，该例中使用pySPACE在脑电波数据中检测特定事件相关电位：

https://github.com/pyspace/pyspace/blob/master/docs/examples/specs/node_chains/ref_P300_flow.yaml

信号处理流程包含数据标准化，抽取，带通滤波，降维（xDAWN是一个监督的降维方法），特征提取（局部直线特征），和特征标准化。下图给出了pySPACE中分类之前可用的流程各部分的一个概貌。


```python
Image(filename='algorithm_types_detailed.png', width=800, height=600)
```


  ![](http://dataunion.org/wp-content/uploads/2015/04/6941baebgw1eqtpnh5t3tj20kk0aqtby.jpg)


机器学习的一个长远目标，也是深度学习领域的追求，是可以学习大部分这样的流程，而不是手工编写它们。




##### 评价逻辑回归模型优劣的两个重要参数AIC和BIC 

[传送](http://blog.sciencenet.cn/home.php?mod=space&uid=74956&do=blog&id=489160)



赤池信息量准则，即Akaike information criterion、简称AIC，是衡量统计模型拟合优良性的一种标准，是由日本统计学家赤池弘次创立和发展的。赤池信息量准则建立在熵的概念基础上，可以权衡所估计模型的复杂度和此模型拟合数据的优良性。
优先考虑的模型应是AIC值最小的那一个。

贝叶斯信息准则，BIC＝ Bayesian Information Criterions

The log likelihood of the model is the value that is maximized by the process that computes the maximum likelihood value for the Bi parameters. 

The Deviance is equal to -2*log-likelihood.

Akaike’s Information Criterion (AIC) is -2*log-likelihood+2*k where k is the number of estimated parameters. 

```shell
	AIC = -2 * log-likelihood + 2 * k 
    			k : 是估计的参数的个数
```

The Bayesian Information Criterion (BIC) is -2*log-likelihood + k*log(n) where k is the number of estimated parameters and n is the sample size.  The Bayesian Information Criterion is also known as the Schwartz criterion.

```shell
  BIC = -2 * log-likelihood + k *log(n)
  		
        	k: 估计的参数的个数
            n: 样本的个数
```

DTREG和最新版的SPSS都可以直接给出


#### 九、 特征hash (Feature Hashing)

[传送](http://breezedeus.github.io/2014/11/20/breezedeus-feature-hashing.html)
##### 1. 介绍

在特征处理（Feature Processing）中我介绍了利用笛卡尔乘积的方法来构造组合特征。这种方法虽然简单，但麻烦的是会使得特征数量爆炸式增长。比如一个可以取N个不同值的类别特征，与一个可以去M个不同值的类别特征做笛卡尔乘积，就能构造出N*M个组合特征。

特征太多这个问题在具有个性化的问题里尤为突出。如果把用户id看成一个类别特征，那么它可以取的值的数量就等于用户数。把这个用户id特征与其他特征做笛卡尔积，就能产生庞大的特征集。做广告算法的公司经常宣称自己模型里有几十上百亿的特征，基本都是这么搞出来的。

当然，特征数量多的问题自古有之，目前也已经有很多用于降维的方法。比如聚类、PCA等都是常用的降维方法1。但这类方法在特征量和样本量很多的时候本身就计算量很大，所以对大问题也基本无能为力。

本文介绍一种很简单的降维方法——特征哈希（Feature Hashing）法

> 特征哈希法的目标是把原始的高维特征向量压缩成较低维特征向量，且尽量不损失原始特征的表达能力。

记哈希前的特征向量为 $x \in \sf{R}^N$,我们要把这个原始的N维特征向量压缩成M维（M < N）。

记 $h(n): \{1, …, N\} \rightarrow \{1, …, M\}$ 为一个选定的均匀哈希函数，
$\xi(n): \{1, …, N\} \rightarrow \{-1, 1\}$ , 为另一个选定的均匀哈希函数。h(n)和ξ(n)是独立选取的,它们没关系。按下面方式计算哈希后的M维新特征向量
$\phi \in \sf{R}^M$ 的第i个元素值 (（ϕ是依赖于x的，所以有时候也把ϕ写成ϕ(x)）:

$$ \phi_i = \sum_{j:\ h(j)=i} \xi(j) \ x_j \ \ \text{。}$$

可以证明，按上面的方式生成的新特征ϕ在概率意义下保留了原始特征空间的内积，以及距离2：

$$% <![CDATA[

\begin{eqnarray}
	x^T x' &\approx& \phi^T \phi' \ \ \text{，} \\
	\parallel x - x' \parallel &\approx& \parallel \phi - \phi' \parallel \ \ \text{，}
\end{eqnarray}
 %]]> $$

其中x和x′为两个原始特征向量，而ϕ和ϕ′为对应的哈希后的特征向量.

利用上面的哈希方法把x转变成ϕ后，就可以直接把ϕ用于机器学习算法了。这就是利用特征哈希法来降低特征数量的整个过程。需要说的是，这里面的两个哈希函数h和ξ并不要求非要是把整数哈希成整数，其实它们只要能把原始特征均匀哈希到新特征向量上就行。例如在NLP里，每个特征代表一个单词，那么只要保证h和ξ把单词均匀哈希到{1,…,M}和{−1,1}就行。

下面具体说明如何把特征哈希法应用于多任务学习（multitask learning）问题。所谓多任务学习，就是同时求解多个问题。个性化问题就是一种典型的多任务学习问题，它同时学习多个用户的兴趣偏好。

在世纪佳缘我们使用Logistic Regression (LogReg) 模型学习每个男性用户的交友兴趣以便预测他给具有某些特征的女性的发信概率。这时候学习一个男性用户的交友兴趣就是一个学习任务。记男性用户集合为U，抽取出的女性特征维度为d。我们为每个(女性？)用户u∈U学习一组参数 $w_u \in \sf{R}^d$ 。再加上一组全局参数w0，总共有 $N \triangleq d \cdot (1+\mid \bf{U} \mid)$  个参数。这种表达方式就是把男性用户id与所有特征x做了笛卡尔积。下图给出了一个有3个用户且x长度为2时扩展后各个用户对应特征向量的示例图。LogReg模型通过计算
$(w_0 + w_u)^T x$ 来获得最终的预测概率值。


![](http://breezedeus.github.io/images/feature_hashing1.png)
这个问题也可以转化到特征哈希后的空间来看。我们为每个用户引入一个不同的转换函数 $\phi_u(x)$ 。一般取 $\phi_u(x)=\phi((u, x))$ 即可。
那么用户u对应的扩展向量通过哈希转换后为

$$ x^h_u \triangleq \phi_0(x)+\phi_u(x) \ \ \text{；}$$

扩展向量对应的权重参数 $[w_0^T, \ldots, w_{\mid \bf{U} \mid}^T] $ 通过哈希转换后为
  $$ w^h \triangleq \phi_0(w_0) + \sum_{u \in \bf{U}} \phi_u(w_u) \ \ \text{。} $$
  
那么在哈希转换后的空间里，

$$  % <![CDATA[

\begin{aligned}
(w^h)^T x^h_u \ &= \left(\phi_0(w_0) + \sum_{u \in \bf{U}} \phi_u(w_u)\right)^T \left(\phi_0(x)+\phi_u(x)\right) \\
&\approx \phi_0(w_0)^T \phi_0(x) + \phi_u(w_u)^T \phi_u(x) \\
&\approx w_0^T x + w_u^T x \\
&= (w_0 + w_u)^T x  \ \ \text{。}
\end{aligned}
 %]]>$$


这从理论上证明了特征哈希可用于此多任务学习问题。 上面公式中第一个近似等式利用了不同任务之间哈希转换后的参数 $\phi_u(w_u)$ 和 $\phi_{u’}(x)$:

$$\phi_u(w_u)^T \phi_{u'}(x) \approx 0 , \ \forall u \neq u' \ \ \text{。} $$


具体实现算法时，我们并不需要关心 $w^h$, 只需要把原始特征x通过哈希转换成$x^h_u $, 即可。剩下的就是标准机器学习流程了。

特征哈希法可以降低特征数量，从而加速算法训练与预测过程，以及降低内存消耗；但代价是通过哈希转换后学习的模型变得很难检验，我们很难对训练出的模型参数做出合理解释。特征哈希法的另一个问题是它会把多个原始特征哈希到相同的位置上，出现哈希里的collision现象。但实际实验表明这种collision对算法的精度影响很小3。

最后，总结下特征哈希法相对于其他机器学习降维算法的优势：

* 实现简单，所需额外计算量小；
* 可以添加新的任务（如新用户），或者新的原始特征而保持哈希转换后的特征长度不变，很适合任务数频繁变化的问题（如个性化推荐里新用户，新item的出现）；
* 可以保持原始特征的稀疏性，既然哈希转换时只有非0原始特征才起作用；
* 可以只哈希转换其中的一部分原始特征，而保留另一部分原始特征（如那些出现collision就会很影响精度的重要特征）。


#### Hashing Trick 的伪代码实现

本博客已经迁往http://www.kemaswill.com/, 博客园这边也会继续更新, 欢迎关注~
在机器学习领域, kernel trick是一种非常有效的比较两个样本(对象)的方法. 给定两个对象 
$x_i, x_j \in \mathcal{X}$ , 用 $ k(x_i, x_j) :=\left <\phi(x_i), \phi(x_j)\right>$ 来比较两个对象的特征 $\phi(x_i), \phi(x_j) $ 的内积, 大大减少计算时间. 但是Weinberger[1]等人提出, 在实际中, 尤其是文本分类领域, 原始的输入空间几乎是线性可分的, 但是, 训练集太大, 特征维度太高. 在这种情况下, 没必要把输入向量映射到一个高维的特征空间. 相反的, 有限的内存可能存不下核矩阵. 为此, Langford[2], Qinfeng Shi[3]等人提出了hashing trick, 把高维的输入向量哈希到一个低维的特征空间
$ \mathbb{R}^m$

##### 1、 Hashing Trick

最简单的hashing trick是将原始的每个特征名(或者特征索引)hash到一个低维向量的索引上, 然后将该特征的值累加到该低维向量的索引上:

$$ \bar{\phi}_j(x) = \sum_{i\in \mathcal{J}; h(i) = j}\phi_i(x)$$

其中 $ \phi(x) \in \mathbb{R}^{\mathcal{J}}$ 为原始的输入向量, $h: \mathcal{J} \to {1,..,n} $ n为哈希函数. 算法伪代码为:

```shell
 function hashing_vectorizer(features:array of string,N:integer):
 	x := new vector[N]
    for f in features:
    	h := hash(f)       # f 是特征名，也可以是特征的索引
        x[h mod N] += 1    # 此处累加的是特征的值， 也可以是特征的索引
    return x
```

##### 2. Signed Hash Trick

Weinberger等人提出了一个新的变种, 可以称作signed hash trick.  做法是累加的值不再是固定的1或者特征值, 而是由另外一个哈希函数确定: $ \xi : \mathbb{N} \to {\pm 1} $  这样的好处是可以得到一个无偏的估计.
$$\bar{\phi}_j(x) = \sum_{i\in \mathcal{J}; h(i) = j}\xi(i)\phi_i(x) $$

伪代码：

```shell
function hashing_vectorizer(features: array of string, N:integer):
	x := new vector[N]
    
    for f in features:
    	h := hash(f)
        idx := h mod N
        
        if ξ(f) == 1:
        	x[idx] += 1    # 此处累加的是1， 也可以是特征值
        else ：
        	x[idx] -= 1    # 此处累加的是-1， 也可以是特征值 * -1
            
    return x

```

##### 3. Multiple Hashing

为了防止哈希冲突(亦即不同的特征被哈希到了相同的索引上)带来的负面影响, 可以对那些特征值比较大的特征哈希多次, 如果哈希c次, 则每个索引需要累加的值为 $ \frac{1}{\sqrt{c}}\phi_i(x)$



#### 十、判别式模型 vs. 生成式模型 
[传送](http://www.cnblogs.com/kemaswill/p/3427422.html)

##### 一、 说明
生成式模型(generative model)会对x和y的联合分布p(x,y)进行建模,然后通过贝叶斯公式来求得p(y|x), 最后选取使得p(y|x)最大的yi. 具体地 $ y_{*}=arg \max_{y_i}p(y_i|x)=arg \max_{y_i}\frac{p(x|y_i)p(y_i)}{p(x)}=arg \max_{y_i}p(x|y_i)p(y_i)=arg \max_{y_i}p(x,y_i)$

判别式模型(discriminative model)则会直接对p(y|x)进行建模.

  关于二者之间的优劣有大量的讨论. SVM的发明者Vapnik声称"one should solve the (classification) problem directly and never solve a more general problem as an intermediate step [such as modeling p(x|y)]", 但是, 最近Deep Learning大行其道, 其代表性算法DBN就是生成式模型. 通常来说, 因为生成式模型要对类条件密度(class conditional density)p(x|yi)进行建模, 而判别式模型只需要对类后验密度(class-posterior density)进行建模, 前者通常会比后者要复杂, 更难以建模, 如下图所示.
  
  ![](http://images.cnitblog.com/blog/326731/201311/17013658-f59a7096522d40f893555f4d895d1a97.png)
  
##### 二、 对比

下面简单比较下生成式模型和判别式模型的优缺点.

1. 一般来说, 生成式模型都会对数据的分布做一定的假设, 比如朴素贝叶斯会假设在给定y的情况下各个特征之间是条件独立的: $p(X|y)=\prod_{i=1}^{N}p(x_i|y) $ GDA会假设 $ p(X|y=c,\theta)=\mathcal{N}(\mu_c,\Sigma_c)$  当数据满足这些假设时, 生成式模型通常需要较少的数据就能取得不错的效果, 但是当这些假设不成立时, 判别式模型会得到更好的效果.
2.  生成式模型最终得到的错误率会比判别式模型高, 但是其需要更少的训练样本就可以使错误率收敛[限于Genarative-Discriminative Pair, 详见[2]].

3. 生成式模型更容易拟合, 比如在朴素贝叶斯中只需要计下数就可以, 而判别式模型通常都需要解决凸优化问题.
4. 当添加新的类别时, 生成式模型不需要全部重新训练, 只需要计算新的类别 $y_{new}$ 和x的联合分布 $p(y_{new},x)$ 即可, 而判别式模型则需要全部重新训练.
5. 生成式模型可以更好地利用无标签数据(比如DBN), 而判别式模型不可以.
6. . 生成式模型可以生成x, 因为判别式模型是对p(x,y)进行建模, 这点在DBN的CD算法中中也有体现, 而判别式模型不可以生成x.
7.  判别式模型可以对输入数据x进行预处理, 使用ϕ(x)来代替x, 如下图所示, 而生成式模型不是很方便进行替换.

![](http://images.cnitblog.com/blog/326731/201311/17015101-fecf91ae378c41e1ac640e7260efec98.png)

左图中直接使用x进行逻辑斯蒂回归, 而右图则使用径向基核对x进行变换后再使用逻辑斯蒂回归.

##### 3. 二者所包含的算法

1. 生成式模型

    判别式分析
    朴素贝叶斯
    K近邻(KNN)
    混合高斯模型
    隐马尔科夫模型(HMM)
    贝叶斯网络
    Sigmoid Belief Networks
    马尔科夫随机场(Markov Random Fields)
    深度信念网络(DBN)
    
2.  判别式模型

    线性回归(Linear Regression)
    逻辑斯蒂回归(Logistic Regression)
    神经网络(NN)
    支持向量机(SVM)
    高斯过程(Gaussian Process)
    条件随机场(CRF)
    CART(Classification and Regression Tree)


##### 十一、 偏置-方差分解(Bias-Variance Decomposition)
[传送](http://www.cnblogs.com/kemaswill/archive/2013/06/15/3138170.html)

机器学习的目标是学得一个泛化能力比较好的模型。所谓泛化能力，是指根据训练数据训练出来的模型在新的数据上的性能。这就牵扯到机器学习中两个非常重要的概念：欠拟合和过拟合。如果一个模型在训练数据上表现非常好，但是在新数据集上性能很差，就是过拟合，反之，如果在训练数据集和新数据集上表现都很差，就是欠拟合，如下图所示

![](http://images.cnitblog.com/blog/326731/201306/15221133-e08f50431b9f49ce90cede67009b60e3.png)

 其中蓝叉点表示训练数据，蓝色的线表示学到的模型。左边学到的模型不能很好的描述训练数据，模型过于简单，是欠拟合(Under-fitting)。中间的模型可以比较好的描述训练数据。右边的模型过度的拟合了训练数据(所谓过度，是指训练数据集其实是包含一定的噪声的，如果完全拟合训练数据，会把这些随机噪声也拟合进去)，导致模型过于复杂，很可能在新数据集上表现极差，称为过拟合(Over-fitting)。

  偏置-方差分解(Bias-Variance Decomposition)是统计学派看待模型复杂度的观点。具体如下：

  假设我们有K个数据集，每个数据集都是从一个分布p(t,x)中独立的抽取出来的(t代表要预测的变量，x代表特征变量)。对于每个数据集D，我们都可以在其基础上根据学习算法来训练出一个模型y(x;D)来。在不同的数据集上进行训练可以得到不同的模型。学习算法的性能是根据在这K个数据集上训练得到的K个模型的平均性能来衡量的，亦即：
  
 ![](http://images.cnitblog.com/blog/326731/201306/15222553-96ca28ab8e5749c7a85d1ee68c5dbe4b.png)
 
其中的h(x)代表生成数据的真实函数，亦即t=h(x).

  我们可以看到，给定学习算法在多个数据集上学到的模型的和真实函数h(x)之间的误差，是由偏置(Bias)和方差(Variance)两部分构成的。其中偏置描述的是学到的多个模型和真实的函数之间的平均误差，而方差描述的是学到的某个模型和多个模型的平均之间的平均误差(有点绕，PRML上的原话是variance measures the extent to which the solutions for individual data sets vary around their average)。

  所以在进行学习时，就会存在偏置和方差之间的平衡。灵活的模型(次数比较高的多项式)会有比较低的偏置和比较高的方差，而比较严格的模型(比如一次线性回归)就会得到比较高的偏置和比较低的方差。下图形象的说明了以上两种情况：
  
![](http://images.cnitblog.com/blog/326731/201306/15224835-457c43cdc09347f9a41a521aaf1e14eb.png)

用于训练的是100个数据集，每个数据集包含25个由h(x)=sin(2πx)[右图中的绿线]随机生成的点的。 参数λ控制模型的灵活性(复杂度)，λ越大，模型越简单(严格)，反之越复杂(灵活)。我们生成多个模型(左图中的红线)，并区多个模型的平均值(右图中的红线)。我们可以看到，当λ较大时(最上面的两个图)，平均模型比较简单(最上面的右图)，不能很好的拟合真实函数h(x)，亦即偏差较大，但是多个模型之间比较相似，差距不大，方差较小(最上面的左图)。当λ较小时(最下面的两个图)，平均模型能够非常好的拟合真实函数h(x)，亦即偏差较小(最下面的右图)，但是多个模型之间差距很大，方差比较大(最下面的左图)。

  使用Bagging方法可以有效地降低方差。Bagging是一种再抽样方法(resampling)，对训练数据进行有放回的抽样K次，生成K份新的训练数据，在这K个新的训练数据上训练得到K个模型，然后使用K个模型的平均来作为新的模型。随机森林(Random Forest)是一种基于Bagging的强大的算法。

  造成偏置和方差的原因除了学习方法的不同和参数的不同(比如λ)之外，数据集本身也会对其造成影响。如果训练数据集和新数据集的分布是不同的，会增大偏置。如果训练数据集过少，会增大方差。

  偏置-方差分解是统计学派解释模型复杂度的观点，但是其实用价值不大(Bagging也许是一个例外吧~)，因为偏置-方差分解是基于多个数据集的，而实际中只会有一个训练数据集，将这个数据集作为一个整体进行训练会比将其划分成多个固定大小的数据集进行训练再取平均的效果要好。
  
  
##### 十二、 线性搜索 方法

[传送](http://www.cnblogs.com/kemaswill/p/3416231.html)

##### 十三、 凸优化(Convex Optimization)浅析 

[传送](http://www.cnblogs.com/kemaswill/p/3439552.html)


##### 十四、 计算广告学-多点归因模型(Multi-Touch Attribution Model)

 计算广告学中的一个重要的问题是, 如果用户产生了一次转化(conversion, 比如购买, 注册等), 且该用户在转化之前看过大量不同频道(比如搜索, 展示, 社交等等)的广告, 那么我们如何确定是哪个(或)那些频道的广告导致的这次转化呢?

  这就是归因(Attribution)问题, 如下图所示:
  
 ![](http://images.cnitblog.com/blog/326731/201311/12175205-745a4f9dd8874cc3b0f6dc83c0147f1e.png)
 
  工业界采取的两种方法是“最后阅读获胜”(Last View Win)和“最后点击获胜”(Last Click Win), 前者会把转化归因于这个用户最后一次阅读的广告属于的频道, 后者会归因于最后一次点击的广告属于的频道(如果一直没有点击, 则归因于最后一次阅读的广告属于的频道). 以上两种方法统称为Last-Touch Attribution. 这种方法很简单, 但是忽略了除最后一个频道以外的所有其他频道的广告的影响, 所以效果不是很好.

  实际上, 每个频道都对用户最终的转化产生了影响, 这种考虑多个频道的影响的模型称之为多点归因模型(Multi-Touch Attribution Model). 比较简单的就是线性归因模型和时间衰退归因模型:
  
![](http://images.cnitblog.com/blog/326731/201311/12233041-79066bd6f0bb4a5284312522247e0b91.png)

![](http://images.cnitblog.com/blog/326731/201311/12233103-8e9db51f56004ff9ad97eeaf4534bdff.png)


Xuhui Shao等人提出了使用机器学习的方法来解决归因问题. 把归因看作是分类问题, 对于每个用户, 如果其有转化, 则是正样本, 否则是否样本. 特征则选择该用户在各个频道上的广告的阅读量. 在解决归因问题时, 我们不仅仅要求模型得到很好地分类性能(正确的预测用户是否有转化), 更重要的是, 得到各个频道对于用户的转化的影响, 以确定各个频道对于用户的转化的作用. 

  可以使用逻辑斯蒂回归(Logistic Regression)作为模型, 其得到的各个特征(频道)的系数作为其对该用户的转化的影响. 另外, 因为用户的行为很复杂, 所以单个逻辑斯蒂模型得到的系数估计的变化性可能很大, 这样不利于解释(因为重复试验时得到的各个频道对转化的影响变化很大). Xuhui等人提出了一种新的衡量标准: V-A metric. V(variability)衡量的是模型得到的特征系数(亦即各频道对转化的影响)的可变性, A表示的是对用户分类的准确性.

  Xuhui等人提出使用装袋(bagging)方法训练多个逻辑斯蒂回归模型, 对于每个特征, 我们求得其系数的估计的标准差, 然后取所有特征的标准差的平均值来作为V. 使用多个逻辑斯蒂模型的准确率的平均值来作为A. 算法的具体步骤为:

 1.   从所有的数据中随机的抽取ps比例的训练样本, 以及pc比例的特征. 训练一个逻辑斯蒂模型. 记录各个特征的系数.
 2.   以上步骤迭代M次, 取各个特征的系数的平均值作为各个特征最终的系数值.



#### 十五、 Adaboost 与指数损失

[传送](http://breezedeus.github.io/2015/07/12/breezedeus-adaboost-exponential-loss.html)
Adaboost是著名的ensemble分类算法，具体算法描述见下图1：

![](http://breezedeus.github.io/images/adaboost.png)

上面算法步骤里，有两个关键点：

  1.  在第j步迭代中，每个样本的权重为：

		$$ \omega_i = \frac{\exp \left( -f(x_i)y_i \right)}
 {\sum^n_{i'=1}\exp \left( -f(x_{i'})y_{i'} \right)}, \ \ 
 \forall i = 1, \ldots, n$$
  2.  新子分类器φj对应的权重θj使用如下方式获得：
  
  	$$\theta_j = \frac12 \log \frac{1-R(\varphi_j)}{R(\varphi_j)}  \ \ \text{。} $$
    
	注：子分类器φj的输出为1或−1。
    


下面从可加模型和指数损失的角度来说明为什么样本权重和子分类器权重的计算公式是上面那样的。

记 $m \triangleq f_{\theta}(x) y $ （其中y∈{−1,1}），指数损失如下定义，它也是0/1损失的一种近似（见下图）：

$$ J_{\text{exp}}(m) = \exp (-m) \ \ \text{。} $$

![](http://breezedeus.github.io/images/exponentialloss.png)

而可加模型是多个输出为1和−1的二值基函数 $ \{\varphi_j\}^b_{j=1}$

$$f_{\theta}(x) = \sum_{j=1}^b \theta_j \varphi_j(x)  \ \ \text{，} $$

它通过最小化指数损失来逐步学习φj：

$$ \min_{\theta} \sum_{i=1}^n \exp \left( -f_{\theta}(x_i) y_i \right) \ \ \text{。} $$
    
 例如，我们已经获得了 $\tilde{f}(x)$ , 那么学习下一个基函数φ和θ就是通过最小化下面的指数损失目标函数来获得：
 
 $$ \min_{\theta,\ \varphi} \sum_{i=1}^n \exp \left( 
- \left\{ \tilde{f}(x_i) + \theta \varphi(x_i) \right\} y_i 
\right) 
= \min_{\theta,\ \varphi} \sum_{i=1}^n \tilde{\omega}_i
\exp \left( 
- \theta \varphi(x_i) y_i 
\right) 
\ \ \text{，}$$

其中：

 $$ \tilde{\omega}_i \triangleq \exp \left(
- \tilde{f}(x_i) y_i
 \right)
 \ \ \text{。} $$

这就是Adaboost里样本权重的计算公式（未归一化）。

不妨设θ≥0（θ<0时只要把φ的符号反一下就行），则：

$$ % <![CDATA[

\begin{aligned}
&\sum_{i=1}^n \tilde{\omega}_i
\exp \left( - \theta \varphi(x_i) y_i \right)  \\
&= \exp(-\theta) \sum_{i:\ y_i = \varphi(x_i)} \tilde{\omega}_i 
+ \exp(\theta) \sum_{i:\ y_i \neq \varphi(x_i)} \tilde{\omega}_i \\
&= \left( \exp(\theta)-\exp(-\theta) \right) 
\sum_{i=1}^n \frac{\tilde{\omega}_i}{2} \left( 1-\varphi(x_i)y_i \right)
+ \exp(-\theta) \sum_{i=1}^n \tilde{\omega}_i \ \ \text{。}
\end{aligned}
 %]]>$$
 
 所以，φ的估计可以通过最小化下面的目标函数获得：:
 
 $$ \hat{\varphi} = \arg\min_{\varphi} \sum_{i=1}^n 
\frac{\tilde{\omega}_i}{2} \left( 1-\varphi(x_i)y_i \right)
\ \ \text{。} $$

而前面的式子对θ求导，并使其等于0，可以得到θ的估计：

$$ \hat{\theta} = \frac12 \log \frac{1-\hat{R}}{\hat{R}}
\ \ \text{，} $$

其中：

$$ \hat{R} \triangleq \frac{1}{\sum_{i=1}^n \tilde{\omega}_i}
\sum_{i=1}^n \frac{\tilde{\omega}_i}{2} \left( 1-\varphi(x_i)y_i \right)
\ \ \text{。} $$


上面计算$\hat{\theta}$的公式，就是Adaboost里子分类器对应权重的计算公式。

经过上面的推导，我们可以认为Adaboost对应着指数损失函数的可加模型。很多研究者也提出了基于其他损失函数的Boosting算法，如MAdaboost和Logitboost等。

更详细的资料可见参考文献1。


##### references:

杉山将，《图解机器学习》第9.3节，2015。




#### 十六、 机器学习实践中的7种常见错误


统计建模非常像工程学。

在工程学中，有多种构建键-值存储系统的方式，每个设计都会构造一组不同的关于使用模式的假设集合。在统计建模中，有很多分类器构建算法，每个算法构造一组不同的关于数据的假设集合。

当处理少量数据时，尝试尽可能多的算法，然后挑选最好的一个的做法是比较合理的，因为此时实验成本很低。但当遇到“大数据”时，提前分析数据，然后设计相应“管道”模型（预处理，建模，优化算法，评价，产品化）是值得的。

正如我之前文章中所指出的，有很多种方法来解决一个给定建模问题。每个模型做出不同假设，如何导引和确定哪些假设合理的方法并不明确。在业界，大多数实践者是挑选他们更熟悉而不是最合适的建模算法。在本文中，我想分享一些常见错误（不能做的），并留一些最佳实践方法（应该做的）在未来一篇文章中介绍。

 
##### 1. 想当然地使用缺省损失函数

许多实践者使用缺省损失函数(如，均方误差)训练和挑选最好的模型。实际上，现有损失函数很少符合业务目标。以欺诈检测为例，当试图检测欺诈性交易时，业务目标是最小化欺诈损失。现有二元分类器损失函数为误报率和漏报率分配相等权重，为了符合业务目标，损失函数惩罚漏报不仅要多于惩罚误报，而且要与金额数量成比例地惩罚每个漏报数据。此外，欺诈检测数据集通常含有高度不平衡的标签。在这些情况下，偏置损失函数能够支持罕见情况（如，通过上、下采样）。
##### 2．非线性情况下使用简单线性模型

当构建一个二元分类器时，很多实践者会立即跳转到逻辑回归，因为它很简单。但是，很多人也忘记了逻辑回归是一种线性模型，预测变量间的非线性交互需要手动编码。回到欺诈检测问题，要获得好的模型性能，像“billing address = shipping address and transaction amount < $50”这种高阶交互特征是必须的。因此，每个人都应该选择适合高阶交互特征的带核SVM或基于树的分类器。
##### 3．忘记异常值

异常值非常有趣，根据上下文环境，你可以特殊关注或者完全忽略它们。以收入预测为例，如果观察到不同寻常的峰值收入，给予它们额外关注并找出其原因可能是个好主意。但是如果异常是由于机械误差，测量误差或任何其它不可归纳的原因造成的，那么在将数据输入到建模算法之前忽略掉这些异常值是个不错的选择。

相比于其它模型，有些模型对异常值更为敏感。比如，当决策树算法简单地将每个异常值计为一次误分类时，AdaBoost算法会将那些异常值视为“硬”实例，并为异常值分配极大权值。如果一个数据集含有相当数量的异常值，那么，使用一种具有异常值鲁棒性的建模算法或直接过滤掉异常值是非常重要的。
##### 4．样本数少于特征数（n<<p）时使用高方差模型

SVM是现有建模算法中最受欢迎算法之一，它最强大的特性之一是，用不同核函数去拟合模型的能力。SVM核函数可被看作是一种自动结合现有特征，从而形成一个高维特征空间的方式。由于获得这一强大特性不需任何代价，所以大多数实践者会在训练SVM模型时默认使用核函数。然而，当数据样本数远远少于特征数（n<<p）—业界常见情况如医学数据—时,高维特征空间意味着更高的数据过拟合风险。事实上，当样本数远小于特征数时，应该彻底避免使用高方差模型。
##### 5．尚未标准化就进行L1/L2/等正则化

使用L1或L2去惩罚大系数是一种正则化线性或逻辑回归模型的常见方式。然而，很多实践者并没有意识到进行正则化之前标准化特征的重要性。

回到欺诈检测问题，设想一个具有交易金额特征的线性回归模型。不进行正则化，如果交易金额的单位为美元，拟合系数将是以美分为单位时的100倍左右。进行正则化，由于L1/L2更大程度上惩罚较大系数，如果单位为美元，那么交易金额将受到更多惩罚。因此，正则化是有偏的，并且趋向于在更小尺度上惩罚特征。为了缓解这个问题，标准化所有特征并将它们置于平等地位，作为一个预处理步骤。
##### 6． 不考虑线性相关直接使用线性模型

设想建立一个具有两变量X1和X2的线性模型，假设真实模型是Y=X1+X2。理想地，如果观测数据含有少量噪声，线性回归解决方案将会恢复真实模型。然而，如果X1和X2线性相关（大多数优化算法所关心的），Y=2*X1, Y=3*X1-X2或Y=100*X1-99*X2都一样好，这一问题可能并无不妥，因为它是无偏估计。然而，它却会使问题变得病态，使系数权重变得无法解释。
##### 7. 将线性或逻辑回归模型的系数绝对值解释为特征重要性

因为很多现有线性回归量为每个系数返回P值，对于线性模型，许多实践者认为，系数绝对值越大，其对应特征越重要。事实很少如此，因为：(a)改变变量尺度就会改变系数绝对值；(b)如果特征是线性相关的，则系数可以从一个特征转移到另一个特征。此外，数据集特征越多，特征间越可能线性相关，用系数解释特征重要性就越不可靠。

 

这下你就知道了机器学习实践中的七种常见错误。这份清单并不详尽，它只不过是引发读者去考虑，建模假设可能并不适用于手头数据。为了获得最好的模型性能，挑选做出最合适假设的建模算法—而不只是选择你最熟悉那个算法，是很重要的。



#### 十七、 核逻辑回归
[传送](http://doc.okbase.net/JasonDing1354/archive/142531.html)


#### 十八、  GBDT(Gradient Boosting Decision Tree) 没有实现只有原理 

[传送](http://blog.csdn.net/dark_scope/article/details/24863289)

不同的损失函数和极小化损失函数方法决定了boosting的最终效果，我们现在来说几个常见的boosting：
![](http://img.blog.csdn.net/20140502093849718)

我们知道Gradient Boosting最重要的一步就是去拟合下式：
![](http://img.blog.csdn.net/20140503081116468)

对于不同的Loss function，其梯度有不同的表达式：

![](http://img.blog.csdn.net/20140503081334109)


前向分布算法 实际上是一个贪心的算法，也就是在每一步求解弱分类器Φ(m)和其参数w(m)的时候不去修改之前已经求好的分类器和参数：

![](http://img.blog.csdn.net/20140502090057125)

下面是GBDT的大概框架：(Gradient Tree Boosting应该是GBDT另一种说法，有误请指正)

![](http://img.blog.csdn.net/20140503085235609)

整个框架描述得其实已经很清晰了，就不在这里赘述了，总之所谓Gradient就是去拟合Loss function的梯度，将其作为新的弱回归树加入到总的算法中即可。


#### 正则化方法：L1和L2 regularization、数据集扩增、dropout(含有导数形式)

[传送](http://www.mamicode.com/info-detail-517504.html)

过拟合表现在训练数据上的误差非常小，而在测试数据上误差反而增大。其原因一般是模型过于复杂，过分得去拟合数据的噪声和outliers. 正则化则是对模型参数添加先验，使得模型复杂度较小，对于噪声以及outliers的输入扰动相对较小。 以正则化项和损失函数都是l_2 norm 为例，下面贴一张上课用的slide.

![](http://pic2.zhimg.com/88232c4e94d0b7dc2e076f665d1e3c29_b.jpg)

我们相当于是给模型参数w 添加了一个协方差为1/alpha 的零均值高斯分布先验。 对于alpha =0，也就是不添加正则化约束，则相当于参数的高斯先验分布有着无穷大的协方差，那么这个先验约束则会非常弱，模型为了拟合所有的训练数据，w可以变得任意大不稳定。alpha越大，表明先验的高斯协方差越小，模型约稳定， 相对的variance也越小。

也正如其他答题者所说， 加入正则化是 在bias和variance之间做一个tradeoff. 


#### 十八、 SVM 的MR 实现：Pegasos 算法

Pegasos 算法的工作流程为： 从训练集中随机的挑选一些样本添加到待处理列表中， 之后按序判断每个样本点是否被正确分类， 如果是则忽略，如果不是则将其添加到待更新集合， 批处理完毕后，权重向量按照这些错分类样本进行更新， 

算法的伪代码为：

```shell
将w初始化为0
对每次批处理
	随机选择k个样本点（向量）
    对每个向量
    	如果该向量被错分：
        	更新权重向量W
    累加对w的更新
```

 python 版的PageSos算法
 
```python
def predict(w,x):
	return w*x.T
    
def batchPegasos(dataSet,labels,lam,T,k):
	m,n = shape(dataSet)
    w = zeros(n)
    dataIndex = range(m)
    for t in range(1,T+1):
    	wDelta = mat(zeros(n))
        eta = 1.0 / (lam*t)
        random.shuffle(dataIndex)
        for j in range(k):
        	i = dataIndex[j]
            p = predict(w,dataSet[i,:])
            if labels[i] * p < 1:
            	wDelta += labels[i] * dataSet[i,:].A
        w = (1.0 - 1/t) * w + (eta/k)*wDelta
    return w
```

说明：
	输入值T和K 分别设定了迭代次数和待处理列表的大小，在T 次迭代过程中，每次需要重新计算eta，它是学习速率，代表了权重调幅度的大小，在外层循环中需要选择一批样本进行下一次批处理；在内层循环中执行批处理，将分类错误的值全部累加后更新向量


##### MapReduce 版本的SVM

首先要明白如何将该算法划分成map和reduce阶段， 确认， 那些可以并行， 哪些不行

上面的单机算法中， 大量的时间花费在内积计算上， 另外内积运算可以并行， 但创建新的权重变量w是不能并行的， 这就是 将算法改写成 MapReduce作业的一个切入点。

###### mrjob 中分布式Pegasos 算法的外围代码

```python

from mrjob.job import MRJob

import pickle
from numpy import *

class MRsvm(MRJob):
	DEFAULT_INPUT_PROTOCOL = "json_value"
    
    def __init__(self,*args,**kwargs):
    	super(MRsvm,self).__init__(*arg,**kwargs)
        self.data = pickle.load()
        
        self.w = 0
        self.eta = 0.69
        self.dataList = []
        self.k = self.options.batchsize
        self.numMappers = 1
        self.t = 1
        
    def configure_options(self):
    	super(MRsvm,self).configure_options()
        self.add_passthrough_option(
        '--iterations',dest='iterations',default=2,type='int',
        	help='T: number of iterations to run')
        self.add_passthrough_option(
        '--batchsize',dest='batchsize',default=100,type='int',
        help='K:number of data points in a batch')
        
    def steps(self):
    	return ([self.mr(mapper=self.map,mapper_final=self.map_fin,\
        reducer=self.reduce)]*self.options.iterations)
        
        
if __name__ == "__main__":
	MRsvm.run()

```

###### 分布式pegasos 算法的 mapper 和 reducer 代码

```python

def map(self,mapperId,inVals):
	if False:
    	yield
    if inVals[0] == 'w':
    	self.w = inVals[1]
    elif inVals[0] == 'x':
    	self.dataList.append(inVals[1])
    elif inVals[0] == 't':
    	self.t = inValsp[1]
        
def map_fin(self):
	labels = self.data[:,-1],x= self.data[:,0:-1]
    if self.w == 0:
    	sekf.w = [0.001] * shape(X)[1]
    for index in self.dataList:
    	p = mat(self.w) * X[index,:].T
        if labels[index]*p < 1.0
        	yield(1,['u',index])
            
    yeild (1,['w',self.w])
    yeild (1,['t',self.t])
    
    
def reduce(self,_,packedVals):
	for valArr in packagedVals:
    	if valArr[0] == 'u':
        	self.dataList.append(valArr[1])
        elif valArr[0] == 'w':
        	self.w = valArr[1]
        elif valArr[0] == 't':
        	self.t = valArr[1]
            
    labels = self.data[:,-1]
    X = self.data[:,0:-1]
    wMat = mat(self.w)
    wDelta = mat(zeros(len(self.w)))
    for index in self.dataList:
    	wDelta += float(labels[index]) * X[index,:]
    eta = 1.0 / (2.0 * self.t)
    wMat = (1.0 - 1.0/self.t) * wMat + (eta/self.k)*wDelta
    for mapperNum in range(l,self.numMappers + 1):
    	yield (mapperNum,['w',wMat.tolist()[0]])
        if self.t < self.options.iterations:
        	yeild (mapperNum,['t',self.t + 1])
            for j in range(self.k/self.numMappers):
            	yeild (mapperNum,['x',random.randint(shape(self.data)[0])])

```










