参数推断教程

[toc]

##### 一、概述
[传送](http://blog.sina.com.cn/s/blog_890c6aa30100zc38.html)

图模型（Graphical Models）是一个用来表示概率模型的工具。所谓概率模型，也就是在刻画一组随机变量之间的相互关系。图模型就是用来显式地刻画这些变量之间关系的。在 图模型中，每个变量由图中的一个结点表示，而每一条边则代表其所连接的两个变量之间有相互依赖关系。根据图的结构可以方便地判断变量之间的独立性、条件独 立性等关系，并且可以指导我们做一些inference的工作。

图模型有两种，用无向图表示的称为马尔可夫随机 场（Markov Random Field，MRF），有向图表示的称为贝叶斯网络（Bayesian Network）。其区别在于，贝叶斯网络中的边有明确定义的条件依赖关系。有向图可以转换为无向图，转换的过程称为moralization。转换过程 中，原模型所能表示的一些独立性条件可能会损失掉。不论在哪个模型中，所有变量的联合概率都可以分解为一些因子的乘积，这些因子定义在团（clique） 上。

概率模型中的推断（inference）就是根据给定的一些事实，计算一些相关的随机变量的概 率。典型的例子是计算某些变量的边缘分布，或者计算给定一些变量的观察值之后计算另一些变量的条件概率，这两个问题实际上是相通的。由于计算边缘分布涉及 到对变量的求和或者积分，对于无法计算出显式计算公式的情况，离散变量要穷举所有可能的值，而连续变量则根本无法求解，这就带来了计算问题。以离散变量为 例，穷举所有变量值的计算量就是指数级别的。

精确推断（Exact Inference）利用了图模型中蕴含的条件独立或者依赖关系，将这一穷举过程进行了有序的分解（实际上是分解为一些clique），从而达到一次只穷 举计算一部分的效果，这样就避免了所有变量的同时穷举，使得穷举规模大大降低。消息传播算法以及延伸的junction tree算法都是采用的着一种策略。但这类方法的复杂度受限于图中最大的clique的大小，与其规模成指数关系。所以，对于一些复杂的图而言，精确计算 是不可能的。

基于此，就有学者提出了一些近似方法。典型的有基于变分的方法 （Variational Methods）还有基于采样的方法（Sampling Methods）。杠杆看了一点Tom Jaakkola介绍Variational Methods的Tutorial，对这种方法有了些小认识，在此稍总结一下它的基本思想。对于更深入的思考，等以后有了再记。

 * 思想：
**这一类近似算法的基本策略是将一个计算问题转化为一个优化问题，这个优化问题的最优解就是计算问题的解。因此，当我们朝着该方向进行优化的时候就可以确认我们正逐步接近于我们想要的答案。**

* 过程概述
在图模型中，计算边缘概率的对数值的时候，我们可以把这一算式写成一个简单的优化问题，即优化该对数概率减去一个KL divergence。具体而言，就是写成这样的形式：
　　　　　　　　　　J(Q) = logP(X_v) - KL(Q(X_h)||P(X_h|X_v))
这 其中v代表观察到的变量，h为未观察到的变量，我们所要估计的就是概率项P(X_v)以及条件概率P(X_h|X_v)。由于KL divergence的非负特性，上面的目标函数是logP(X_v)的下界，当我们找到合适的Q使得上式最小时，此时的目标函数值就是P(X_v)，此 时的Q也就是P(X_h|X_v)。该优化问题中，变量是函数Q，这也就是这种方法称为变分法的原因。
经过几步变换，上述目标函数还可以写成
　　　　　　　　　　J(Q) = H(Q) + E_Q{logP(X_v, X_h)}
的形式。这里H(Q)为变分分布的熵，后者为联合分布的对数在Q分布下的期望。如此一来，对数符号中的求和可以挪到对数符号外面，形式上简化了许多。但仅仅这样还远远不够，如果事先对Q的形式没有任何了解，则优化这一函数仍然是不可能的。
近 似方法就是引入了一些对解的限制，减小了可行解空间，从而使问题得到简化，另一方面也使得我们能够得到的最好的解并不一定是原问题的最优解。在图模型的变 分近似方法中，通常通过对变分分布Q引入一些独立性假设，使得问题简化。一种最简单的假设就是假定在Q分布中各变量都是独立的，因此Q函数可分解为单个变 量函数值的积。对于优化过程，则可以一个变量一个变量地进行有序优化，这又进一步使得复杂性得到了降低。解这样一个问题可以采用mean field equation进行迭代计算。这可以保证单调收敛到一个极值点。但我们对问题的简化使得问题不具有全局凸性，因此找到的极值点不保证为最优解。这就是近 似的代价。


##### 二、 理论概述
[传送](http://blog.sina.com.cn/s/blog_a8fead9b010160y5.html)

在Bayesian Statistical中，我们不直接求参数的最大似然估计，而是计算其关于数据的后验概率，并利用这个后验概率来处理其他数据。Bayesian方法相对于统计方法有很多好处，但也带来了一些问题，其中一个就是后验概率的计算往往要比最大似然估计要复杂很多，很多时候后验概率甚至是无法表示的。针对这个问题，Bayesian统计学家提出了一些近似的解决方法，其中一个就是Variational Inference。
Variational Inference通过在一个Restricted Form的函数族中寻找和实际后验概率最接近的一个来达到近似后验概率的目的，通常选择的函数族是Factorized Family，即假设Hidden Variable的后验概率可以分解为若干个independent的部分的概率乘积。

![](http://s12.sinaimg.cn/middle/a8fead9bgc84b88fd429b&690)

在VI方法中，我们试图寻找一个函数q使得其和真实的后验概率p(Z|X)的KL Divergence最小：

![](http://s11.sinaimg.cn/middle/a8fead9bgc84b89e7926a&690)
通过对上式右端针对某一项qi进行优化，我们得到：

![](http://s12.sinaimg.cn/middle/a8fead9btcbc26014b2fb&690)

使得KL Divergence最小的解满足：

![](http://s9.sinaimg.cn/middle/a8fead9btcbc260b916c8&690)

 这就提供了计算qi的迭代方法，即每次依据其他qj的分布来计算某一项Zi的log-likelihood的期望值，然后Normalize用来当作qi的概率分布。这个式子有非常直观的含义，就是每个Z_i的Log-likelihood等于固定Z_i然后让其他Hidden Variable取Variational Distribution后的Log-likelihood的平均值。
Variational Inference方法和EM算法非常相似，两者都是通过计算log-likelihood的期望值来完成迭代，不同的是EM算法的Hidden Variable的后验概率是通过参数计算出来的，而VI方法中参数也被视为Hidden Variable并拥有概率分布，因此迭代出现在若干个概率分布之间。
最后我们指出Variational Inference可以计算log-marginal probability的lower bound，这一数值可以某种程度上比较两次Variational Inference的结果，和EM算法中类似，我们可以将log marginal probability分解如下：

![](http://s6.sinaimg.cn/middle/a8fead9btcbd5b0217d05&690)

其中，

![](http://s11.sinaimg.cn/middle/a8fead9bt7ac89130e10a&690)

因此L(q)是边缘对数似然度的一个下界，通过这个数值的计算我们同样也可以估计算法是否收敛。


##### 另一个介绍

[心怀敬畏的博客地址](http://crescentmoon.info/2013/10/03/%E5%8F%98%E5%88%86%E6%8E%A8%E6%96%AD%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B01%E2%80%94%E2%80%94%E6%A6%82%E5%BF%B5%E4%BB%8B%E7%BB%8D/)


上面的过程总结如下

theta : 参数  theta_p ： 变分参数（variational parameter）  

1. 目标： 估计 p(z|x)
2. 找 p(x | theta) 的似然函数 的一个下界， 在最大化这个下界，找到 p(z|x)的近似分布q(z|theta_P)
3. 找 complete likelihood ln[p(x,z)] 的关于 qj j!=i的期望
4. incomplete likelihood的lower bound 应该是 q(z)*ln p(x,z)/q(z)这个玩意， complete likelihood 乘除q(z)


##### 贝叶斯参数推断 综述

[传送](http://www.blog.huajh7.com/variational-bayes/)

###### key_word: 贝叶斯推断，平均场理论，变分估计，贝叶斯推断，KL散度，确定性估计

###### 一 、 前言

上世纪90年代，变分推断在概率模型上得到迅速发展，在贝叶斯框架下一般的变分法由Attias的两篇文章给出。Matthew J.Beal的博士论文《Variational Algorithms for Approximate Bayesian Inference》中有比较充分地论述，作者将其应用于隐马尔科夫模型，混合因子分析，线性动力学，图模型等。变分贝叶斯是一类用于贝叶斯估计和机器学习领域中近似计算复杂（intractable）积分的技术。它主要应用于复杂的统计模型中，这种模型一般包括三类变量：观测变量(observed variables, data)，未知参数（parameters）和潜变量（latent variables）。在贝叶斯推断中，参数和潜变量统称为不可观测变量(unobserved variables)。变分贝叶斯方法主要是两个目的:

1. 近似不可观测变量的后验概率，以便通过这些变量作出统计推断。
2. 对一个特定的模型，给出观测变量的边缘似然函数（或称为证据，evidence）的下界。主要用于模型的选择，认为模型的边缘似然值越高，则模型对数据拟合程度越好，该模型产生Data的概率也越高。

对于第一个目的，蒙特卡洛模拟，特别是用Gibbs取样的MCMC方法，可以近似计算复杂的后验分布，能很好地应用到贝叶斯统计推断。此方法通过大量的样本估计真实的后验，因而近似结果带有一定的随机性。与此不同的是，变分贝叶斯方法提供一种局部最优，但具有确定解的近似后验方法。

从某种角度看，变分贝叶斯可以看做是EM算法的扩展，因为它也是采用极大后验估计(MAP)，即用单个最有可能的参数值来代替完全贝叶斯估计。另外，变分贝叶斯也通过一组相互依然（mutually dependent）的等式进行不断的迭代来获得最优解。

###### 二、 问题描述

重新考虑一个问题：1）有一组观测数据D，并且已知模型的形式，求参数与潜变量（或不可观测变量）Z={Z1,...,Zn} 的后验分布:P(Z|D)。

正如上文所描述的后验概率的形式通常是很复杂(Intractable)的,对于一种算法如果不能在多项式时间内求解，往往不是我们所考虑的。因而我们想能不能在误差允许的范围内，用更简单、容易理解(tractable)的数学形式Q(Z)来近似P(Z|D),即P(Z|D)≈Q(Z)。


由此引出如下两个问题：

1. 假设存在这样的Q(Z),那么如何度量Q(Z)与P(Z|D)之间的差异性（dissimilarity）？

2.  如何得到简单的Q(Z)?


对于问题一，幸运的是，我们不需要重新定义一个度量指标。在信息论中，已经存在描述两个随机分布之间距离的度量，即相对熵，或者称为Kullback-Leibler散度。

对于问题二，显然我们可以自主决定Q(Z)的分布，只要它足够简单，且与P(Z|D)接近。然而不可能每次都手工给出一个与P(Z|D)接近且简单的Q(Z)，其方法本身已经不具备可操作性。所以需要一种通用的形式帮助简化问题。那么数学形式复杂的原因是什么？在“模型的选择”部分，曾提到Occam's razor，认为一个模型的参数个数越多，那么模型复杂的概率越大;此外，如果参数之间具有相互依赖关系(mutually dependent)，那么通常很难对参数的边缘概率精确求解。

幸运的是，统计物理学界很早就关注了高维概率函数与它的简单形式，并发展了平均场理论。简单讲就是：系统中个体的局部相互作用可以产生宏观层面较为稳定的行为。于是我们可以作出后验条件独立（posterior independence）的假设。即，∀i,p(Z|D)=p(Zi|D)p(Z−i|D)

###### 三、 Kullback-Leibler 散度

在统计学中，相对熵对应的是似然比的对数期望，相对熵D(p||q)度量当真实分布为p而假定分布为q时的无效性。

**定义** 两个概率密度函数为p(x)和q(x)之间的相对熵定义为

KL散度有如下性质：
1. DKL(p||q)≠DKL(q||p) ；
2. DKL(p||q)≥0 ，当且仅当p=q时为零；
3. 不满足三角不等式。

Q分布与P分布的KL散度为：

DKL(Q||P)=∑ZQ(Z)logQ(Z)P(Z|D)=∑ZQ(Z)logQ(Z)P(Z,D)+logP(D)

或者

logP(D)=DKL(Q||P)−∑ZQ(Z)logQ(Z)P(Z,D)=DKL(Q||P)+L(Q)

由于对数证据logP(D)被相应的Q所固定，为了使KL散度最小，则只要极大化L(Q)。通过选择合适的Q，使L(Q)便于计算和求极值。这样就可以得到后验P(Z|D)的近似解析表达式和证据（log evidence）的下界L(Q)，又称为变分自由能（variational free energy）：

L(Q)=∑ZQ(Z)logP(Z,D)−∑ZQ(Z)logQ(Z)=EQ[logP(Z,D)]+H(Q)


![](http://blog.huajh7.com/wp-content/uploads/2013/03/vb-1-300x235.png)


###### 四、 平均场理论(Mean field method)

数学上说，平均场的适用范围只能是完全图，或者说系统结构是well-mixed，在这种情况下，系统中的任何一个个体以等可能接触其他个体。反观物理，平均场与其说是一种方法，不如说是一种思想。其实统计物理的研究目的就是期望对宏观的热力学现象给予合理的微观理论。物理学家坚信，即便不满足完全图的假设，但既然这种“局部”到“整体”的作用得以实现，那么个体之间的局部作用相较于“全局”的作用是可以忽略不计的。

根据平均场理论，变分分布Q(Z)可以通过参数和潜在变量的划分（partition）因式分解，比如将Z划分为Z1…ZM

Q(Z)=∏i=1Mq(Zi|D)

注意这里并非一个不可观测变量一个划分，而应该根据实际情况做决定。当然你也可以这么做，但是有时候，将几个潜变量放在一起会更容易处理。

###### 4.1 平均场方法的合理性

在量子多体问题中，用一个（单体）有效场来代替电子所受到的其他电子的库仑相互作用。这个有效场包含所有其他电受到的其他电子的库仑相互作用。这个有效场包含了所有其他电子对该电子的相互作用。利用有效场取代电子之间的库仑相互作用之后，每一个电子在一个有效场中运动，电子与电子之间的运动是独立的(除了需要考虑泡利不相容原理)，原来的多体问题转化为单体问题。

同样在变分分布Q(Z)这个系统中，我们也可以将每一个潜变量划分看成是一个单体，其他划分对其的影响都可以用一个看做是其自身的作用。采用的办法是迭代(Iterative VB(IVB) algorithm)。这是由于当变分自由能取得最大值的时候，划分Zi与它的互斥集Z−i(或者更进一步，马尔科夫毯(Markov blanket), mb(Zi))具有一个简单的关系：

Q(Zi)∝1Cexp⟨lnP(Zi,Z−i,D)⟩Q(Z−i)orQ(mb(Zi))

（为保持文章的连贯性，此处先不证明，下文将详细说明）

于是，对于某个划分Zi,我们可以先保持其他划分Z−i不变，然后用以上关系式更新Zi。相同步骤应用于其他划分的更新，使得每个划分之间充分相互作用，最终达到稳定值。

具体更新边缘概率（VB-marginal）步骤如下：

 1.  初始化Q(0)(Zi)，可随机取；

 2.  在第k步，计算Z−i的边缘密度Q[k](Z−i|D)∝exp∫Z∗iQ[k−1](Zi|D)logP(Zi,Z−i,D)dZi

 3.  计算Zi的边缘密度Q[k](Zi|D)∝exp∫Z∗−iQ[k](Z−i|D)logP(Zi,Z−i,D)dZ−i

 4. 理论上Q[∞](Zi|D)将会收敛，则反复执行(2), (3)直到Q(Zi),Q(Z−i)稳定，或稳定在某个小范围内。

 5.  最后，得 Q(Z)=Q(Zi|D)Q(Z−i|D)


###### 4.2 平均场估计下边缘概率的无意义性（VB-marginals）

注意到Q(Z)估计的是联合概率密度，而对于每一个Qi(Zi)，其与真实的边缘概率密度Pi(Zi)的差别可能是很大的。不应该用Qi(Zi)来估计真实的边缘密度，比如在一个贝叶斯网络中，你不应该用它来推测某个节点的状态。而这其实是很糟糕的，相比于其他能够使用节点状态信息来进行局部推测的算法，变分贝叶斯方法更不利于调试。

比如一个标准的高斯联合分布P(μ,x)和最优的平均场高斯估计Q(μ,x)。Q选择了在它自己作用域中的高斯分布，因而变得很窄。此时边缘密度Qx(x)变得非常小，完全与Px(x)不同。

![](http://blog.huajh7.com/wp-content/uploads/2013/03/vb-2-300x300.png)

##### 五、 边缘密度（VB-marginal）公式的推导

上文已经提到我们要找到一个更加简单的函数D(Z)来近似P(Z|D)，同时问题转化为求解证据logP(Z)的下界L(Q)，或者L(Q(Z))。应该注意到L(Q)并非普通的函数，而是以整个函数为自变量的函数，这便是泛函。我们先介绍一下什么是泛函，以及泛函取得极值的必要条件。

5.1 泛涵的概念

**泛涵** 设对于(某一函数集合内的)任意一个函数y(x)，有另一个数J[y]与之对应，则称J[y]为y(x)的泛函。泛函可以看成是函数概念的推广。这里的函数集合，即泛函的定义域，通常要求y(x) 满足一定的边界条件，并且具有连续的二阶导数．这样的y(x)称为可取函数。

**泛涵不同于复合函数** 例如g=g(f(x)); 对于后者，给定一个x值，仍然是有一个g值与之对应；对于前者，则必须给出某一区间上的函数y(x)，才能得到一个泛函值J[y]。(定义在同一区间上的)函数不同，泛函值当然不同，为了强调泛函值J[y]与函数y(x)之间的依赖关系，常常又把函数y(x)称为变量函数。

泛函的形式多种多样，通常可以积分形式：J[y]=∫x1x0F(x,y,y′)dx

###### 5.2 泛涵取极值的必要条件

* 泛涵的极值
 “当变量函数为y(x)时，泛函J[y]取极大值”的含义就是：对于极值函数y(x)及其“附近”的变量函数y(x)+δy(x)，恒有J[y+δy]≤J[y];

所谓函数y(x)+δy(x)在另一个函数y(x)的“附近”，指的是：

 1.  |δy(x)|<ε;
 2.  有时还要求|(δy)′(x)|<ε.

这里的δy(x)称为函数y(x)的变分。

*  Euler–Lagrange方程

可以仿造函数极值必要条件的导出办法，导出泛函取极值的必要条件，这里不做严格的证明，直接给出。泛函J[y]取到极大值的必要条件是一级变分δJ[y]为0，其微分形式一般为二阶常微分方程，即Euler-Largange方程：

∂F∂y−ddx∂F∂y′=0

*  泛函的条件极值

在约束条件 下求函数J[y]的极值，可以引入Largange乘子λ，从而定义一个新的泛函，J~[y]=J[y]−λJ0[y]。仍将δy看成是独立的，则泛函J~[y]在边界条件下取极值的必要条件就是，

(∂∂y−ddx∂∂y′)(F−λG)=0

###### 5.3 问题求解

对于L(Q(Z))=EQ(Z)[lnP(Z,D)]+H(Q(Z))，将右式第一项定义为能量(Energy)，第二项看做是信息熵(Shannon entropy)。我们只考虑自然对数的形式，因为对于任何底数的对数总是可以通过换底公式将其写成自然对数与一个常量的乘积形式。另外根据平均场假设可以得到如下积分形式，

L(Q(Z))=∫(∏iQi(Zi))lnP(Z,D)dZ−∫(∏kQk(Zk))∑ilnQi(Zi)dZ

其中Q(Z)=∏iQi(Zi)，且满足 ∀i.∫Qi(Zi)dZi=1

考虑划分Z={Zi,Z−i}，其中Z−i=Z∖Zi，先考虑能量项(Energy)（第一项），

EQ(Z)[lnP(Z,D)]=∫(∏iQi(Zi))lnP(Z,D)dZ

=∫Qi(Zi)dZi∫Q−i(Z−i)lnP(Z,D)dZ−i

=∫Qi(Zi)⟨lnP(Z,D)⟩Q−i(Z−i)dZi

=∫Qi(Zi)lnexp⟨lnP(Z,D)⟩Q−i(Z−i)dZi

=∫Qi(Zi)lnQ∗i(Zi)dZi+lnC

其中定义Q∗i(Zi)=1Cexp⟨lnP(Z,D)⟩Q−i(Z−i)，C为的归一化常数。再考虑熵量(entropy)（第二项），

H(Q(Z))=−∑i∫(∏kQk(Zk))lnQi(Zi)dZ
=−∑i∫∫Qi(Zi)Q−i(Z−i)lnQi(Zi)dZidZ−i
=−∑i⟨∫Qi(Zi)lnQi(Zi)dZi⟩Q−i(Z−i)
=−∑i∫Qi(Zi)lnQi(Zi)dZi

此时得到泛函，

L(Q(Z))=∫Qi(Zi)lnQ∗i(Zi)dZi−∑i∫Qi(Zi)lnQi(Zi)dZi+lnC

=(∫Qi(Zi)lnQ∗i(Zi)dZi−∫Qi(Zi)lnQi(Zi)dZi)−∑k≠i∫Qk(Zk)lnQk(Zk)dZk+lnC
=∫Qi(Zi)lnQ∗i(Zi)Qi(Zi)dZi−∑k≠i∫Qk(Zk)lnQk(Zk)dZk+lnC
=−DKL(Qi(Zi)||Q∗i(Zi))+H[Q−i(Z−i)]+lnC

注意到L(Q(Z))并非只有一个等式，如果不可观测变量有M个划分。那么将有M个方程。为了使得L(Q(Z))达到最大值，同时注意到约束条件，根据泛函求条件极值的必要条件，得，

∀i.∂∂Qi(Zi){−DKL[Qi(Zi)||Q∗i(Zi)]−λi(∫Qi(Zi)dZi−1)}:=0

直接求解将得到Gibbs分布，略显复杂;实际上，注意到KL散度，我们可以直接得到KL散度等于0的时候，L(D)达到最大值，最终得到

Qi(Zi)=Q∗i(Zi)=1Cexp⟨lnP(Zi,Z−i,D)⟩Q−i(Z−i)

C为归一化常数C=∫exp⟨ln(Zi,Z−i,D)⟩Q−i(Z−i)dZ−i , Q(Zi)为联合概率函数在除Zi本身外的其他划分下的对数期望。又可以写为 lnQi(Zi)=⟨lnP(Zi,Z−i,D)⟩Q−i(Z−i)+const

参考文献

























