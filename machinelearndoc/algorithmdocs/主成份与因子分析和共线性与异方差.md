主成份与因子分析和共线性与异方差

[传送门][0]

[toc]

#####简介
在综合评价问题中，都会涉及到排名，多指标排名问题。

   一说到多指标排名，楼主不禁想到前不久刚做的一个项目，也使用了综合评分排名。这当中的关键无疑是怎样统一量纲，给予权数。权数的确定方法很多，但发现近来用主成分和因子分析法赋权的文章越来越多。这两个方法多元统计必讲，但……我相信很多人除了知道因子旋转一下，其余基本是因子主成分傻傻分不清的……（包括楼主自己）

#####一、主成份分析简述
1. 是否可以用较少的几个相互独立的指标代替原来的多个指标，**使其既能减少指标个数，又能综合反映其原指标的信息**？主成分分析结解决这个问题。
2. 有些变量不能或不易直接观察，他们只能**通过其他多个可观察指标来间接反映**。
3. **主成分分析：基本思想降维，将多个相互关联的数值指标转化为少数几个互不相关的综合指标，综合后的指标就是原来多指标的主要成分。**
4. 举例：两个指标x1(年龄)和x2(身高)，x1和x2呈线性正相关，将该直线作为新坐标系的横轴z1，取一条与z1垂直的纵轴z2。在新坐标系中，n个点不再呈线性相关，即z1和z2两个新变量互相独立，且**变异主要集中在z1方向**，说明z1的**方差较大**，z2的**方差较小**。如果此时要研究n个儿童年龄与身高，只需要考虑z1这个变量即可。
![][1]
5. 我们称z1为第一主成分，z2为第二主成分。
6. 主成分个数的选取：
   *  前k个主成分的累积贡献率达到某一特定值（一般采用70%或80%）
   *  特征根>=1
######结果分析
1. 各指标间的相关矩阵
2. 公因子方差：初始值为1，提取里有0说明是特殊因素
3. 解释的总方差：选取主成分个数
4. 成分矩阵：根据0.5原则，大于0.5的作为主成分包含的内容
5. 成分得分系数矩阵：将所有的主成分标示为各个变量的线性组合。

#####二 、因子分析概述
1. 有些变量不能或不易直接观察，他们只能通过其他多个可观察指标来间接反映。例如：医院医疗工作质量不易直接观察，但可以通过门诊人次、出院人数、诊断符合率、治愈率、病死率等一些可观测指标来反映医院医疗工作质量这个**潜在变量**。
2. 通常，多变量之间具有相关性，其产生的原因可能是潜在的因素对观察的变量起支配作用，如何找出这些潜在的因素？这些潜在因素是如何对原始指标起支配作用？因子分析解决这个问题。
3. 因子分析：一种寻找潜在支配因子的模型分析方法，作用是分析可观察到的原始多个变量，找出数目相对较少的，对原始变量有潜在支配作用的因子。找出共性因子变量，估计因子模型，计算共性因子变量的取值和对共性因子变量做出合理的解释。
4. 因子分析分为两类：探索性因子分析，确定性因子分析。
  * 探索性因子分析(简称因子分析):应用在**数据分析初期阶段**，目的是**探究原可测变量的特征、性质及其内部的关联性**，揭示哪些主要的潜在因子可能影响这些可测变量。分析的结果一般不需要进行统计检验，可建立理论变量。
  * 确定性因子分析：在探索性因子分析的基础上进行的，**进一步明确每个潜在因子对可测变量的影响程度和关联程度**，该分析不要求找出潜在因子之间相互独立，目的是**明确潜在因子之间关联性**。分析结果需**要统计校验**。
######结果分析：
 * 主成分信息，取特征值大于1的，如果大于1的累计贡献率过低，也可以选取特征值小于1的。这里可看出，约82.488%的总方差可以由2个潜在因子解释。
 * 累计贡献率达到85%
结果分析有一个统计图 绘制出来
  * 公因子方差比
  * 旋转后的因子矩阵： 比旋转前的因子起到了明显的分离作用，使各因子具有较明确的专业意义。
  
#####三、 主成份分析和因子分析的异同

######1、原理不同
 * 主成分分析基本原理：利用降维（线性变换)的思想，在损失很少信息的前提下把多个指标转化为几个不相关的综合指标（主成分),即每个主成分都是原始变量的线性组合,且各个主成分之间互不相关,使得主成分比原始变量具有某些更优越的性能（主成分必须保留原始变量90%以上的信息），从而达到简化系统结构，抓住问题实质的目的。
 * 因子分析基本原理：利用降维的思想，由研究原始变量相关矩阵内部的依赖关系出发，把一些具有错综复杂关系的变量表示成少数的公共因子和仅对某一个变量有作用的特殊因子线性组合而成。就是要从数据中提取对变量起解释作用的少数公共因子（因子分析是主成分的推广，相对于主成分分析，更倾向于描述原始变量之间的相关关系）
 
######2、线性表示的方向不同
* 因子分析是把变量表示成各公因子的线性组合
* 主成分分析中则是把主成分表示成各变量的线性组合。

######3、 假设条件不同
* 主成份分析： 不需要有假设(assumptions)
* 因子分析： 需要一些假设。因子分析的假设包括：各个共同因子之间不相关，特殊因子（specificfactor）之间也不相关，共同因子和特殊因子之间也不相关。 

######4、求解方法不同
* 求解主成分的方法：从协方差阵出发（协方差阵已知），从相关阵出发（相关阵R已知），采用的方法只有主成分法。（实际研究中，总体协方差阵与相关阵是未知的，必须通过样本数据来估计）

注意事项：由协方差阵出发与由相关阵出发求解主成分所得结果不一致时，要恰当的选取某一种方法；一般当变量单位相同或者变量在同一数量等级的情况下，可以直接采用协方差阵进行计算；对于度量单位不同的指标或是取值范围彼此差异非常大的指标，应考虑将数据标准化，再由协方差阵求主成分；实际应用中应该尽可能的避免标准化，因为在标准化的过程中会抹杀一部分原本刻画变量之间离散程度差异的信息。此外，最理想的情况是主成分分析前的变量之间相关性高，且变量之间不存在多重共线性问题(会出现最小特征根接近0的情况)；

* 求解因子载荷的方法： 主成分法，主轴因子法，极大似然法，最小二乘法，a因子提取法。

######5. 主成份和因子的变化不同
* 主成份分析： 当给定的协方差矩阵或者相关矩阵的特征值唯一时，主成分一般是固定的独特的
* 因子分析： 因子不是固定的，可以旋转得到不同的因子。

######6. 因子数目与主成份的数目
* 主成分分析：主成分的数量是一定的，一般有几个变量就有几个主成分（只是主成分所解释的信息量不等），实际应用时会根据碎石图提取前几个主要的主成分。
* 因子分析：因子个数需要分析者指定（SPSS和sas根据一定的条件自动设定，只要是特征值大于1的因子主可进入分析），指定的因子数量不同而结果也不同；

###### 7. 解释的重点不同
* 主成份分析： 重点在于解释个变量的总方差
* 因子分析：则把重点放在解释各变量之间的协方差。

######8. 算法上的不同
* 主成份分析： 协方差矩阵的对角元素是变量的方差
* 因子分析： 所采用的协方差矩阵的对角元素不在是变量的方差，而是和变量对应的共同度（变量方差中被各因子所解释的部分）

######9. 优点不同
因子分析：对于因子分析，可以使用旋转技术，使得因子更好的得到解释，因此在解释主成分方面因子分析更占优势；其次因子分析不是对原有变量的取舍，而是根据原始变量的信息进行重新组合，找出影响变量的共同因子，化简数据
主成份分析：
1. 如果仅仅想把现有的变量变成少数几个新的变量（新的变量几乎带有原来所有变量的信息）来进入后续的分析，则可以使用主成分分析，不过一般情况下也可以使用因子分析；
2。 通过计算综合主成分函数得分，对客观经济现象进行科学评价；
3. 它在应用上侧重于信息贡献影响力综合评价。
4. 应用范围广，主成分分析不要求数据来自正态分布总体，其技术来源是矩阵运算的技术以及矩阵对角化和矩阵的谱分解技术，因而凡是涉及多维度问题，都可以应用主成分降维；

######10. 应用的场景不同
* 主成份分析：可以用于系统运营状态做出评估，一般是将多个指标综合成一个变量，即将多维问题降维至一维，这样才能方便排序评估；此外还可以应用于经济效益、经济发展水平、经济发展竞争力、生活水平、生活质量的评价研究上；主成分还可以用于和回归分析相结合，进行主成分回归分析，甚至可以利用主成分分析进行挑选变量，选择少数变量再进行进一步的研究。一般情况下主成分用于探索性分析，很少单独使用，用主成分来分析数据，可以让我们对数据有一个大致的了解。
几个常用的组合：
  1. 主成分分析+判别分析，适用于变量多而记录数不多的情况；
  2. 主成分分析+多元回归分析，主成分分析可以帮助判断是否存在共线性，并用于处理共线性问题
  3. 主成分分析+聚类分析，不过这种组合因子分析可以更好的发挥优势。
  
* 因子分析： 
  1. 首先，因子分析+多元回归分析，可以利用因子分析解决共线性问题；
  2. 其次，可以利用因子分析，寻找变量之间的潜在结构；
  3. 再次，因子分析+聚类分析，可以通过因子分析寻找聚类变量，从而简化聚类变量；
  4. 此外，因子分析还可以用于内在结构证实；

###### 总结： 
因子分析(factor analysis)是一种数据简化的技术。它通过研究**众多变量之间**的**内部依赖关系**，探求**观测数据中的基本结构**，并用少数几个**假想变量**来表示其基本的数据结构。这几个**假想变量能够反映原来众多变量的主要信息**。**原始的变量**是可观测的**显在变量**，而**假想变量是不可观测的潜在变量，称为因子**。

要对样本X进行因子分析，只需知道要分解的因子数（z的维度）即可。通过EM，我们能够得到转换矩和误差协方差。

因子分析实际上是降维，在得到各个参数后，可以求得z。但是z的各个参数含义需要自己去琢磨。

例如，在企业形象或品牌形象的研究中，消费者可以通过一个有24个指标构成的评价体系，评价百货商场的24个方面的优劣。

  但消费者主要关心的是三个方面，即商店的环境、商店的服务和商品的价格。因子分析方法可以通过24个变量，找出反映商店环境、商店服务水平和商品价格的三个潜在的因子，对商店进行综合评价。而这三个公共因子可以表示为：
  $$x_i = \mu_i + \alpha_{i1}F_1 + \alpha_{i2}F_2 + \alpha_{i3}F_3 + \epsilon_i;    \   \ \ \ \ \ \ \ \  i=1,2,...,24$$
  
  这里的x_i就是样例x的第i个分量， $\mu_i$就是$\mu$的第i个分量，$\alpha_{ij}$就是
  称$F_i$是不可观测的潜在因子。24个变量共享这三个因子，但是每个变量又有自己的个性，不被包含的部分clip_image213[1]，称为特殊因子。   
  
   注：
 * 因子分析与回归分析不同，因子分析中的因子是一个比较抽象的概念，而回归因子有非常明确的实际意义；
 * 主成分分析分析与因子分析也有不同，主成分分析仅仅是变量变换，而因子分析需要构造因子模型。
 * 主成分分析:原始变量的线性组合表示新的综合变量，即主成分；
 * 因子分析：潜在的假想变量和随机影响变量的线性组合表示原始变量。
##### FQA：
###### 1. 为什么要降维？
A:在实际分析问题时，研究者往往选择很多的指标。这些指标之间经常会存在一定程度的线性相关，这样就会导致信息的重叠。直白说就是用多个指标分析一个问题，由于某些指标反映的是问题的同一方面，这样如果把全部指标都同等地纳入模型，就会导致结果失真。例如衡量学生成绩时，成绩表里有语文、数学、物理、化学。可是化学老师勤快，一学期测验过好多次，所以这里就有多个化学成绩。那么计算总分的时候，如果不把几个化学成绩降维成一个化学成绩，就会由于信息的重叠导致结果失真。(当然还有另外一种情况，学校的科目开设的非常多，比如财务管理，会计学，审计学，概率论，统计学，高数……也可通过降维简单划分为财会类，数理类）

###### 2. 线性相关就一定是信息重叠么？
A： 这个不一定吧。我们举个例子。比如：要衡量经济发展的影响因素，理论上讲，刺激经济发展的三驾马车是投资、消费和出口，那么我们用于衡量经济发展程度是不是就把这三个指标主成分一下？肯定不是。正确的做法应该是这三个相加，纵然他们之间可能存在相关，甚至是高度相关，也不能使用主成分。因为这种相关不是信息的重叠。所以这里记住一点，线性相关并不意味着信息重叠。

######3. 降维一定要用主成份么？
A:这个答案更容易回答，相信很多人都会说否。但实际中却一直这么操作。因为觉得其他降维方法不会呀，而且主成分貌似很高深，用它倍有面子。其实，实际中使用主成分是因为从主观上没有办法删减变量，如果主观上就能区别出哪些是核心原因，哪些不是，直接将不是的删了就行了，没必要搞个神秘的主成分来把问题复杂化。要知道主成分使用时，第一步是标准化，这样一来很多指标的意义就模糊了。这种删减指标的降维方法估计人人都会，可实际中统计专业的达人们却不屑使用。总觉得用这个方法太没面子了。所以这里再强调点，使用方法是为了有效解决问题。有效才是解决问题的关键。

######4. 使用主成份时，相关变量一起上么？
A： 答案依然为否。在使用主成分前，应该先对指标大致分类，将指标中同一类型或者衡量同一个方面的指标归为一类，这样在分类的基础上进行研究。这里有点验证性因子分析的意思。别跟我说这样太主观，其实主观比客观有效的多。如果只有客观，软件就可以解决问题，要人干吗？再说，在人类社会中基本没有绝对客观的东西，所有的客观分析都建立在主观的基础上。高考客观吗？卷子是主观出的。GDP客观吗？指标是主观定的……

######5. 主成份加权很科学么？
A：主成分加权是一种广泛采用的客观赋权方法。赋权的依据是各个主成分的方差贡献率。但是方差大权重就应该大吗？重要性的判定应该依据指标的实际意义或者作用，而不应该简单地依靠方差大小来判定吧。所以在没有弄清楚主成分意义的情况下而盲目加权是不是有点太武断了！

#####共线性与异方差、

######共线性
传统的多元线性回归模型中，总是存在一个基本假设：解释变量为非随机变量且彼此间互不相关（实际上一般只要求不存在完全共线性），随机误差项相互独立且服从零均值同方差的特性。那么这次，我们就来看看时常困扰我们的共线性和异方差的问题。
首先我们来了解一下什么是**共线性**：
所谓多重共线性是指线性回归模型中的解释变量之间由于存在精确相关关系或高度相关关系而使模型估计失真或难以估计准确。一般来说，由于经济数据的限制使得模型设计不当，导致设计矩阵中解释变量间存在普遍的相关关系。完全共线性的情况并不多见，一般出现的是在一定程度上的共线性，即近似共线性。它产生的主要原因有三个方面：

1. 经济变量相关的共同趋势
2. 滞后变量的引入
3. 样本资料的限制

如果解释变量存在共线性，那么模型的参数估计情况就不太乐观了，比如完全共线性下参数估计量不存在；近似共线性下OLS估计量非有效，多重共线性使参数估计值的方差增大；参数估计量经济含义不合理；变量的显著性检验失去意义，可能将重要的解释变量排除在模型之外；模型的预测功能失效（变大的方差容易使区间预测的“区间”变大，使预测失去意义）
需要注意：即使出现较高程度的多重共线性，OLS估计量仍具有线性性等良好的统计性质。但是OLS法在统计推断上无法给出真正有用的信息。

Q：我们该如何判断变量存在共线性？
1. 系数判定法
  * 如果决定系数很大（一般大于0.8），但模型中全部或部分参数却不显著，那么，此时解释变量之间往往存在多重共线性。
  * 从经济理论知某些解释变量对因变量有重要影响，或经检验变量之间线性关系显著，但其参数的检验均不显著，一般就应怀疑是多重共线性所致。
  * 如果对模型增添一个新的解释变量之后，发现模型中原有参数估计值的方差明显增大，则表明在解释变量之间（包括新添解释变量在内）可能存在多重共线性。
2. 用解释变量之间所构成的回归方程的决定系数进行判别
3. 逐步回归判别法：被解释变量逐个引入解释变量，构成回归模型，进行参数估计，根据决定系数的变化决定新引入的变量是否能够加入模型之中。首先将对所有的解释变量分别作回归，得到所有的模型，取决定系数最大的模型中的解释变量加入模型，作为第一个引入模型的变量；其次，再对剩余的解释变量分别加入模型，进行二元回归，再次，取决定系数最大的解释变量加入模型；依次做下去，直到模型的决定系数不再改善为止。
4. 方差膨胀因子VIF判别法：对于多元线性回归模型，一般当VIF＞10时(此时 ＞0.9)，认为模型存在较严重的多重共线性。

Q: 遇到多重共线性，我们该怎么办？
1. 删除不重要的自变量： 
   自变量之间存在共线性，说明自变量所提供的信息是重叠的，可以删除不重要的自变量减少重复信息。但从模型中删去自变量时应该注意从实际经济分析确定为相对不重要并从偏相关系数检验证实为共线性原因的那些变量中删除。如果删除不当，会产生模型设定误差，造成参数估计严重有偏的后果。
2. 追加样本信息：
   多重共线性问题的实质是样本信息的不充分而导致模型参数的不能精确估计，因此追加样本信息是解决该问题的一条有效途径。但是，由于资料收集及调查的困难，要追加样本信息在实践中有时并不容易。
3. 利用非样本先验信息：
   非样本先验信息主要来自经济理论分析和经验认识。充分利用这些先验的信息，往往有助于解决多重共线性问题。
4. 改变解释变量的形式：
   改变解释变量的形式是解决多重共线性的一种简易方法，例如对于横截面数据采用相对数变量，对于时间序列数据采用增量型变量
5. 逐步回归法：
  是一种常用的消除多重共线性、选取“最优”回归方程的方法。其做法是将逐个引入自变量，引入的条件是该自变量经F检验是显著的，每引入一个自变量后，对已选入的变量进行逐个检验，如果原来引入的变量由于后面变量的引入而变得不再显著，那么就将其剔除。引入一个变量或从回归方程中剔除一个变量，为逐步回归的一步，每一步都要进行F 检验，以确保每次引入新变量之前回归方程中只包含显著的变量。这个过程反复进行，直到既没有不显著的自变量选入回归方程，也没有显著自变量从回归方程中剔除为止。
6. 做主成份回归：
  利用主成分提取的原理（比如变量间方差最大等）提取新的变量，新变量间一般不存在线性相关（但也会存在如何给新变量命名的问题），这是比较常用的一种降维方式。

######异方差问题
通常说的异方差性就是(Var(εi ) ≠Var (εj ) , 当i≠j时) 。经济现象是错综复杂的,在建立经济问题的回归分析模型时,经常会出现某一因素或一些因素随着解释变量观测值的变化而对被解释变量产生不同的影响,导致随机误差项产生不同方差,即异方差性。当我们所研究的问题存在异方差性时,线性回归模型的基本假定就被违反了。引起随机误差项产生异方差的原因很多,其中样本数据为截面数据时较容易出现异方差性。而当一个回归问题存在异方差性时,如果仍用OLS估计未知参数,就会造成估计值不是最优、参数的显著性检验失效、回归方程的应用效果极不理想等严重后果。

让我们来看看同方差和异方差残差图上的区别：
![][2]

* 如何更精确的检测是否存在异方差？
  1. 戈德菲尔德-夸特(Goldfeld-Quandt)检验：G-Q检验以F检验为基础，适用于样本容量较大、异方差递增或递减的情况。G-Q检验的思想：先按某一解释变量(通常是可能引起异方差的解释变量)对样本排序，再将排序后的样本一分为二，对子样①和子样②分别作回归，然后利用两个子样的残差平方和之比构造统计量进行异方差检验。
步骤：（1）将解释变量排序，从中间去掉c=n/4个观测值
     （2）分成两个部分，利用样本1和样本2分别建立回归模型
     （3）根据回归求出各自残差平方和RSS1和RSS2
     （4）在同方差假定下，构造F统计量：F=(RSS2/V2)/(RSS1/V1)，与F（V2，V1）进行比较,若F值大于等于比较值则拒绝同方差假设，否则则不拒绝
注意：
   1. 该检验的功效取决于c 值，c 值越大，则大小方差的差异越大，检验功效越好
   2. 两个回归所用的观测值的个数是否相等并不重要，因为可以通过公式改变自由度和统计量的计算公式来调整
   3. 当模型中包含多个解释变量时，应对每个可能引起方差的解释变量都进行检验
   
 2. 怀特（White）检验：怀特检验不需要排序，且适合任何形式的异方差。
    1. 对原模型进行OLS 回归，得到残差ei
    2. 以ei2为被解释变量，以各种解释变量、个解释变量的平方项、解释变量之间两两交叉项为解释变量建立辅助回归方程，并估计
    3. 根据辅助回归方程估计结果构造并计算统计量nR2 ,它服从卡方分布（自由度为辅助回归式中解释变量个数）
    4. 根据临界值判断，若大于临界值，拒绝同方差假定；小于临界值，则不拒绝同方差假定
 注意：
   *  辅助回归中可引入解释变量的更高次幂
   *  在多元回归中，由于解释变量个数太多，可去掉辅助回归式中解释变量间的交叉项

Q：检验出异方差后我该做些什么？
1. WLS（加权最小二乘估计）是一种特殊的广义最小二乘估计，其思想就是对于误差越大的关系额赋予更加大的权重。可以得到关于GLS的稳健的统计量。GLS系数的解释要回到原先的方程中去。如果分析的问题不是个体的数据，而是一个组或者是一个地区的数据平均值，那么就会出现系统性的异方差性。
2. FGLS（可行的最小二乘估计），当我们不知道误差函数的时候，可以采用相应的模型，然后使用数据来估计相应的参数，得到相应的函数形式，最后替代GLS估计中的函数，后面的操作就不变。
  



































[0]:http://bbs.pinggu.org/thread-3070160-1-1.html
[1]:http://1046.edu.pinggu.com/forum/201406/12/091222z87hx2akn8d1n3xt.gif
[2]:http://1046.edu.pinggu.com/forum/201406/01/161924vnvvvyl0ip3z5vvl.jpg