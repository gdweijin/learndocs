####集成学习知识点整理

集成学习是机器学习中的一个非常重要热门的分支，是由多个弱分类器构成一个强分类器。一般的弱分类器可以由决策树，神经网络，贝叶斯分类器，k-近邻构成。

一、*集成学习的三个算法为*： `boosting` `bagging` `stacking`
&nbsp;&nbsp;&nbsp;&nbsp;**booting算法**的弱分类器形成使用同一种机器学习算法，只是其数据抽取时的权值在不断跟新，每一次都是提高前一次分错了的数据集的权值，最后得到T个弱分类器，且分类器的权值也跟其中间结果有关  
&nbsp;&nbsp;&nbsp;&nbsp;**Bagging算法**也是使用的同一种弱分类器，其数据的来源使用bootstrap算法得到的。
&nbsp;&nbsp;&nbsp;&nbsp;**Stack算法**分为2层，第一层使用不同的算法形成T个弱分类器，同时产生一个与原数据集大小相同的新书聚集，利用这个新数据集和一个新算法构成第二层的分类器。

二、*集成学习有效的前提*： 1、每个弱分类器的错误率不能高于0.5.弱分类器之间的性能要有较大的差别，否则集成效果不好。

三、*集成学习按照基本分类器之间的关系可以分为*：异态继承学习和同态继承学习。异态集成学习是指弱分类器之间本身不同，而同态继承学习是指弱分类器之间本身相同只是参数不同。

四、*怎样形成不同的基本分类器*：
 * 基本分类器本身的种类，即构成算法不同
 * 对数据处理不同，比如说boosting，bagging，stacking，cross-validation,hold-out test.etc
 * 对输入特征进行处理和选择
 * 对输出结果进行处理，比如说有的学者提出的纠错码
 * 引入随机扰动
 
五、*基本分类器之间的整合方式*，一般有简单投票，贝叶斯投票，基于D-S证据理论的整合，基于不同特征子集的整合

六、*基础学习性能的分析方法*主要有bias-variance分析方法。

七、*目前的一般性实验结论*：
* Boosting方法的集成分类器效果明显由于bagging，但是在某些数据集上boosting算法的效果还不如单个分类器的。使用随机化的人工神经网络初始权值来进行集成的方法往往能够取得和bagging同样好的效果
* Boosting算法一定程度上依赖于数据集，而bagging对数据集的依赖没有那么明显
* Boosting算法不仅能够减少偏差还能够减少方差，但是bagging算法只能减少方差，对偏差的减少作用不大

八、*未来的研究方向*：
1. 集成学习算法的可理解行需要提高
2. 怎样构造有差异的基础分类器
3. 与SVM的结合
4. 提高boosting的鲁棒性，即降低其对噪声的敏感。
5. 完善出集成学习的一般理论框架

