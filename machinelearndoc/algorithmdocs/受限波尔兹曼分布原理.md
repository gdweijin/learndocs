####受限的波尔兹曼机理论基础

&nbsp;&nbsp;&nbsp;&nbsp; 波尔兹曼网络是一种随机网络，如何来描述一个随机网络，主要有以下亮点
* *概率分布函数* 因为网络节点的取值状态是随机的，从贝叶斯网络的观点看来，要描述整个网络，需要用三种概率分布来描述系统。即联合概率分布，边缘概率分布和条件概率分布。贝叶斯网络的观点，受限的波尔兹曼网络是一个双向的有向图，即从输入层节点可以计算隐藏层节点取某一种状态值的概率，反之亦然。
* *能量函数* 。随机神经网络是根植于统计理学的。受统计力学中能量泛涵的启发，引入了能量函数。能量函数是描述整个系统状态的一种测度。系统越有序，或者概率分布越集中，系统的能量越小。反之越无序或者概率分布越趋于均匀分布，则能量越大。能量函数的最小值，对应于系统状态的最稳定状态。

**网络结构和学习算法**
&nbsp;&nbsp;&nbsp;&nbsp; 正如前面所述，描述RBM的方法是能量函数和概率分布函数，实际上将上述二者结合起来，也就是概率分布是能量函数的泛涵，其能量泛涵和联合概率分布如下：
$$
E(v,h) = - \sum_{i \in visible}a_iv_i - \sum_{j \in hidden}b_jh_j - \sum_{i,j}v_ih_jw_{ij}                  （1）
$$

$$
p(v,h) = \frac{1}{Z}e^{-E(v,h)}   （2）
$$

其上式中的Z 是归一化系数，它的定义如下：
$$
Z = \sum_{v,h}e^{-E(v,h)} （3）
$$
而输入层的边缘概率，是我们感兴趣的，它的计算如下：
$$
p(v) = \frac{1}{Z}\sum_he^{-E(v,h)}  （4）
$$
网络的学习目的是最大可能的拟合输入数据。根据极大似然学习法则，我们的目的就是极大化上面的公式（4） ，其极大化似然函数的定义如下：
$$
\theta^{*} = argmax_{\theta} {\rm L}(\theta) = argmax_{\theta} \sum_{t=1}^{T}logP(v^{t}|\theta)         (5)
$$

注意上面的公式中多了个theta，theta就是网络的权值，包括公式（1） 中的w，a，b，是网络学习需要优化的参数。上面的公式中都有theta这个变量。

2.2 **对比散度学习算法**
&nbsp;&nbsp;&nbsp;&nbsp;根据公式（5） ，逐步展开，运用梯度下降策略，可以导出网络权值的跟新策略如下
$$
\Delta w_{ij} = \epsilon ( )
$$















