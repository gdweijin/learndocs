####基于明确语义分析的自动文摘算法

#####标题
基于ESA的自动文摘算法，单从论文标题可以看出是ESA作为一种工具在“自动文摘”领域中的具体应用，就同《基于显性语义分析的专家相似度研究》一样是在“专家检索”中的应用。所以很好奇想通过比较，探究一下ESA的core在哪，ESA的外延又怎么应用。
#####解决问题
在摘要和概要中，作者表明本文要解决的是限定在医药学领域范围内，用ESA方法对文章进行自动文摘采集，便于数字图书馆的文献检索。同时要设计出实现的算法，并以数学公式或伪代码的方式表达出。
#####相关工作
######概念空间
 语义表示是自然语言处理的核心问题，也就是一个词语要通过某种抽象，以便计算机或程序能处理，而同时尽量反应出其本有的知识背景。ESA借助维基百科中的内容，对词语的语义构建一个概念空间的向量表示，以保证其可读性，即知识背景。维基百科中，每个词条包含了一个特定的主题，可以看做是一个概念；同时词条中的文本内容由该主题下许多词语构成，词语之间、词语和概念之间存在的关系需要由某种模型进行抽象。
 在ESA方法中，以“概念”为列，以“词语”为行构建概念空间矩阵，在概念空间中，概念抽象成“坐标轴”，即一个概念表示一维，词语抽象成“向量”，向量的坐标值表示该词与每个概念的相关度（或相似度），这样整个维基百科可以抽象一个巨大的概念空间。在本文中，词语和概念的相关度用tf-idf值进行度量，在一个N维空间中，其计算公式如下：
 ![image_1][1]
 
  因本文解决的问题限定在医药学领域范围内，故只抽取了中文维基中该领域下的5847个词条内容（即N=5847），及分词、停词后的词语9612个，用以构建医药学领域概念空间。
######基于统计学习的自动文摘算法
在 文献的自动文摘过程中，首先对文献进行逻辑抽象，抽象的过程按如下进行：定义文献集为D = {d1，d2，……dn}；每个文档d中，包含句子集S（d）= {s1，s2，……sn}；每个文档d中，包含词集T（d）= {t1，t2，……tn}。
自动文摘的典型算法是基于统计学，是一种无背景的，纯粹依靠对文献中各个指标的统计数值进行文摘的生成，步骤如下：

![image_2][2]

######“第三”部分——基于ESA的自动文摘算法
分为四步：（1）文本预处理；（2）词权计算；（3）句子权值计算。
1. 文本预处理
首先是最基本的分词过程。然后为了减少计算量，去除噪音是一个非常重要的步骤，本文提出的方法是只保留概念领域中已有的词，也就是上文提到的9612个词语，即不在概念空间中的词都被当作噪音处理掉。
2. 基于语义相似度的词权计算
  在传统方法中，一般是以词在文中出现的频次即count（t，d）作为判断词的重要性的依据。然而同一个语义往往有不同的表达方式，也就是说，不同的词存在语义相似的可能，而在以ESA构建的概念空间中，这种语义的相似性被表现出来，原理是：所有词语都被表示成概念空间中的一个向量，这些向量之间的夹角余弦值可以用来代表对应词语之间在语义上的相似度。因此，本文提出一个新的词权值计算公式：
![image_3][3]
 这里着重对similiar（t，tk）做分析，它相当于代替了传统方法中的count（t，d）。在简单基于统计学方法中，由于其无背景性质，故默认词语t与本身的相关度为1，与其他词语为0，所以统计的就是单纯的count（t，d），即词t的重要程度正比于词t出现的次数，但是一旦有背景的加入，t与其他词语相关度不等于0，而是词t 与d 中所有词的相似度之和。所以count（t，d）相当于similiar（t，tk）在背景知识为“零”的一个特例。
  除了语义相关性对词的权值有影响之外，词出现的具体位置也是一个不小的影响。本文通过实验，只对文章标题这个特殊位置加以考虑，即词如果出现在标题中，其结果要体现在权值的提高中，这种提高体现在系数α上，公式如下：
![image_4][4]
   通过上述公式的演变，文章内部词语之间的语义关系被从分挖掘出来。此外，本文提取出一个更为广义的“接口”公式：
![image_5][5]  
（3）、（4）涉及到自动文摘的特有的知识，省略。

######实验和评测
本文所用的实验数据是从作者所在学校医药论文数据库中采集的200篇论文作为语料库。所用的评测方法是Lin Chin的Roung。工具用到了WEKA的K-MEANS聚类。
 本文中所用的评价指标是Roung中的共现统计，也就是通过计算系统产生的文摘和人工文摘最n 元词的共现统计方法来评价系统效果。并且有准确率P、召回率R和反应系统总体性能F1值，F1 = 2× P× R /(P + R)。

######结果分析
本文结果分析采用的图标对比展示，即对比本文提出的基于ESA和传统tf-idf方法分别在Roung-1和Roung-2中P指标、R指标和F1指标发表现情况，并在对比表下附上了详细的文字说明，最后对反应出的差异给出了1、2、3、4详细的原因解释。
######小结
这篇文献虽不是专门介绍ESA，但是作为一篇通过具体应用去了解ESA，在思路上较为清晰。文中简要提到的Roung评测方法（Lin Chin-Yew. ROUGE: A Package for Automatic Evaluation of Summaries），可以作为后续阅读去了解。

####自动文摘的四种方法
#####简介
所谓自动文摘就是利用计算机自动地从原始文献中提取文摘。文摘是准确全面地反映某一文献中心内容的简洁连贯的短文。
下面我们逐一介绍自动摘录、基于理解的自动文摘、信息抽取和基于结构的自动文摘4种主要的文摘方法。
#####1. 自动摘录
######1.1步骤和依据
自动摘录(Automatic Extraction)将文本视为句子的线性序列，将句子视为词的线性序列。它通常分4步进行：(1)计算词的权值；(2)计算句子的权值；(3)对原文 中的所有句子按权值高低降序排列，权值最高的若干句子被确定为文摘句；(4)将所有文摘句按照它们在原文中的出现顺序输出。
在自动摘录中，计算词权、句权、选择文摘句的依据是文本的6种形式特征：
(1)词频(Frequency)。能够指示文章主题的所谓有效词(Significant Words)往往是中频词。根据句子中有效词的个数可以计算句子的权值，这是Luhn首先提出的自动摘录方法的基本依据［2］。V.A.Oswald主张句子的权值应按其所含代表性“词串”的数量来计算［3，4］，而Doyle则重视共现频度最高的“词对”［5］。美国IBM公司在1960年前后研制了一套文摘自动生产程序ACSI-Matic［3，4］，该程序在句权的计算方面对Luhn的方法进行了改进。1995年美国GE研究与开发中心的Lisa.F.Rau等人完成了ANES(Automatic News Extraction System)系统，该系统采用相对词频作为词的权值［6］。
(2)标题(Title)。标题是作者给出的提示文章内容的短语，借助停用词词表(Stoplist)，在标题或小标题中剔除功能词或只具有一般意义的名词，剩下的词和原文内容往往有紧密的联系，可以作为有效词。
(3)位置(Location)。美国的P.E.Baxendale的调查结果显示：段落的论题是段落首句的概率为85%，是段落末句的概率为7%［3］。因此，有必要提高处于特殊位置的句子的权值。
(4)句法结构(Syntactic Structure)。句式与句子的重要性之间存在着某种联系，比如文摘中的句子大多是陈述句，而疑问句、感叹句等则不宜进入文摘。
(5)线索词(Cue)。Edmundson的文摘系统中有一个预先编制的线索词词典，词典中的线索词分为3种：取正值的褒义词(Bonus Words)，取负指的贬义词(Stigma Words)，取零值的无效词(Null Words)［7，8，9，10］。句子的权值就等于句中每个线索词的权值之和。70年代初，俄亥俄州立大学的James A.Rush教授和他的学生开发了ADAM(Automatic Document Abstracting Method)系统。ADAM强调的是排斥句子的标准而不是选择句子的标准，词控表(WCL)中大多数词是否定性的［11］。
(6)指示性短语(Indicative Phrase)。1977年，英国Lancaster大学的Paice提出根据各种“指示性短语”来选择文摘句的方法［4］。和线索词相比，指示性短语的可靠性要强得多。
Edmundson用一个简单的线性方程W=a1C+a2K+a3T+a4L将4种基本的句子选择方法集成在一起［7，8，9，10］。W代表句子的最终权值，C代表线索词(Cue)权值，K代表根据词频计算而得的关键词(Key)权值，T代表题名词(Title)权值，L代表位置(Location)权值，a1、a2、a3和a4是调节参数。这种将不同性质的因素简单地线性叠加的方式缺乏充分的理由，实践表明确实不够理想。在经过10年的探索之后，Edmundson断言：今后的文摘自动化方法必须考虑文献正文的句法特征和语义特征，而不能简单地依赖粗糙的统计数据。

######1.2多种形式特征的综合运用
上面我们介绍了文本的6种形式特征，即：F-词频、T-标题、L-位置、S-句法结构、C-线索词、I-指示性短语。这6种特征是自动摘录的依据，它 们从不同角度指示了文章的主题，但都不够准确，不够全面。如果能够将上述各种特征“有机”地结合起来，即以W=f(F,T,L,S,C,I)作为计算句子 权值的公式，那么摘录的质量可望进一步提高。问题的关键在于函数f如何确定。
目前的许多自动摘录系统都综合考虑了两种或多种形式特征。比如，新加坡南洋(Nanyang)大学研制的图书馆新闻删节系统(Library Newspaper Cutting System)，提供了题名法、位置法、关键词法和指示性短语法4种自动摘录方法供用户选择。但是在多种特征的结合方面尚待深入研究。
1997年，日本的Tadashi Nomoto等人提出了一种基于语料库的自动摘录方法。他们将一批文献分为训练集和测试集，对训练集中每篇文献内的每个句子自动建立一个包括“该句在文本 中的位置(Location in Text)”、“与标题相近似的程度(Similarity to Title)”等属性的属性集，并人为地将句子分为两类，一类是文摘句，另一类不是文摘句。然后在训练集的基础上建立统计模型，对测试集中每个句子是否能 够作为文摘句的判别问题，将转化为依据决策树对该句进行分类的问题［12］。这种设计思想是在语料库语言学的影响下提出的，它让计算机自动地从训练集中提炼各个特征的结合函数，为多种形式特征的综合利用开辟了一条新的道路。但是，由于它毕竟是建立在文本表层的形式特征基础之上，缺乏深度，所以发展潜力将受到限制。
######1.3 自动摘录的不足
自动摘录所依据的是文本形式上的规律，总的来说，任何一篇文章都不同程度地符合这些规律，因此自动摘录能够适用于非受限域，这是它突出的优点。然而， 具体地讲，一篇文章常常在某些形式特征上符合常规，而在另一些形式特征上违反常规，或者是在文章的某一部分符合常规，而在另一部分违反常规，摘录的结果能 否抓住原文的中心内容要看文章在多大程度上符合常规。因此，自动摘录的质量很不稳定。当加权函数调整时又总是顾此失彼，对这一类文章的效果好了，对另一类 文章的效果又差了。
此外，自动摘录还存在以下3方面的不足：
(1)不全面。对于多主题的文献，用自动摘录方法生成的文摘有时仅包含了原文着重谈论的某个主题，而对于其它方面的内容却只字不提，从而影响了文摘的全面性。这是由于自动摘录缺乏对文本结构的分析而造成的。
(2)不简洁。作者常常在文章中的不同位置用不同形式的句子重复文章的中心内容，以便起到强调的作用。这些句子都是关键句，很容易同时进入文摘，从而 造成文摘内容的冗余。ACSI-Matic程序首先探讨了消除冗余的方法。它对文摘句集合进行检查，如果两个文摘句中的词有1/4以上是相同的，则删除其 中的一句。采用这种粗糙的方法是很难识别出真正的同义句的，而且如果文摘中的任意两个句子都要进行这种比较，那开销就太大了。
(3)不连贯。抽取文章中的若干原句组成的摘要往往缺乏连贯性，因为文章是一个有机的整体，每一个句子通过省略、指代、同义词、相同词以及内在的逻辑 关系与其上下文融为一体。当把文章中不同位置上出现的若干关键句连缀成一个段落时，这些关键句由于脱离了上下文而难以准确地理解。同时，句子之间由于缺乏 逻辑次序而显得杂乱无章，无法在整个段落中构成平滑的概念流。用户在阅读这样的段落时需要进行猜测和推理，不但加重了负担，有时还可能得出与原文不符的观点。
Rush首先对文摘的连贯性标准给予重视。在ADAM系统中，如果某个文摘候选句需要有一个先行词，那么位于该句前面的三个句子即使权值很低也要选入 文摘，从而保证文摘中不出现由于先行词丢失而造成的令人费解的现象。ADAM系统是人们从自动摘录向自动文摘迈出的第一步［3，4］。
Mathis是Rush的学生，她研制的文摘系统能够在词性判定、短语识别和子句识别的基础上，通过并列连词、从属连词等将两个具有相同主语或相同谓 语的简单句归并为一个复合句。该系统还能够通过修改句子中和其它句子相关联的短语而消除句间参照关系，从而进一步改善文摘的连贯性［3］。

#####2 基于理解的自动文摘
基于理解的文摘方法是以人工智能，特别是自然语言理解技术为基础而发展起来的文摘方法。这种方法与自动摘录的明显区别在于对知识的利用，它不仅利用语言学知识获取语言结构，更重要的是利用领域知识进行判断、推理，得到文摘的意义表示，最后从意义表示中生成摘要。
######2.1 基本步骤
基于理解的自动文摘通常有以下步骤：
(1)语法分析。借助词典中的语言学知识对原文中的句子进行语法分析，获得语法结构树。
(2)语义分析。运用知识库中的语义知识将语法结构描述转换成以逻辑和意义为基础的语义表示。
(3)语用分析和信息提取。根据知识库中预先存放的领域知识在上下文中进行推理，并将提取出来的关键内容存入一张信息表。
(4)文本生成。将信息表中的内容转换为一段完整连贯的文字输出。
######2.2 篇章意义的机内表示
篇章意义的机内表示是原文分析的结果和文摘生成的依据，它在基于理解的文摘系统中处于中枢地位。不同系统采用的篇章意义机内表示有所不同。

2.2.1　脚本(Script)
70年代末80年代初，美国耶鲁大学的Schank在脚本的基础上研制了SAM(Script Applier Mechanism)系统。该系统应用脚本分析简单的故事，在此基础上对故事进行总结［14］。
美国耶鲁大学的DeJong于1979年研制了著名的FRUMP(Fast Reading Understanding and Memory Program)系统，该系统用于快速阅览英文新闻资料，是理解文摘系统的样板。FRUMP由预言器和验证器两部分组成。预言器利用梗概剧本预测当前情形 下可能出现的一个或一组事件，验证器的任务是去证实这些被预测的事件，并给出实际信息。FRUMP的应用范围受内部存储的梗概剧本的限制，如果文章中没有 该系统所期望的内容则无法生成任何摘要，会有被误导，以致望文生义的可能［15］。

2.2.2　概念从属结构(Concept Dependency Structure)
美国J.I.Tait的Scrable系统对FRUMP系统进行了改进，它要求输入的资料在处理前先转换成CD(Conceptual Dependency Structrue)结构，在此基础上分析和确定被预测的信息与未预测的信息之间的关系，并将这两部分信息合理地组织成一篇完整连贯的文摘。然而由于CD结构过于复杂，所以实现起来困难较大［15］。
2.2.3　框架(Frame)
80年代末，美国GE研究与开发中心的Lisa F.Rau等研制了SCISOR概念信息缩写、组织和检索系统(System for Conceptual Information Summarization,Organization and Retrieval)。SCISOR属于典型的理解文摘，它处理的对象是有关“公司合并”的新闻报导。SCISOR首先采用关键词过滤和模式匹配的方法对 待处理文献进行主题分析，以便判定该报道的内容是否与“公司合并”有关；然后采用与领域无关的自底向上的分析器TRUMP(TRansportable Understanding Mechanism Package)识别每个句子的结构，生成类似于框架(Frame)的概念表示；最后运用自顶向下的预期驱动的分析器TRUMPET(TRUMP Expectation Tool)从概念表示中提取预期的内容［16，17］。
80年代初，德国康斯坦茨大学的Hahn等人研制了TOPIC系统，该系统针对微处理器领域的科技文献，以框架作为知识表示的基础，通过对全文的语法语义分析生成不同长度的摘要。
2.2.4　一阶谓词
意大利Udine大学的Danilo FUM等人在80年代初研制了SUSY(SUmmarizing System)缩写系统，该系统以一阶谓词逻辑为基础，取得了较好的效果，体现出了逻辑方法的潜力。
SUSY系统由两部分组成。第一部分称为纲要生成器(Schema Builder)，它收集用户需求，形成摘要纲要(Summary Schema)和文本纲要(Text Schema)。第二部分包括分析器(Parser)和缩写器(Summarizer)。分析器自底向上地分析原文，建立起一阶谓词形式的机内表示。缩写 器首先使用文本纲要和加权规则产生加权的内部表示，然后使用摘要纲要和选择规则修剪加权的内部表示，最后从输入文本中检索出基本单元(词、短语或整句)， 将它们装配成摘要［19，20］。

2.3　理解文摘的不足
理解文摘的不足在于领域严格受限。造成领域受限的原因在于：
(1)面向大规模真实语料的语法语义分析技术尚未完全成熟，因此如果想获得高质量的语言分析结果，就必须将待处理的语料限制在某个范围之内。
(2)理解文摘方法的基础是框架等知识表示，框架需要根据领域知识预先拟定，因此如果想把适用于某个领域的理解文摘系统推广到另一领域，则需重新拟定框架，这种填充和组织领域知识的沉重负担使理解文摘难以移植。
#####3 信息抽取
基于理解的文摘方法需要对文章进行全面的分析，生成详尽的语义表达，这对于大规模真实文本而言是很难实现的。与之相比，信息抽取(Information Extraction)只对有用的文本片段进行有限深度的分析，其效率和灵活性显著提高。
信息抽取的自动文摘以文摘框架(Abstract Frame)为中枢，分为选择与生成两个阶段。文摘框架是一张申请单，它以空槽的形式提出应从原文中获取的各项内容。例如，针对计算机病毒类的文章可以提出如下的框架：
病毒{病毒名称：
病毒传染对象：
病毒类属：
病毒攻击对象：
……}
在选择阶段，利用特征词从文本中抽取相关的短语或句子填充文摘框架。例如，在文本中发现“……感染可执行文件……”字样，则可以将特征词“感染”后面的短语“可执行文件”作为病毒的感染对象填入文摘框架。

在选择阶段，利用特征词从文本中抽取相关的短语或句子填充文摘框架。例如，在文本中发现“……感染可执行文件……”字样，则可以将特征词“感染”后面的短语“可执行文件”作为病毒的感染对象填入文摘框架。
在生成阶段，利用文摘模板将文摘框架中的内容转换为文摘输出。文摘模板是带有空白部分的现成的套话，其空白部分与文摘框架中的空槽相对应。例如，“该 病毒的感染对象是(病毒传染对象)”是模板中的一个句子，因为在文摘框架中登记的病毒感染对象为“可执行文件”，因此在文摘中将输出这样的句子：“该病毒 的传染对象是可执行文件”。
英国Lancaster大学Paice等人在1993年提出的选择与生成文摘法，实质上就是信息抽取方法。目前该系统主要针对“小麦实验”方面的文章，但研究者们正在努力使它能够方便地移植到其它领域［21，22］。英国曼彻斯特的Black是Paice的合作者［23］。
由于文摘框架的编写完全依赖于领域知识，所以信息抽取仍然是受领域限制的，只不过文摘框架比理解文摘中的脚本等要简单得多，更易于编写。信息抽取要相 应用于多个领域，就必须为每个领域都编写一个文摘框架，在处理文本时先进行主题识别，根据主题调用相应的文摘框架。另外，单凭特征词或特征短语的提示作用 来填充文摘框架并不是非常准确的，而且由于语言的灵活多样，一些有价值的文本片段可能没有明显的特征。最后，由于使用模板生成文摘，使得文摘的语言千篇一 律，十分呆板。

#####4 基于结构的自动文摘
篇章是一个有机的结构体，篇章中的不同部分承担着不同的功能， 各部分之间存在着错综复杂的关系。篇章结构分析清楚了，文章的核心部分自然能够找到。但是语言学对于篇章结构的研究还很不够，可用的形式规则就更少了，这 使得基于结构的自动文摘到目前为止还没有一套成熟的方法，不同学者用来识别篇章结构的手段也有很大差别。
4.1　关联网络
如果将一个语言单元的各个子单元视为节点，并在两个有语义联系的子单元之间引一条边，那么我们就得到了一个关联网络。在网络中，与一个节点相连的边数称为该节点的度。节点的度越大，则节点在网络中的重要性越高。将最重要的若干子单元抽取出来，即可构成文摘。
前苏联的E.F.Skoroxod′ko将文章视为句子的关联网络，句间的关系建立在词间的同义关系基础之上，和很多句子都有联系的中心句被确认为文摘句［4］。 美国Cornell大学的Salton等人则将文章视为段落的关联网络。文献中的每个段落被赋予一个特征向量，两个段落特征向量的内积作为这两个段落的关 联强度。如果两个段落的关联强度超过给定阈值，则认为两个段落有语义联系。和很多段落都有联系的中心段被提取出来组成一篇文献摘要［24，25，26］。
对于篇幅较长的文章，句子之间的关联网络将十分庞大，其时空开销都将是难以承受的。相比之下，段落之间的关联网络要小得多。另外，和由句子组装起来的 文摘相比，由段落拼接起来的文摘连贯性显著提高。不过，由于最重要的段落中也可能包含一些无关紧要的句子，所以基于段落抽取的文摘显得不够精练。

4.2　修辞结构(Rhetorical Structure)
90年代初，日本Toshiba公司的Kenji Ono等基于修辞结构研究自动文摘。他们将修辞关系归纳为举例〈EG〉、原因〈RS〉、总结〈SM〉等34种，首先依据连接词等推导出一种类似于句法树的 修辞结构树，然后对修辞结构树进行修剪，将保留下来的内容根据它们之间的修辞关系组织成一篇连贯的文摘［27，28］。这种方法的不足在于：修辞关系的识别依赖于连接词，如果文章中连接词的数量很少，那么数修辞关系就无法识别了。

4.3　语用功能
这种方法主要是针对科技文献。科技文献的写作有比较严格的规范，文献中不同部分承担着不同的语用功能，根据语用功能可以将文章的主体部分识别出来构成文摘。
1978年，原捷克斯洛伐克布拉格的Jiri Janos提出了依据功能句子观(Functional Sentence Perspective,FSP)理论进行文本浓缩的方法。FSP是布拉格学派马泰休斯(Mathesius)等人提出的一种语篇理论，Jiri Janos采用该理论的目的是为了通过对句子语用功能的分类将文本的主干(称为Text Proper)和枝叶(称为Metatext)区别开来。文本的主干由主题(Theme)和述题(Rheme)构成，以不同的主题推进方式(Thematic Progression,TP)相互衔接。主干是需要重点分析和摘录的内容，而枝叶则可以排除在文摘之外［29，30］。
日本北海道大学的Maeda将句子的信息功能分为：背景(B)、主题(T)、方法(M)、结果(R)、例子(E)、应用(A)、比较(C)和讨论(D)，并认为T、M、R和D是主干，应进入文摘；E、A、C和B是枝叶，应排除在文摘之外［31］。美国纽约Syracuse大学的Liddy通过对人工文摘的大量调查归纳出经验文摘(Empirical Abstract)的基本结构：背景—目的—方法—结果—结论—附录，其中每一项内容中又包括了一些细则［32，33，34］。如果将文摘中承担这些功能的片段识别出来，就可以组成文摘。
和用其它方法生成的文摘相比，根据语用功能提炼出来的文摘更符合科技文献文摘编写的标准。如果想把这种方法推广到科技文献以外的文本中去，则需要对各类文章的结构深入研究。其实即使是科技文献也有各种类型，理论文章、实验文章和综述文章的结构区别也很大。
#####5　中文自动文摘的研究状况
　　我国从1985年开始介绍国外自动文摘方面的研究情况［35］，从80年代末开始研究自动文摘实验系统，至今也有10余年的历史了。
  在形式特征方面，汉语和西文主要区别是汉语词间没有空格，因而存在着自动分词问题。汉语自动分词是一项经过多年研究仍未圆满解决的难题，以致于南京大学信息管理系的李明提出了从汉字频率统计出发提取文摘的权宜之计，从而回避自动分词问题［36］。 然而因为汉语中真正负载信息的是词而不是字，所以如果分词技术能够满足大规模真实文本处理的需要，那么以词为基础的自动文摘必然优于以字为基础的自动文 摘。实际上，大多数中文文摘系统都要对文本进行分词处理，只是由于采用的分词方法不同，使得分词精度有所不同。此外，汉语的词汇极为丰富，同一个概念可以 用很多不同的词汇表达，这给词频统计带来了一定的困难。上海交通大学王永成教授从80年代末就开始研究自动摘录技术，1997年研制了OA中文文献自动摘 要系统。该系统集成了位置法、指示短语法、关键词法和标题法等多种方法，是一个实用的系统［37，38，39，40］。
  在语言的深层结构方面，汉语存在一些有别于西文的特点。比如，汉语缺乏词形的变化，增加了句法分析的难度；汉语有一些特殊的句式，如兼语、连动等。采 用理解的方法研究中文文摘，必须充分考虑汉语的特点。80年代末，沈阳东北大学姚天顺教授和香港城市理工大学联合开展了“中文全文自动摘要系统”的研究， 该系统采用脚本知识表示，通过与用户交互获取文摘［19，41］。1990 年前后，中科院软件所的李小滨、徐越，在北京大学马希文教授的指导下，对英文自动文摘进行了研究，并研制了一套实验系统——EAAS(English Automatic Abstract System)。该系统是一个标准的理解文摘系统，它局限于“就业机会介绍”这样一个领域。系统首先通过与用户交互获得信息焦点集，然后对文章进行语法语 义分析，生成文章意义框架amf，接着按照信息焦点集从amf中搜索推理出有关信息，最后生成有一定逻辑性的文摘［15，42，43］。
哈尔滨工业大学王开铸教授于1992年研制了基于自然语言理解的文摘实验系统MATAS［44］，1994年研制了自动摘录类的HIT-863Ⅰ型自动文摘系统［41］。1996年笔者提出了基于信息抽取和文本生成的自动文摘方案［45］，1998年完成了基于篇章多级依存结构的HIT-863Ⅱ型自动文摘系统。
近两年来，从事这项研究的单位不断增加。北京邮电大学信息工程系钟义信教授采用的文摘方法类似于Paice的选择与生成文摘法，目前主要针对计算机病毒方面的文章［46］。山西大学郭炳炎教授也在开展自动文摘的研究，他们采用基于统计的方法分析文本结构［47］。据悉，IBM中国研究中心和大陆微软公司都在研制中文自动文摘的产品。
综上所述，自动摘录和理解文摘各有长短。自动摘录能够适用于非受限域，这符合当前自然语言处理技术面向真实语料，面向实用化的总趋势，但是由于它局限 于对文本表层结构的分析，所以经过近40年的发展已接近技术极限，文摘质量很难再有质的飞跃。理解文摘牺牲领域宽度，换取了理解深度，它作为理论探索的价 值很高，但实用性较低，在可预见的未来中前景暗淡。信息抽取能够通过重编文摘框架使文摘系统适应新的领域，它的文摘质量并不比理解文摘逊色，而适用范围更 宽了。基于结构的自动文摘不涉及领域知识，所以它和自动摘录一样是面向非受限域的，同时由于引入了篇章结构方面的知识，使文摘质量有了较大的提高。这两种 文摘方法的综合效果均优于自动摘录和理解文摘。
　　作者简介：刘挺，1973年生，博士研究生，主要从事中文自动文摘、中文自动校对等方面的研究。王开铸，1933年生，教授，长期从事自然语言理解、中文信息处理的研究。
  





 [1]:http://img.my.csdn.net/uploads/201303/30/1364635488_5747.PNG
 [2]:http://img.my.csdn.net/uploads/201303/30/1364636776_5823.PNG
 [3]:http://img.my.csdn.net/uploads/201303/30/1364648286_4078.PNG
 [4]:http://img.my.csdn.net/uploads/201303/30/1364649019_9897.PNG
 [5]:http://img.my.csdn.net/uploads/201303/30/1364649308_4064.PNG